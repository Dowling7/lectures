{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "f1bac091",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.316x Course Team, Copyright 2022 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 5: Uncertainty</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0537c2e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a88af959",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_1\">L5.1 What Do We Call Uncertainty?</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_1\">L5.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_2\">L5.2 Extracting Uncertainty For Linear Fit</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_2\">L5.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_3\">L5.3 Computing Uncertainty</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_3\">L5.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_4\">L5.4 Introduction to Likelihood</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_4\">L5.4 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_5\">L5.5 An Example: Auger Data (Part 1)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_5\">L5.5 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_6\">L5.6 An Example: Auger Data (Part 2)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_6\">L5.6 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_7\">L5.7 Log-Likelihood and Chi-square</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_7\">L5.7 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_8\">L5.8 Minimizing</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_8\">L5.8 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_5_9\">L5.9 Comparison Using lmfit</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_5_9\">L5.9 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dbcc5674",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "In this Lesson, we are going to understand what uncertainty is, and how uncertainty is obtained. For this Lesson, we will focus on data from the Auger Experiment, which measures high energy cosmic rays.\n",
    "\n",
    "We will explore the following topics:\n",
    "\n",
    "- What are fit residuals?\n",
    "- Extracting the fit uncertainty on parameters\n",
    "- Likelihood\n",
    "- Interpreting Likelihood\n",
    "- Getting things to fit: Chi-by-eye and More"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c5601dd",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cell below to import the relevant libraries for this notebook. \n",
    "\n",
    "If you're using a Colab cloud runtime, also run the cell labeled \"for Colab users.\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "18ef64b2",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell00\n",
    "\n",
    "!pip install lmfit"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1d9c7909",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell01\n",
    "\n",
    "import numpy as np               #https://numpy.org/doc/stable/\n",
    "import matplotlib.pyplot as plt  #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n",
    "import csv                       #https://docs.python.org/3/library/csv.html \n",
    "import math                      #https://docs.python.org/3/library/math.html\n",
    "from scipy import stats          #https://docs.scipy.org/doc/scipy/reference/stats.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f5ea5b79",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cf06f143",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.0-runcell02\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "42c71b69",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.1 What Do We Call Uncertainty?</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_0) | [Exercises](#exercises_5_1) | [Next Section](#section_5_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7873076",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_01.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "67f4c8b7",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_01.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "96f0aa56",
   "metadata": {
    "tags": [
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_01.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L05/slides_L05_01.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7eab159b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>What are Fit Residuals?</h3>\n",
    "\n",
    "In the last Lesson, we fit the supernovae data, and got a pretty good fit for Hubble's constant. However, we didn't really try to understand how good our fit was, nor did we try to extract an uncertainty on the fit parameters. To understand what is going on, let's first look at our previous fit to the supernovae data and try to understand residuals. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1b80be9",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "\n",
    "#Let's try to understand how good the fits we made in last Lesson are, let's load the supernova data again\n",
    "label='data/sn_z_mu_dmu_plow_union2.1.txt'\n",
    "\n",
    "\n",
    "def distanceconv(iMu):\n",
    "    power=iMu/5+1\n",
    "    return 10**power\n",
    "\n",
    "def distanceconverr(iMu,iMuErr):\n",
    "    power=iMu/5+1\n",
    "    const=math.log(10)/5.\n",
    "    return const*(10**power)*iMuErr\n",
    "\n",
    "#Now let's zoom in on the small redshift data\n",
    "def load(iLabel,iZMax):\n",
    "    redshift=np.array([])\n",
    "    distance=np.array([])\n",
    "    distance_err=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in plots:\n",
    "            if float(row[1]) > iZMax:\n",
    "                continue\n",
    "            redshift = np.append(redshift,float(row[1]))\n",
    "            distance = np.append(distance,distanceconv(float(row[2])))\n",
    "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n",
    "    return redshift,distance,distance_err\n",
    "\n",
    "redshift=np.array([])\n",
    "distance=np.array([])\n",
    "distance_err=np.array([])\n",
    "redshift,distance,distance_err = load(label,0.1)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589b7015",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell02\n",
    "\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Now let's run the regression again\n",
    "def variance(isamples):\n",
    "    mean=isamples.mean()\n",
    "    n=len(isamples)\n",
    "    tot=0\n",
    "    for pVal in isamples:\n",
    "        tot+=(pVal-mean)**2\n",
    "    return tot/n\n",
    "\n",
    "def covariance(ixs,iys):\n",
    "    meanx=ixs.mean()\n",
    "    meany=iys.mean()\n",
    "    n=len(ixs)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/n\n",
    "\n",
    "def linear(ix,ia,ib):\n",
    "    return ia*ix+ib\n",
    "\n",
    "def regress(redshift,distance):\n",
    "    #Let's regress\n",
    "    var=variance(redshift)\n",
    "    cov=covariance(redshift,distance)\n",
    "    A=cov/var\n",
    "    b=distance.mean()-A*redshift.mean()\n",
    "    #Done!\n",
    "    return A,b\n",
    "\n",
    "def plotAll(redshift,distance,distance_err,A,b):\n",
    "    #now let's plot it\n",
    "    xmax=np.max(redshift)\n",
    "    xvals = np.linspace(0,xmax,100)\n",
    "    yvals = []\n",
    "    for pX in xvals:\n",
    "        yvals.append(linear(pX,A,b))\n",
    "\n",
    "    #Plot the line\n",
    "    plt.plot(xvals,yvals)\n",
    "    plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "    plt.xlabel('redshift(z)', fontsize=15) #Label x\n",
    "    plt.ylabel('distances(parsec)', fontsize=15)#Label y\n",
    "    plt.show()\n",
    "    #Print it out\n",
    "    print(\"Hubbles Constant:\",1e6*3e5/A,\"intercept\",b)#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n",
    "\n",
    "A,b=regress(redshift,distance)\n",
    "plotAll(redshift,distance,distance_err,A,b)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "76104126",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "Now that we have loaded and fit the data again, we would like to actually understand how good the fit is. To do that, we are going to define the residual in $y$. We can define this as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = f(x_{i})-y_{\\rm true} = \\hat{y}_{i}-y_{\\rm true}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Alternatively, we could also divide the above by the uncertainty $\\sigma$. For now, let's compute it for this data, and make a histogram."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3a508016",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.1-runcell03\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def residual(func,args,distance,distance_err=[]):\n",
    "    residuals=[]\n",
    "    for i0 in range(len(redshift)):\n",
    "        pResid=func(redshift[i0],args[0],args[1])-distance[i0]\n",
    "        if len(distance_err) > 0:      \n",
    "            pResid=pResid/distance_err[i0]\n",
    "        residuals.append(pResid)\n",
    "    return np.array(residuals)\n",
    "\n",
    "#This time we are going to look at a histogram of the residuals\n",
    "def plotHist(residuals):\n",
    "    y0, bin_edges = np.histogram(residuals, bins=30)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid',label='residual')\n",
    "    \n",
    "    #for good measure, let's compare this to a gaussian distribution\n",
    "    k=np.linspace(bin_edges[0],bin_edges[-1],100)\n",
    "    normal=stats.norm.pdf(k,0,residuals.std())\n",
    "    plt.plot(k,normal,'o-',label='normal')\n",
    "    plt.xlabel(\"y$_{residual}$\")\n",
    "    plt.ylabel(\"probability\")\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "residuals=residual(linear,[A,b],distance)\n",
    "plotHist(residuals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72cf5ff1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "So, you can see that our residual distribution looks somewhat like a normal (Gaussian) distribution. As a reminder, here is the analytic form of the normal distribution. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "N(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "It has these very important properties, which you can derive yourselves:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[N(x,\\mu,\\sigma)]=\\mu \\\\\n",
    "V[N(x,\\mu,\\sigma)]=\\sigma^2 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "Now, recall from previous Lessons that the *distribution* of the sum of random numbers sampled from any distribution converges to a Gaussian distribution in the large $N$ limit. That means that the noise (i.e., the fluctuations) when taking large enough samples from any random set of distributions should be approximately Gaussian. This is a very powerful statement, that we will use again and again. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "564622fc",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_1'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_1) | [Next Section](#section_5_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4b3c1ae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.1</span>\n",
    "\n",
    "Consider the residual distribution found above, but now divide by the uncertainty of each measurement, that is, following the formula:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = \\frac{f(x_{i})-y_{true, i}}{\\sigma_{y,i}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "If we plot this, and it is truly Gaussian, what are the *expected* values of the mean and stdev of the normalized residual distribution? Enter your answer as a list of two numbers with precision 1e-3: `[mean, stdev]`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "79d60ce9",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[0,1]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The `[mean,stdev]` of this Gaussian distribution should be `[0,1]`.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a2c193ca",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.2</span>\n",
    "\n",
    "Now use the data to compute the mean and standard deviation of the distribution defined by:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "y^{resid}_{i} = \\frac{f(x_{i})-y_{true, i}}{\\sigma_{y,i}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "**Hint: There is an option to do this in the function that we previously defined.**\n",
    "\n",
    "How do the actual values compare to the values of an ideal Gaussian? Enter your answers found using the data as a list of two numbers with precision 1e-3: `[mean, stdev]`.\n",
    "\n",
    "\n",
    "You may wish to use the starting code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1f17aace",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "residuals=residual(linear,[A,b],distance,distance_err)\n",
    "plotHist(residuals)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "779d12df",
   "metadata": {
    "hideCode": true,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "residuals=residual(linear,[A,b],distance,distance_err)\n",
    "plotHist(residuals)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals.std())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00f0ae55",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[0.067,0.981]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "From the above exercise, we see that the residuals are very close to a perfect Gaussian distribution. This means that our uncertainties are very well determined. It's not always the case that this is true. Often experimentalists will overestimate their uncertainties yielding the scenario where the standard deviation is significantly less than $1$ since $\\sigma$ is too large.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71e7901",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.1.3</span>\n",
    "\n",
    "Load the supernova data for entries up to a redshift of 0.2. You can do that by completing the line of code below. Now, compute the residuals dividing by the error, as above. What is the mean and RMS of the residuals now? Why are the residuals not a Gaussian shape? \n",
    "\n",
    "Enter your answer as a list of two numbers with precision 1e-3: `[mean, stdev]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcd00014",
   "metadata": {
    "tags": [
     "py",
     "draft",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "redshift02,distance02,distance02_err = load(label,0.2)\n",
    "\n",
    "#Find these parameters by running the regression using regress()\n",
    "A02,b02=regress(redshift02,distance02)\n",
    "\n",
    "plotAll(redshift02,distance02,distance02_err,A02,b02)\n",
    "\n",
    "#plot regression to check\n",
    "residuals02=residual(linear,[A02,b02],distance02,distance02_err)\n",
    "plotHist(residuals02)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals02.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals02.std())\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "30ad9cf9",
   "metadata": {
    "hideCode": true,
    "hidePrompt": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "redshift02,distance02,distance02_err = load(label,0.2)\n",
    "A02,b02=regress(redshift,distance)\n",
    "plotAll(redshift02,distance02,distance02_err,A02,b02)\n",
    "\n",
    "residuals02=residual(linear,[A02,b02],distance02,distance02_err)\n",
    "plotHist(residuals02)\n",
    "\n",
    "print(\"Ideally we should have a Gaussian with mean(residuals)=0 Instead we have\",residuals02.mean())\n",
    "print(\"Ideally we should have a Gaussian with std(residuals)=1. Instead we have\",residuals02.std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d5f96cd",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[2.1624047,10.581414]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "\n",
    "The distribution is not Gaussian anymore because the higher redshift seems to have a non-linear dependence. This is indicative of a problem in the choice of fit function, maybe a linear dependence is not sufficient to describe the data!\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d5c500",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.2 Extracting Uncertainty For Linear Fit</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_1) | [Exercises](#exercises_5_2) | [Next Section](#section_5_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "554e99c1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, given the residuals above for our fit, we would like to deduce the variance of the parameters $A$ and $b$. For a linear regression, we can do this analytically (i.e., with math). We need two new definitions for the derivation.\n",
    "\n",
    "First, define the residual sum of squares, often referred to as the RSS. It is defined as \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\rm{RSS} & = & \\sum_{i=1}^{N} \\left(y_{i} - f\\left(x_{i})\\right) \\right)^2\n",
    "         & = & \\sum_{i=1}^{N} \\left(y_{i} - Ax_{i}-b \\right)^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Furthermore, we can define the mean squared error($\\hat{\\sigma}_{\\rm{MSE}}$) as the average of the RSS. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "Var(y) & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\left(y_{i} - f\\left(x_{i})\\right) \\right)^2\n",
    "                        & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\left(y_{i} - Ax_{i}-b \\right)^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Here, we don't divide by $N$ because of the fact that $A$ and $b$ are determined from the data, and thus are actually not free parameters, thus removing 2-degrees of freedom in the data. To understand this, imagine what Y(often written as $\\hat{\\sigma}_{\\rm{MSE}}$)  would be if you fit 2 points. It would be exactly 0 since the line $f(x)$ would just connect the two points, so in fact there are no degrees of freedom of variance. A third point would thus fluctuate about the line with an MSE consistent with one point fluctuations. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b058ce8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "Now, we want to compute the variance of $A$. To understand the variance calculation, we need to consider which variables are random and which are not. In a linear regression, we fit $f(x_{i})$ with the understanding it cannot predict $y_{i}$ perfectly. However, $x_{i}$ are defined in this construction to be fixed, true numbers. As a consequence, we can consider $y_{i}$ the one true random variable, with a variance defined as $\\rm{RSS}$. \n",
    "\n",
    "\n",
    "To get the variance on $A$ we can use the variance in $y_{i}$. The way to think about this is that $y_{i}=\\hat{y}_{i}+u_{i}$ where $u_{i}$ is a random variable defining the variation of the observed $y_{i}$ with respect to the result predicted by the fit. \n",
    "\n",
    "Now, let's go back to the definition of $A$ from earlier:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "   A & = &  \\frac{\\frac{1}{N} \\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right) \\left(y_{i}-\\bar{y}\\right)}{\\frac{1}{N} \\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2} \\\\\n",
    "     & = & \\sum_{i=1}^{N}  \\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}   \\left(y_{i}-\\bar{y}\\right)\\\\\n",
    "     & = & \\sum_{i=1}^{N}  w_{i} \\left(y_{i}-\\bar{y}\\right)\n",
    "\\end{eqnarray}   \n",
    "$$\n",
    "\n",
    "\n",
    "where we have written the first bit as a weight $w_{i}$ for brevity. Now, let's compute the variance of this as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\rm{Var}(A) & = & \\rm{Var} \\left(\\sum_{i=1}^N (w_iy_i-w_i\\bar y)\\right) \\\\\n",
    "  & = & \\rm{Var} \\left(\\sum_{i=1}^N w_iy_i\\right) -  \\rm{Var} \\left(\\bar{y}\\sum_{i=1}^N w_i\\right)  = \\sum_{i=1}^N\\rm{Var}(w_iy_i)\\\\\n",
    "  & = & \\sum_{i=1}^N w_i^2 \\rm{Var}(y_i)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where we have used properties of variance and the fact that the quantities $w_i$ are fixed — the $x_i$ are fixed numbers and $\\bar x$ is a constant. We also used the fact that $\\sum_{i=1}^{N} w_{i} = \\sum_{i=1}^{N}(x_{i}-\\bar{x}) = 0$ since $\\bar y$ is a single number in our context. Thus, we have that the variance of the above will just be proportional to the variance of the one number $y_{i}$ about its prediction $\\hat{y}_{i}$.\n",
    "\n",
    "Noting that on average, we can write $\\mathrm{Var}(y)=\\left(y_{i}-\\hat{y}_{i}\\right)^2$, we then get the variance of $A$ as a weighted sum of the variance in $y$. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\rm{Var}(A) & = & \\sum_{i=1}^{N}  w^{2}_{i} \\left(y_{i}-\\hat{y_{i}}\\right)^2 \\\\\n",
    "              & = & \\sum_{i=1}^{N}  w^{2}_{i} \\rm{Var}(y) \\\\\n",
    "              & = & \\sum_{i=1}^{N}  \\left(\\frac{\\left(x_{i}-\\bar{x}\\right)}{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}\\right)^2 \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2}{\\left(\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2\\right)^2} \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{1}{\\left(\\sum_{i=1}^{N} \\left(x_{i}-\\bar{x}\\right)^2\\right)} \\rm{Var}(y) \\\\\n",
    "              & = & \\frac{1}{N\\rm{Var}(x)} \\rm{Var}(y) \\\\\n",
    "              & \\rightarrow & \\frac{1}{N-1}\\frac{\\rm{Var}(y)}{\\rm{Var}(x)} \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where, in the last step, we have reduced $N\\rightarrow N-1$ to account for the fact that $A$ and $b$ are determined from the fit. We can then write the variance of $b$ noting:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "b           & = & \\bar{y} - A\\bar{x} \\\\ \n",
    "\\rm{Var}(b) & = & \\rm{Var}(\\bar{y}) + \\rm{Var}(A)\\bar{x}^2 \\\\\n",
    "            & = & \\frac{1}{N} \\rm{Var}(y) + \\rm{Var}(A)\\bar{x}^2  \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "Recall again that $\\bar{x}$ is not a random variable and so has no variance. Let's now calculate the uncertainties on $A$ and $b$. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0b0d532d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_2'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_2) | [Next Section](#section_5_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bda1e8ef",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 5.2.1</span>\n",
    "\n",
    "Consider a fourth-order polynomial:\n",
    "\n",
    "$$f(x) = a_{4} x^4 + a_{3} x^3 + a_{2} x^2 + a_{1} x + a_{0}$$  \n",
    "\n",
    "If we are using this as our fit function for data consisting of `N` points, how many degrees of freedom do we have? Express your answer in terms of `N`."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ed80a9af",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "N-5\n",
    "</pre>\n",
    "  \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The fit function has 5 parameters, so we have N-5 free points (degrees of freedom).\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "be7545d4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Exercise 5.2.2</span>\n",
    "\n",
    "For the linear function in the example above, the variance in $A$ was expressed as: $\\rm{Var}(A)=\\frac{1}{N-2}\\frac{\\rm{Var}(y)}{\\rm{Var}(x)}$.\n",
    "\n",
    "What is the variance in $b$, expressed in terms of $\\rm{Var}(y)$,  $\\rm{Var}(x)$, $\\bar{x}$, and $N$? Use the appropriate degrees of freedom, and express $\\rm{Var}(y)$ as `Vary`, $\\rm{Var}(x)$ as `Varx`, and $\\bar{x}$ as $xbar$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9c551720",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "'Vary/(N-2) * (1 + xbar^2/Varx)'\n",
    "</pre>\n",
    "  \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Use the definition of $\\rm{Var}(b) = \\frac{1}{N-2} \\rm{Var}(y) + \\rm{Var}(A)\\bar{x}^2 $, and the expression for $\\rm{Var}(A)$.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41757bd4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.3 Computing Uncertainty</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_2) | [Exercises](#exercises_5_3) | [Next Section](#section_5_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2c796c40",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Let's compute the uncertainties.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "aac617bc",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.3-runcell01\n",
    "\n",
    "residuals=residual(linear,[A,b],distance)\n",
    "VarY=np.sum(residuals**2)/(len(redshift)-2)\n",
    "VarA=VarY/variance(redshift)/(len(redshift)-1)\n",
    "Varb=VarA*(redshift.mean())**2+VarY/(len(redshift))\n",
    "print(\"Hubbles Constant:\",1e6*3e5/A,\"+/-\",1e6*3e5*math.sqrt(VarA)/A/A,\"intercept\",b,\"+/-\",math.sqrt(Varb))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5da7553e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Using our findings, we can write the weighted regression uncertainties. We will skip the full derivation and write the answer. The best fit parameters for the weighted regression are\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\bar{y}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{y_{i}}{\\sigma_{i}^2}}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} } \\\\\n",
    " \\bar{x}_{w} & = & \\frac{\\sum_{i=1}^{N} \\frac{x_{i}}{\\sigma_{i}^2}}{\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} } \\\\\n",
    " A           & = &  \\frac{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}}\\left(x_{i}-\\bar{x}_{w}\\right) \\left(y_{i}-\\bar{y}_{w}\\right)}{\\frac{1}{N} \\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} \\left(x_{i}-\\bar{x}_{w}\\right)^2} \\\\\n",
    "b            & = & \\bar{y}_{w} - A\\bar{x}_{w}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and the variance for these parameters can be written below as\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "  \\sigma^{2}     & = & \\frac{1}{N-2}\\sum_{i=1}^{N} \\frac{1}{\\sigma_{i}^{2}} \\left(y_{i} - Ax_{i}-b\\right)^2 \\\\\n",
    "  \\sigma^{2}_{A} & = & \\frac{\\sigma^{2}}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}} \\left(x_{i}-\\bar{x}_{w}\\right)^2} \\\\\n",
    "  \\sigma^{2}_{b} & = & \\left(\\frac{1}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}}}+\\frac{\\bar{x}^2_w}{\\sum_{i=1}^{N}\\frac{1}{\\sigma_{i}^{2}}\\left(x-\\bar{x}_{w}\\right)^2}\\right)\\sigma^2\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "These very much parallel the variances of the parameters except now we have weighted the various events to reflect their respective uncertainties in the measurement. Let's see how our weighted best fit result changes the value of Hubble's constant. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3e703909",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.3-runcell02\n",
    "\n",
    "weights=np.array([])\n",
    "for pVal in distance_err:\n",
    "    weights = np.append(weights,1./pVal/pVal)\n",
    "\n",
    "#Now let's do it with weights\n",
    "def variance_w(isamples,iweights):\n",
    "    mean=np.average(isamples,weights=iweights)\n",
    "    sumw=np.sum(iweights)\n",
    "    tot=0\n",
    "    for i0 in range(len(isamples)):\n",
    "        tot+=iweights[i0]*(isamples[i0]-mean)**2\n",
    "    return tot/sumw\n",
    "\n",
    "def covariance_w(ixs,iys,iweights):\n",
    "    meanx=np.average(ixs,weights=iweights)\n",
    "    meany=np.average(iys,weights=iweights)\n",
    "    sumw=np.sum(iweights)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=iweights[i0]*(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/sumw\n",
    "\n",
    "def regress_w(redshift,weights,distance):\n",
    "    varw=variance_w(redshift,weights)\n",
    "    covw=covariance_w(redshift,distance,weights)\n",
    "    Aw=covw/varw\n",
    "    bw=np.average(distance,weights=weights)-Aw*np.average(redshift,weights=weights)\n",
    "    return Aw,bw\n",
    "\n",
    "Aw,bw=regress_w(redshift,weights,distance)\n",
    "plotAll(redshift,distance,distance_err,Aw,bw)\n",
    "\n",
    "def resid_w(func,args,distance,weights):\n",
    "    residualsw=[]\n",
    "    for i0 in range(len(redshift)):\n",
    "        pResid=linear(redshift[i0],args[0],args[1])-distance[i0]\n",
    "        residualsw.append(weights[i0]*pResid**2)\n",
    "    return np.array(residualsw)\n",
    "\n",
    "residualsw = resid_w(linear,[Aw,bw],distance,weights)\n",
    "sumw=np.sum(weights)\n",
    "rsw=np.average(redshift,weights=weights)\n",
    "sigmaw=np.sum(residualsw)/(len(redshift)-2)\n",
    "VarAw=sigmaw*1./variance_w(redshift,weights)*1./sumw\n",
    "Varbw=VarAw*(rsw)**2+sigmaw/sumw\n",
    "    \n",
    "print(\"Weighted Hubbles Constant:\",1e6*3e5/Aw,\"+/-\",1e6*3e5*math.sqrt(VarAw)/Aw/Aw,\"intercept\",bw,\"+/-\",math.sqrt(Varbw))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n",
    "print()\n",
    "\n",
    "#Now the previous\n",
    "print(\"Unweighted Hubbles Constant:\",1e6*3e5/A,\"+/-\",1e6*3e5*math.sqrt(VarA)/A/A,\"intercept\",b,\"+/-\",math.sqrt(Varb))#Note 1e6 is from pc to Mpc and 3e5 is c in km/s\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "03d9d649",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_3'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_3) | [Next Section](#section_5_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6c16ca4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.3.1</span>\n",
    "\n",
    "Why is the weighted Hubble's constant different from the unweighted one? Select the best answer below:\n",
    "\n",
    "- Points with larger uncertainty pull on the fit-line more than points with smaller uncertainty.\n",
    "- Points with smaller uncertainty pull on the fit-line more than points with larger uncertainty.\n",
    "- It is impossible to know why the fit has changed.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b2927da5",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Points with smaller uncertainty pull on the fit-line more than points with larger uncertainty\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "By performing weighted fits, we change the importance of some points over other points because of their different uncertainties. As a result, points with smaller uncertainty (larger $w=\\frac{1}{\\sigma}$) pull on the line more than points with larger uncertainty. This means that the points with the smaller uncertainties will have more weight than the points with the larger uncertainties.  \n",
    "   \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9c2cf45",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.3.2</span>\n",
    "\n",
    "Compute the difference between the mean and the weighted mean to two significant digits; call this $\\Delta_{\\mathrm{Hubble}}$.\n",
    "\n",
    "Also compute the uncertainty of the difference of the two measurements to three significant digits; call this $\\sigma_{\\mathrm{tot}}$. Assume they are separate, uncorrelated measurements, thus add the uncertainties in quadrature, i.e.:\n",
    "\n",
    "$$\\sigma^2_{\\rm{tot}} = \\sigma^2_{1} + \\sigma^{2}_{2}$$\n",
    "\n",
    "Enter your answer as a list of numbers: [$\\Delta_{\\mathrm{Hubble}}$, $\\sigma_{\\mathrm{tot}}$]\n",
    "\n",
    "\n",
    "Are these variations consistent with each other? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4ce560c6",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "h_weighted = 1e6*3e5/Aw\n",
    "h_unweighted = 1e6*3e5/A\n",
    "\n",
    "unc_weighted = 1e6*3e5*math.sqrt(VarAw)/Aw/Aw\n",
    "unc_unweighted = 1e6*3e5*math.sqrt(VarA)/A/A\n",
    "\n",
    "Delta_h = abs(h_unweighted - h_weighted)\n",
    "unc_tot = np.sqrt(unc_weighted**2. + unc_unweighted**2.)\n",
    "\n",
    "print('Delta_h: ',Delta_h)\n",
    "print('unc_h: ',unc_tot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1524070",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[0.9679, 1.102267]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "We often define consistency to mean that numbers are within 1 standard deviation of one another when we compute the combined uncertainty, defined as $\\sigma^2_{\\rm tot} = \\sigma^2_{1} + \\sigma^{2}_{2}$. In this case we have \n",
    "\n",
    "    \n",
    "$\\Delta_{\\mathrm{Hubble}} = 66.84-65.87 \\pm \\sqrt{0.83^2+0.73^2}$\n",
    "    \n",
    "   \n",
    "$\\Delta_{\\mathrm{Hubble}} = 0.97 \\pm 1.10$\n",
    "\n",
    "or in other words $0.97 < 1.10$ so the two numbers differ by less than 1 standard deviation, meaning we say they are consistent. \n",
    "\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50c03472",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.4 Introduction to Likelihood</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_3) | [Exercises](#exercises_5_4) | [Next Section](#section_5_5) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "44a5fc55",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.4-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_04.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "74d4ea65",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Likelihood</h3>\n",
    "\n",
    "It turns out that 40% of all people have a gene that makes their pee smell after eating asparagus. Call this fraction $p$. Let's say we have 100 people and we count 56 people who claim that their pee smells. What is the probability that this occurred, for a given true value of $p$?\n",
    "\n",
    "\n",
    "From the binomial distribution we can write this event as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "P(56\\mathrm{~smell~}|p)=p^{56}(1-p)^{44}\\frac{100!}{44!56!}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, we can flip this relation and compute the probability of $p$ being some value, given our data that 56 of 100 participants reported their pee smelling.  We call this object the \"likelihood\":\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\mathrm{The~Likelihood}}~P(p~|~56\\mathrm{~smell})=p^{56}(1-p)^{44}\\frac{100!}{44!56!}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "\n",
    "It looks the same, but unlike before, where the probability was a function of data (we assumed $p$ was fixed), now this \"probability\" is a function of the *parameter* $p$, and we leave the data fixed. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "91223e5d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Maximum Likelihood Estimator</h3>\n",
    "\n",
    "If we have some data (the observation of 56/100) and a model (that there's a flat population fraction $p$), the maximum likelihood estimator (MLE) gives an estimate of $p$ by finding the value of $p$ that maximizes the probability of corresponding to the observed data, i.e., the likelihood. Let's compute the maximum likelihood estimate $\\hat p$ of this setup (we will just use the term \"data\" in place of the description \"56 smell\").\n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " \\mathrm{MLE~} P(\\rm{data}|p)&=&\\frac{d}{dp} \\left(p^{56}(1-p)^{44}\\frac{100!}{44!56!}\\right) = 0 \\\\\n",
    "                 &=& 56 \\left( p^{55}(1-p)^{44}\\right) -44 \\left(p^{56}(1-p)^{43}\\right) = 0 \\\\\n",
    "                               &=& 56 \\left(1-p\\right) -44 \\left(p\\right) = 0 \\\\\n",
    "              &=& 56 - 100 p = 0 \\\\\n",
    "\\hat p_{\\mathrm{MLE}}             &=& \\frac{56}{100}            \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Given the data we used, this result should not be surprising! So, the maximum likelihood differs from what we thought for our distribution (i.e., 40%). For a simple model and data input like this, one can reason that we are most likely to find that 56/100 people have the asparagus pee trait when the true population fraction is 56/100. But for more complex systems, it's less clear. For instance, in this scenario we are varying $p$ and not varying the actual decision.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1e6aa21",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Log Likelihood</h3>\n",
    "\n",
    "For completeness, we can also write the log likelihood. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "{\\rm ~The~Log~Likelihood~} \\log\\left(P(\\rm{data}|p)\\right)= 56 \\log p + 44 \\log(1-p)+\\log\\left(\\frac{100!}{44!56!}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We often write things in terms of $\\log$ (which is base $e$) since probabilities can vary by large absolute values and $\\log$ helps to mitigate the large variations. Also, since $\\log$ is positive-definite and we can construct a one-to-one mapping, minimizing $\\log(f)$ equates to minimizing $f$. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e20c65b0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_4'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_4) | [Next Section](#section_5_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3fa8cdf",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.4.1</span>\n",
    "\n",
    "What is the log likelihood evaluated at $p=40$%, given that you observe 56/100 people who claim they can smell asparagus after they pee?\n",
    "\n",
    "You can use the starting code below, if you wish.\n",
    "\n",
    "Report your answer with precision 1e-3."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "280ad89a",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def prob(p,nobs,ntrials):\n",
    "    #nobs: the number of positive observations\n",
    "    #ntrials: the total number of observations (trials)\n",
    "    return #YOUR CODE HERE\n",
    "\n",
    "print(\"Probability is \",prob(0.4,56,100))\n",
    "print(\"Log Likelihood is\", np.log(prob(0.4,56,100)))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5a7aff4e",
   "metadata": {
    "hideCode": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "def prob(p,nobs,ntrials):\n",
    "    nnot = ntrials-nobs\n",
    "    p1 = np.power(p,nobs)\n",
    "    p2 = np.power(1-p,nnot)\n",
    "    nconst=np.math.factorial(ntrials)/(np.math.factorial(nnot)*np.math.factorial(nobs))\n",
    "    return nconst*p1*p2\n",
    "\n",
    "print(\"Probability is \",prob(0.4,56,100))\n",
    "print(\"Log Likelihood is\", np.log(prob(0.4,56,100)))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cfbc5d1c",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "-7.7193\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "\n",
    "We simply take the log of the binomial probability.\n",
    "\n",
    "<pre>\n",
    "def prob(p,nobs,ntrials):\n",
    "    nnot = ntrials-nobs\n",
    "    p1 = np.power(p,nobs)\n",
    "    p2 = np.power(1-p,nnot)\n",
    "    nconst=np.math.factorial(ntrials)/(np.math.factorial(nnot)*np.math.factorial(nobs))\n",
    "    return nconst*p1*p2\n",
    "\n",
    "print(\"Probability is \",prob(0.4,56,100))\n",
    "print(\"Log Likelihood is\", np.log(prob(0.4,56,100)))\n",
    "</pre>\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "36aef3e4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 5.4.1a (ungraded)\n",
    ">\n",
    ">Plot the likelihood and log likelihood as a function of $p$, given the observation of a 56/100 result. Alternatively, plot the probability of attaining a given outcome vs. the number of positive outcomes (in other words, varying $x$, the number of positive results). How are these plots different?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63caaf65",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.5 An Example: Auger Data (Part 1)</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_4) | [Exercises](#exercises_5_5) | [Next Section](#section_5_6) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c91ce742",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_05.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b7eee84",
   "metadata": {
    "tags": [
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_05.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L05/slides_L05_05.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d9b0bd5e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Let's consider a Poisson process: high energy cosmic rays arriving on the Earth per unit area. This is known as the cosmic ray flux. Let's analyze data from the Auger experiment: https://www.auger.org/index.php/science/data.\n",
    "\n",
    "Before we go into the physics of cosmic rays, let's look at the data. As mentioned previously, we can envision cosmic ray data as arising from a Poisson process. The arrival of cosmic ray particles of a certain energy should have some measurable rate that corresponds to that of a Poisson process. Think of it as the question: What is the rate at which particles of a certain energy are detected?\n",
    "\n",
    "For the data, we are going to make some cool plots in visual coordinates. The code may look scary, but don't get too stressed; it's just coordinate transformations and nothing deeper.\n",
    "\n",
    "Moving forward, we are plotting things in coordinates of RA (right ascension) and Dec (declination), which measure location on the sky. They are essentially spherical coordinates."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "13b24dd7",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "\n",
    "#Let's say we have\n",
    "label='data/events_a8_1space.dat'\n",
    "\n",
    "def rad(iTheta):\n",
    "    return iTheta/180. * math.pi\n",
    "\n",
    "def rad1(iTheta):\n",
    "    return iTheta/180. * math.pi-math.pi\n",
    "\n",
    "def exposure(dec):\n",
    "    theta_max = np.radians(60) # Maximum zenith angle in the dataset\n",
    "    l = np.radians(-35.23) # Latitude of the center of the array (near Malargüe - Argentina)\n",
    "    arg = (np.cos(theta_max) - np.sin(l)*np.sin(dec)) / (np.cos(l)*np.cos(dec))\n",
    "    hm = np.arccos(arg.clip(-1, 1))\n",
    "    return np.cos(l)*np.cos(dec)*np.sin(hm) + hm*np.sin(l)*np.sin(dec)\n",
    "\n",
    "def load(label):\n",
    "    dec=np.array([])\n",
    "    ra=np.array([])\n",
    "    az=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile,delimiter=' ')\n",
    "        for pRow in plots:\n",
    "            if '#' in pRow[0] or pRow[0]=='':\n",
    "                continue\n",
    "            dec = np.append(dec,rad(float(pRow[2])))\n",
    "            ra  = np.append(ra,rad1(float(pRow[3])))\n",
    "            az  = np.append(az,rad(float(pRow[4])))\n",
    "    return dec,ra,az\n",
    "\n",
    "dec,ra,az = load(label)\n",
    "\n",
    "color_map = plt.cm.Spectral_r\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.add_subplot(111, projection='mollweide')\n",
    "image = plt.hexbin(ra, dec, cmap=color_map,gridsize=45, mincnt=1,reduce_C_function=np.sum)\n",
    "\n",
    "plt.xlabel('R.A.')\n",
    "plt.ylabel('Decl.')\n",
    "plt.grid(True)\n",
    "plt.colorbar(image, spacing='uniform', extend='max')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a26c0c85",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05"
    ]
   },
   "outputs": [],
   "source": [
    "###NOTE: THIS CELL ATTEMPTS TO IMPORT DATA FROM GITHUB, WILL REPLACE CELL ABOVE\n",
    "\n",
    "#>>>RUN: L5.5-runcell01\n",
    "\n",
    "#Now let's plot this in Galactic Coordinates\n",
    "from astropy.coordinates import SkyCoord\n",
    "from astropy import units\n",
    "from astropy.coordinates import Galactic\n",
    "\n",
    "coords = SkyCoord(ra=ra, dec=dec, unit='rad')\n",
    "rap = coords.galactic.l.wrap_at(180 * units.deg).radian\n",
    "decp = coords.galactic.b.radian\n",
    "\n",
    "color_map = plt.cm.Spectral_r\n",
    "fig = plt.figure(figsize=(20, 8))\n",
    "fig.add_subplot(111, projection='mollweide')\n",
    "image = plt.hexbin(rap, decp, cmap=color_map,gridsize=45, mincnt=1,reduce_C_function=np.sum)\n",
    "\n",
    "plt.xlabel('R.A.')\n",
    "plt.ylabel('Decl.')\n",
    "plt.grid(True)\n",
    "plt.colorbar(image, spacing='uniform', extend='max')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9aab3025",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now, if you look at the above code, you will see one tricky piece, which is a correction for the exposure time. This is a latitude based correction aimed at correcting for the fact that most of the events will be seen at the latitude of the detector (at 34 degrees south), which is what you see when we plot the sky.\n",
    "\n",
    "\n",
    "One interesting thing that might not be clear however, is the fact that there is a  variation of the intensity as a function of  the RA. Let's highlight this variation by plotting a histogram of intensity with 30 bins along the axis of RA (units of radians in this plot). "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc1c59c8",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.5-runcell02\n",
    "\n",
    "from scipy import stats\n",
    "\n",
    "def plotHist(iData,iNBins=30,norm=False):\n",
    "    #Ok enough of having fun, let's look at the asymmetry we observe in right asecion\n",
    "    y0, bin_edges = np.histogram(iData, bins=iNBins)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0=len(iData)*(bin_edges[-1]-bin_edges[0])/iNBins\n",
    "    if not norm:\n",
    "        norm0 = 1\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid',linestyle='none')\n",
    "    plt.xlabel(\"RA\")\n",
    "    plt.ylabel(\"Intensity\")\n",
    "    plt.show()\n",
    "    return bin_centers,y0\n",
    "\n",
    "print(len(ra))\n",
    "_,_ = plotHist(ra,30,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "65e428e8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_5'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_5) | [Next Section](#section_5_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b51082a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.5.1</span>\n",
    "\n",
    "What is the range of declination available in the data (the data are given in units of radians).? Report your answer as a list of numbers, in units of radians, with precision 1e-3: `[dec_min, dec_max]`.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5c99823",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "print('[dec_min, dec_max] =', [min(dec),max(dec)])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "00801a07",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[-1.5690509975429023, 0.7801621756414654]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "Use the following code:\n",
    "    \n",
    "<pre>\n",
    "print('[dec_min, dec_max] =', [min(dec),max(dec)])\n",
    "</pre>\n",
    "    \n",
    "We see that the data have a range `[-1.5690, 0.7802]` (radians), or `[-89.897, 44.702]` (degrees).\n",
    "  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4fee79",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.5.2</span>\n",
    "\n",
    "Now let's look at the variations in the intensity as a function of RA for a subset of the data with a certain range of declination. Specifically, select the data with declination above the equator and plot it.\n",
    "\n",
    "Do you see a trend similar to the one you see in $RA$? Select the best answer from the following:\n",
    "\n",
    "- Yes, the trend is even more clear\n",
    "- No, there is conclusively no trend\n",
    "- There does not appear to be a trend, but the uncertainties are too large to tell\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a8d3611",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "ranew=#YOUR Code here\n",
    "\n",
    "_,_ = plotHist(ranew)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e2448c46",
   "metadata": {
    "hideCode": true,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "ranew=ra[dec > 0]\n",
    "_,_ = plotHist(ranew)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3da34707",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "There does not appear to be a trend, but the uncertainties are too large to tell\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "There doesn't seem to be significant trend, but the uncertainties are 3-4 times larger than when plotting the full dataset. When looking at a subset of the data, you should always be mindful of the uncertainties. \n",
    "  \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b0f33b75",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 5.5.2a (ungraded)\n",
    ">\n",
    ">Systematically explore different slices of the data within the range `[dec_min, dec_max]`. Are there any trends that you notice?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "88b89bca",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.6 An Example: Auger Data (Part 2)</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_5) | [Exercises](#exercises_5_6) | [Next Section](#section_5_7) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e8cb6b65",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_06.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b191d329",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Normally, there are some corrections to this distribution to account for non-uniformities in angle of the detector. Note that these non-uniformities are not so large in RA($\\phi$) for the simple fact that the Earth rotates. So, we will neglect them for this study. However, in reality you can correct for these by building a simulation. \n",
    "\n",
    "Ok, so even without corrections we see a trend in RA. Namely,  that negative RA has more events than positive RA. Let's zoom in on this some more, and count the events. Recall that this is a Poisson process, so the uncertainty on any number $N$ is given by $\\sigma_{N}=\\sqrt{N}$. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d6f78ae2",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-runcell01\n",
    "\n",
    "#Let's say we have\n",
    "NLeft=0\n",
    "NRight=0\n",
    "for i0 in range(len(ra)):\n",
    "    if ra[i0] < 0:\n",
    "        NLeft+=1\n",
    "    else:\n",
    "        NRight+=1\n",
    "print(\"NLeft:\",NLeft,\"+/-\",math.sqrt(NLeft),\"NRight:\",NRight,\"+/-\",math.sqrt(NRight),\"total/2\",len(ra)/2.)\n",
    "\n",
    "plotHist(ra,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8e8059e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "On the left hand side, we see nearly 16600 events over a period of 10 years of operation of Auger. If we take this as a rate, we have that $\\lambda=np=16600$, which means that this process behaves as a Poisson process with maximum likelihood at $\\lambda=16600$. To see this, let's write out a Poisson distribution. We can imagine we have $N$ processes that have a probability of $1$ event in the time period the data was taken. The likelihood is just the probability of all $N$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{L} = \\frac{\\lambda^{N}}{N!} e^{-\\lambda}\\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The log-likelihood of this distribution is just the log of this. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\log(\\mathcal{L})=N\\log(\\lambda)-\\lambda-\\log(N!) \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Optimizing this likelihood distribution, we have\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\frac{d\\mathcal{L}}{d\\lambda}=0\\\\\n",
    "\\frac{N}{\\lambda}-1=0\\\\\n",
    "\\lambda=N\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "So, the maximum likelihood for $\\lambda$ is just the number of events we observe in the data. Hence we can use that for the aggregate or split distribution to deduce the deviation. Let's compute this by computing the p-value ratio of the most likely occurence of a Poisson process, and the one actually observed. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bab6d30e",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.6-runcell02\n",
    "\n",
    "def pvalues(NLeft,NRight):\n",
    "    #Let's compute it\n",
    "    lamb=(NLeft+NRight)/2. #average number of events\n",
    "    pleft=stats.poisson.pmf(NLeft,lamb) #probability of left given averge\n",
    "    pright=stats.poisson.pmf(NRight,lamb) #probability of right given averaged \n",
    "    pcheck=stats.poisson.pmf(int(lamb),lamb)#Most likely probability \n",
    "    print(\"Likelihood Ratio-left\",pleft/pcheck,\"Likelihood Ratio-right\",pright/pcheck,\"check\",pcheck/pcheck)\n",
    "    #pleft=1-stats.poisson.cdf(NLeft,lamb) #probability of left given averge\n",
    "    #pright=stats.poisson.cdf(NRight,lamb) #probability of right given averaged \n",
    "    #pcheck=stats.poisson.cdf(int(lamb),lamb)#Most likely probability \n",
    "    #print(\"CDF Prob left\",pleft,\"CDF Prob Right\",pright,\"Check:\",pcheck)\n",
    "pvalues(NLeft,NRight)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a1d6d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "If we predict the rate in right ascension to be uniform, then the number of events that occur at negative (left) RA should be equal to the number of events that occur at positive (right) RA. Let's take the average number of events between left and rights RA to be the expected value of events that occur, if the distribution of events is uniform. Thus, we have that $N_{\\mathrm{avg}}=16093.5$.\n",
    "\n",
    "In this case, the likelihood for $N_{\\mathrm{left}}$ is $p=3\\times10^{-4}$ less likely that than the average value. Likewise with the right.\n",
    "\n",
    "In the following questions we will think a bit more about this deviation, and then perform a similar analysis with a different data set."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "72672bf2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_6'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_6) | [Next Section](#section_5_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0389c82",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.6.1</span>\n",
    "\n",
    "Consider the analysis that we just performed, where we split the data into left and right RA. How many standard deviations is the count in the left bin, $N_{\\mathrm{left}}$, from the expected (or average) count, $N_{\\mathrm{avg}}$, assuming a uniform rate? Recall that $N_{\\mathrm{avg}}=16093.5$ and $N_{\\mathrm{left}} = 16600 \\pm 128.84$.\n",
    "\n",
    "Calculate the same number for the right bin, and report your answer as a list of two positive numbers `[num_stdev_left, num_stdev_right]`, with precision 1e-2."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a1b17ea",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "pass\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e1df48aa",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "Navg=len(ra)/2.\n",
    "num_stdev_left = abs(NLeft-Navg)/np.sqrt(NLeft)\n",
    "num_stdev_right = abs(NRight-Navg)/np.sqrt(NRight)\n",
    "\n",
    "print('[num_stdev_left, num_stdev_right]=',[num_stdev_left, num_stdev_right])\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a493bc4",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[3.931202412702576, 4.056936237524216]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "See solution code:\n",
    "\n",
    "<pre>\n",
    "Navg=len(ra)/2.\n",
    "num_stdev_left = abs(NLeft-Navg)/np.sqrt(NLeft)\n",
    "num_stdev_right = abs(NRight-Navg)/np.sqrt(NRight)\n",
    "\n",
    "print('[num_stdev_left, num_stdev_right]=',[num_stdev_left, num_stdev_right])\n",
    "</pre>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fc4076d",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-5.6.2</span>\n",
    "\n",
    "So far, we've been working with the high energy data. Now load the data at lower energy (4-8 EeV cosmic rays) `events_a4_1space.dat`. As we did previously, plot the RA variation using 30 bins, then make a plot using two bins (splitting the data into left and right sides).\n",
    "\n",
    "Compute the likelihood ratio for the left data bin and the right data bin, as we did previously. Complete the code below to help do this, or write your own.\n",
    "\n",
    "What is the probability that we observe the number of counts in each data bin (left and right), assuming that the rate of counts is uniform in RA? Report your answer as a list of two positive numbers `[likelihood_ratio_left, likelihood_ratio_left]`, with precision 1e-3. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "52890836",
   "metadata": {
    "tags": [
     "py",
     "learner_chopped",
     "draft"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>PROBLEM\n",
    "\n",
    "label_2='data/events_a4_1space.dat'\n",
    "dec_2,ra_2,az_2=load(label_2)\n",
    "\n",
    "#compute asymmetry\n",
    "NLeft_2=0\n",
    "NRight_2=0\n",
    "for i0 in range(len(ra_2)):\n",
    "    if ra_2[i0] < 0:\n",
    "        NLeft_2+=1\n",
    "    else:\n",
    "        NRight_2+=1\n",
    "print(\"NLeft:\",NLeft_2,\"+/-\",math.sqrt(NLeft_2),\"NRight:\",NRight_2,\"+/-\",math.sqrt(NRight_2),\"total/2\",len(ra_2)/2.)\n",
    "\n",
    "plotHist(ra_2,2)\n",
    "\n",
    "\n",
    "#PRINT THE COUNTS WITH UNCERTAINTY\n",
    "\n",
    "#PRINT THE PROBABILITY (LIKELIHOOD RATIO)\n",
    "pvalues(NLeft_2,NRight_2)\n",
    "\n",
    "#PLOT IT"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "28d7d5b7",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "tags": [
     "py",
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "label_2='data/events_a4_1space.dat'\n",
    "dec_2,ra_2,az_2=load(label_2)\n",
    "\n",
    "#compute asymmetry\n",
    "NLeft_2=0\n",
    "NRight_2=0\n",
    "for i0 in range(len(ra_2)):\n",
    "    if ra_2[i0] < 0:\n",
    "        NLeft_2+=1\n",
    "    else:\n",
    "        NRight_2+=1\n",
    "\n",
    "print(\"NLeft:\",NLeft_2,\"+/-\",math.sqrt(NLeft_2),\"NRight:\",NRight_2,\"+/-\",math.sqrt(NRight_2),\"total/2\",len(ra_2)/2.)\n",
    "pvalues(NLeft_2,NRight_2)\n",
    "\n",
    "#plot it\n",
    "plotHist(ra_2)\n",
    "plotHist(ra_2,2)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c824ee68",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[0.9994003979088298, 0.9992170135352566]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Follow the analysis that was done in the lesson. The likelihood ratios are both very close to 1.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c64d0815",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.7 Log-Likelihood and Chi-square</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_6) | [Exercises](#exercises_5_7) | [Next Section](#section_5_8) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bed8fdb",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_07.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "46cfdcd9",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_07.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb88501b",
   "metadata": {
    "tags": [
     "md",
     "lect_07"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L05/slides_L05_07.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L05/slides_L05_07.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30c45ee0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "<h3>Interpreting Likelihood</h3>\n",
    "\n",
    "In the above, we compared the likelihood of two distributions from a flat hypothesis. We did this by considering the likelihood of a deviation from flat. We see that lower energy particles are pretty close to flat, whereas higher energy particles start to deviate from flat. As a physicist, this is when you start to think WTF. Let's talk about the physics implications later, but for now let's understand the statistics a little more. Let's ask a profound question. How can we improve the sensitivity of this measurement? \n",
    "\n",
    "**Use more than just the left and right sides of the distribution.**\n",
    "\n",
    "To extend the sensitivity, what we can do next is to define a likelihood for an arbitrary distribution. We can define the likelihood over a number of points by imagining that instead of just left and right, each bin is a specific measurement that we perform $N^{events}_{bin}$ times. From this, we can consider the probability of each bin given an expected mean prediction for that bin. For $N$ bins where for bin $i$ we have $x_{i}$ number of events and having the same mean prediction, we can write the likelihood as the multiplication of $N$ Poisson experiments each with the same predicted number of events. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} p(x_{i}|\\lambda) \\\\\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} \\frac{\\lambda^{x_{i}}}{x_{i}!}e^{-\\lambda} \\\\\n",
    "\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i-1}^{N}  x_{i}\\log(\\lambda)-\\log(x_{i}!)-\\lambda\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If we want to optimize this for a specific $\\lambda$, then we just take the derivative\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{d}{d\\lambda}\\log(\\mathcal{L}(x|\\lambda))&=&0=\\sum_{i-1}^{N}  \\frac{x_{i}}{\\lambda}-1 \\\\\n",
    " N\\lambda&=&\\sum_{i-1}^{N} x_{i} \\\\\n",
    " \\lambda=\\bar{x}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Surprise, surprise! The optimal value $\\lambda$ is just the average over all the bins.\n",
    "\n",
    "Moreover, we can compute the variance of $\\lambda$ from its definition. In this case we get \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{\\lambda}^2=\\mathrm{Var}\\left(\\frac{1}{N}\\sum_{i=1}^{N} x_{i}\\right)\\\\\n",
    "\\sigma_{\\lambda}^2=\\frac{1}{N^2}\\sum^{N}_{i=1}\\mathrm{Var}(x_{i}) \\\\\n",
    "\\sigma_{\\lambda}^2=\\frac{1}{N^2}N\\bar{x}^2=\\frac{\\bar{x}^2}{N}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's compute the likelihood of the above distributions using all of the bins."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2276fadd",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-runcell00\n",
    "\n",
    "#For the purposes of this section, we will use the low energy data.\n",
    "#Load it here, where we will redefine dec, ra, and az\n",
    "\n",
    "label_2='data/events_a4_1space.dat'\n",
    "dec,ra,az=load(label_2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2b442bc9",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.7-runcell01\n",
    "_,xis=plotHist(ra)\n",
    "plt.axhline(np.mean(xis),c='r')\n",
    "_,xis=plotHist(ra)\n",
    "\n",
    "#Now let's define the log of a Poisson distribution (see above)\n",
    "def logpoisson(lamb): #x is our lambda and y0 is our data\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        test = math.factorial(xi)\n",
    "        lTot += xi*np.log(lamb) - lamb  - math.log(test) \n",
    "    return -1.*lTot\n",
    "\n",
    "#Now lets take the mean of this distribution and compute labmda\n",
    "lamb=xis.mean()\n",
    "print(\"Log Likelihood\",logpoisson(lamb),\"Regular Likelihood\",np.exp(logpoisson(lamb)))\n",
    "x = np.linspace(lamb*0.75, lamb*1.55, 100)\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.plot(x, logpoisson(x));\n",
    "\n",
    "#finally let's compute the minimum of this distribution\n",
    "#from scipy import optimize as opt\n",
    "#sol=opt.minimize_scalar(logpoisson, method='Brent')\n",
    "#print(\"minimum found:\",sol,\"Mean:\",lamb.mean())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c625f7da",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "By minimizing the likelihood we are finding the optimum position of our parameters. Note that the likelihood is some giant number, but the log likelihood is not large (this is because we are multiplying a lot of small probability numbers). This situation isn't so unusual, so perhaps you can see why we prefer working with log likelihood!\n",
    "\n",
    "Now, let's go back to our test. We would like to test this variation in the data and see how consistent each bin is with a Poisson distribution. For this to be the case, our absolute likelihood value has to be a reasonable number. So what is a reasonable number?\n",
    "\n",
    "Here is where we have to rely on some clever trickery. What we are going to do is invoke the central limit theorem, and state that since we are dealing with large numbers, the distribution of our sample about its expectation is going to be Gaussian with standard deviation given for a Poisson distribution. Then, we can compute the log likelihood. Let's do it for each of our bins. \n",
    " \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} p(x_{i}|\\lambda) \\\\\n",
    "\\mathcal{L}(x|\\lambda)&=&\\prod_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\sigma^2}}e^{-\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}} \\\\\n",
    "-\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i=1}^{N}  -\\frac{1}{2}\\log(2\\pi\\sigma^2)+\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The above is for $N$ Gaussian distributed bins. For $N$ Poisson distributed bins we can approximate it as a Gaussian with mean $\\lambda$ and $\\sigma=\\lambda$, and so we can write\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\mathcal{L}(x|\\lambda)&=&\\Pi_{i=1}^{N} \\frac{1}{\\sqrt{2\\pi\\lambda}}e^{\\frac{-(x_{i}-\\lambda)^2}{2\\lambda}} \\\\\n",
    "-\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i=1}^{N}  -\\frac{1}{2}\\log(2\\pi\\lambda)+\\frac{(x_{i}-\\lambda)^2}{2\\lambda}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Recall from above using the Poisson form we had\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\log(\\mathcal{L}(x|\\lambda))&=&\\sum_{i=1}^{N}  x_{i}\\log(\\lambda)-\\log(x_{i}!)-\\lambda\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "If we equate the two we see they are a bit different, and as a consequence our likelihood value will be different. Furthermore there is something else surprising and interesting about this. Let's look at just one term $N=1$ (not just for Poisson).\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "f(x|\\lambda) &=&-\\frac{1}{2}\\log(2\\pi\\sigma^2)+\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "E[f(x)]      &=&-\\frac{1}{2}\\log(2\\pi\\sigma^2)+E[g^{\\prime}(x)]\\\\\n",
    "\\mathrm{where~}g^{\\prime}(x)         &=&\\frac{(x_{i}-\\mu)^2}{2\\sigma^2}\\\\\n",
    "E[g^{\\prime}(x)]      &=&\\frac{\\mathrm{Var}(x)}{2\\sigma^2}\\\\\n",
    "E[g^{\\prime}(x)]      &=&\\frac{\\sigma^2}{2\\sigma^2}=1/2\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "We have focused in on this one parameter $g^{\\prime}(x)$ since the other parameter is just a constant calculated from the parameters of the Gaussian distribution. Since we have assumed $x_{i}$ are each Gaussian distributed with mean $\\mu$, we get the expectation for this distribution is zero and the variance is $\\sigma$, yielding 1/2 when considering $g(x)$ over the data. As a consequence we have that $\\log$ of the likelihood using a gaussian will be $\\frac{1}{2}-\\frac{1}{2}\\log(2\\pi\\sigma^2)$. \n",
    "\n",
    "\n",
    "Since the 1st term doesn't affect the optimization it is often dropped. We can also multiply by 2 to make things look nice, again with no effect. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g(x) &=& \\frac{(x_{i}-\\mu)^2}{\\sigma^2}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "For variables which are truly Gaussian distributed, we expect this to be 1. Furthermore, if we sum over many of these variables we get: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "g(x) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-\\mu)^2}{\\sigma^2}\\\\\n",
    "E[g(x)] & = & N\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We then expect that for $N$ Gaussian distributed variables this will give us a value of $N$. This expectation allows us to actually test the data. If we run over our data and compute $E[g(x)]$ and it's too small ($\\ll N$), then our value for $\\sigma$ is too large. If our value for $E[g(x)]$ is too large ($\\gg N$) then we don't have a good fit to the data and our value for $\\sigma$ is too small. \n",
    "\n",
    "Furthermore, the sum of $N$ independent Gaussian random numbers with variance 1 in itself makes its own distribution. This is a $\\chi^{2}$ distribution with $N$ degrees of freedom, and $g(x)$ above is referred to as $\\chi^{2}$. \n",
    "\n",
    "For a Poisson distribution, we can further simplify this to the classic $\\chi^{2}$ that we physicists know and love and copiously use. \n",
    " \n",
    " \n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\chi^{2}(x) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-\\lambda)^2}{\\lambda}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We are going to talk a lot more about $\\chi^{2}$ distributions. However, an important thing to note is that \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E[\\chi^{2}(x)] &\\approx& N \\\\\n",
    "E[\\chi^{2}(x)/N] &\\approx& 1 \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This last relation is known as the normalized $\\chi^{2}$, which we expect to approach $1$ in the large $N$ limit. This is often why fits report the normalized $\\chi^{2}$. \n",
    "\n",
    "Let's now go back to our minimization problem. In the above code, we minimized the Poisson likelihood. Let's now minimize the Gaussian likelihood, and the $\\chi^{2}$ likelihood, and see what we get. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "10efcf85",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_5_7'></a>     \n",
    "\n",
    "| [Top](#section_5_0) | [Restart Section](#section_5_7) | [Next Section](#section_5_8) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5dec861e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_5_8'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L5.8 Minimizing</h2>  \n",
    "\n",
    "| [Top](#section_5_0) | [Previous Section](#section_5_7) | [Exercises](#exercises_5_8) | [Next Section](#section_5_9) |\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "22613037",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L05/slides_L05_08.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2005cc82",
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.axhline(np.mean(xis),c='r')\n",
    "bins,xis=plotHist(ra)\n",
    "plt.show()\n",
    "print(xis)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "557a82c5",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-runcell01\n",
    "\n",
    "#Log likelihood of a gaussian distribution of our data from above\n",
    "def loggaus(lamb):\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        lTot += (0.5/(lamb+1e-5))*(xi-lamb)**2\n",
    "        lTot -= 0.5*np.log(math.pi*2*lamb)\n",
    "    return lTot\n",
    "\n",
    "#chi2 distribution of our data from above.  \n",
    "def chi2(lamb):\n",
    "    lTot=0\n",
    "    for xi in xis:\n",
    "        lTot += (1./(lamb+1e-10))*(xi-lamb)**2\n",
    "    return lTot\n",
    "\n",
    "\n",
    "lamb=xis.mean()\n",
    "print(\"Poisson Likelihood at minimum\",logpoisson(lamb),lamb,len(xis))\n",
    "print(\"Gaussian Likelihood at minimum\",loggaus(lamb),lamb,len(xis))\n",
    "print(\"chi2 value at minium\",chi2(lamb))\n",
    "\n",
    "x = np.linspace(lamb*0.75, lamb*1.55, 100)\n",
    "plt.plot(x, loggaus(x),label='gaus');\n",
    "plt.plot(x, chi2(x)/2.,label='chi2/2');\n",
    "plt.plot(x, logpoisson(x),label='logpoisson');\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$-\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#Now let's minimize the two\n",
    "#from scipy import optimize as opt\n",
    "#sol0=opt.minimize_scalar(logpoisson, method='Brent')\n",
    "#print(\"Poisson Minimum\",sol0.x)\n",
    "#sol1=opt.minimize_scalar(loggaus, method='Brent')\n",
    "#print(\"Gaussian Minimum\",sol1.x)\n",
    "#sol2=opt.minimize_scalar(chi2, method='Brent')\n",
    "#print(\"Chi2 Minimum\",sol2.x)\n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "331f9fff",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "From the above, we see that we can get to the same minimum with either the full Gaussian or the $\\chi^{2}$ distribution. Now, understanding whether your fit is good or not can be completely interpreted by understanding the $\\chi^{2}$ distribution. The $\\chi^{2}$ is a very powerful distribution that we can compute numerically. We can use python tools to understand it, so let's do just that. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9a4a232e",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L5.8-runcell02\n",
    "\n",
    "#Now let's look at our chi2 distribution and see how this compares\n",
    "x = np.linspace(0,80)\n",
    "chi2d=stats.chi2.pdf(x,30) # w0 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2(lamb), c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"$\\chi^{2}(x)$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4bc4d5e8",
   "metadata": {},
   "source": [
    "Finally, what about our chi2 comparison if we bin the high energy data. How far away are we from a flat distribution? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "abb02443",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#solution\n",
    "label8='data/events_a8_1space.dat'\n",
    "dec,ra,az=load(label8)\n",
    "nbins=30\n",
    "plt.axhline(np.mean(xis),c='r')\n",
    "_,xis=plotHist(ra,nbins)\n",
    "\n",
    "\n",
    "#compute the mean over the bins\n",
    "lamb=xis.mean()\n",
    "\n",
    "lamb=xis.mean()\n",
    "print(\"Poisson Likelihood at minimum\",logpoisson(lamb),lamb,len(xis))\n",
    "print(\"Gaussian Likelihood at minimum\",loggaus(lamb),lamb,len(xis))\n",
    "print(\"chi2 value at minium\",chi2(lamb))\n",
    "\n",
    "x = np.linspace(lamb*0.75, lamb*1.55, 100)\n",
    "plt.plot(x, loggaus(x),label='gaus');\n",
    "plt.plot(x, chi2(x)/2.,label='chi2/2');\n",
    "plt.plot(x, logpoisson(x),label='logpoisson');\n",
    "plt.xlabel(\"$\\lambda$\")\n",
    "plt.ylabel(\"$-\\log(\\mathcal{L}(\\lambda))$\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#Now let's minimize the two\n",
    "sol0=opt.minimize_scalar(logpoisson, method='Brent')\n",
    "print(\"Poisson Minimum\",sol0.x,'-true value--',lamb.mean())\n",
    "sol1=opt.minimize_scalar(loggaus, method='Brent')\n",
    "print(\"Gaussian Minimum\",sol1.x,'-true value--',lamb.mean())\n",
    "sol2=opt.minimize_scalar(chi2, method='Brent')\n",
    "print(\"Chi2 Minimum\",sol2.x,'-true value--',lamb.mean())\n",
    "\n",
    "#Now let's look at our chi2 distribution and see how this compares\n",
    "x = np.linspace(0,80)\n",
    "chi2d=stats.chi2.pdf(x,nbins) # 30 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2(lamb), c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n",
    "print(\"pvalue\",1-stats.chi2.cdf(chi2(lamb),nbins))\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9263f8b0",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
