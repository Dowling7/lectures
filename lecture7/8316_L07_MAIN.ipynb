{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090cb21a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.316 Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 7: Correlations</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82515af8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cd2d2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing on a Surface (1D Scan)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing on a Surface (2D Scan)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783f1b7",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "In this lecture, we wil exploret the nature of correlated variables and how they impact our interpretation of results. Furthermore, we will understand how to go away from correlations to see how we can reduce their impact. Additionally, we will start to scan the parameters of our fit beyond just finding the minimum. By scanning, we start to learn the subtleties of each of these setups, and we start to get deep insight into the nature of what we are minimizing and how to interpret it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ec6d3",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cells below to import the relevant libraries for this notebook. If you're using a Colab cloud runtime, also run the cell labeled \"for Colab users.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c45e6",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4182ec5",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell02\n",
    "\n",
    "# for Colab users\n",
    "!pip install lmfit\n",
    "\n",
    "# Get data and download to root of your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# If you're not comfortable giving permissions for this command, download the file from the URL, \n",
    "# then upload to your Google Drive root folder.\n",
    "!wget -P /content/drive/MyDrive/ https://raw.githubusercontent.com/mitx-8s50/data/main/L07/events_a8_1space.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305ef1b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell02 (ANOTHER WAY?)\n",
    "\n",
    "# for Colab users\n",
    "!pip install lmfit\n",
    "\n",
    "# Downloading data for your runtime\n",
    "!cd /\n",
    "!wget https://raw.githubusercontent.com/mitx-8s50/data/main/L07/events_a8_1space.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ca505",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc09dcc",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell03\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "27250c45",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Fit to Full Cosmological Model</h2>     \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4764cc42",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "<h3>A more sophisticated fit</h3>\n",
    "\n",
    "Now that we have gone on an excursion to understand properties of fits, let's go ahead and analyze our supernovae data, and try to pull in all of the information that we can. Let's first look at our linear fit.  One sec, while we load it all:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "25a790ca",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.6-runcell01\n",
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#Let's try to understand how good the fits we made in last Lesson are, let's load the supernova data again\n",
    "label='data/sn_z_mu_dmu_plow_union2.1.txt'\n",
    "\n",
    "def distanceconv(iMu):\n",
    "    power=iMu/5+1\n",
    "    return 10**power\n",
    "\n",
    "def distanceconverr(iMu,iMuErr):\n",
    "    power=iMu/5+1\n",
    "    const=math.log(10)/5.\n",
    "    return const*(10**power)*iMuErr\n",
    "\n",
    "def load(iLabel,iMaxZ=0.1):\n",
    "    redshift=np.array([])\n",
    "    distance=np.array([])\n",
    "    distance_err=np.array([])\n",
    "    with open(iLabel,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in plots:\n",
    "            if float(row[1]) > iMaxZ:\n",
    "                continue\n",
    "            redshift = np.append(redshift,float(row[1]))\n",
    "            distance = np.append(distance,distanceconv(float(row[2])))\n",
    "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n",
    "    return redshift,distance,distance_err  \n",
    "        \n",
    "redshift,distance,distance_err=load(label,10000)\n",
    "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.xlabel(\"z(redshift)\")\n",
    "plt.ylabel(\"distance(pc)\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "154e3402",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.7-runcell02\n",
    "\n",
    "#We are not going to plot the fit first, let's just use our barrage of statistics to check if its ok\n",
    "def sqrtTerm(z,Om):\n",
    "    pVal=Om*(1+z)**3+(1.-Om)\n",
    "    return np.sqrt(pVal)\n",
    "\n",
    "def lumidistance(z,h0,Om):\n",
    "    integral=0\n",
    "    nint=100\n",
    "    for i0 in range(nint):\n",
    "        zp=z*float(i0)/100.\n",
    "        dz=z/float(nint)\n",
    "        pVal=1./(1e-5+sqrtTerm(zp,Om))###<=== note the 1e-5 hack\n",
    "        integral += pVal*dz\n",
    "    d=(1.+z)*integral*(1e6*3e5/h0)\n",
    "    return d\n",
    "\n",
    "print(\"test Lumi\",lumidistance(1,70,0.3))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "20d88d1c",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<a name='exercises_6_6'></a>     \n",
    "\n",
    "| [Top](#section_6_0) | [Restart Section](#section_6_6) | [Next Section](#section_6_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c0ea7cef",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "Now let's go and run our fit function that included the parameters of the universe. As a first pass, let's just pipe this through lmfit and see if it works. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "933fb849",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.8-runcell01\n",
    "\n",
    "import lmfit\n",
    "\n",
    "model  = lmfit.Model(lumidistance)\n",
    "p = model.make_params(h0=60,Om=0.2)\n",
    "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dab8f1eb",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "Now, since we are experts at fitting, let's go ahead and compute the log likelihood . We are going to minimize the (negative of) loglikelihood with `scipy.optimize` in order to do the fit."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb12ec03",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.8-runcell02\n",
    "from scipy import optimize as opt \n",
    "\n",
    "def chi2(x):\n",
    "    xtest=lumidistance(redshift,x[0],x[1])\n",
    "    val=(distance-xtest)/distance_err\n",
    "    return np.sum(val**2)\n",
    "    \n",
    "def residuals(x):\n",
    "    residuals=np.array([])\n",
    "    for i0 in range(len(redshift)):\n",
    "        pResid=lumidistance(redshift[i0],sol.x[0],sol.x[1])-distance[i0]\n",
    "        residuals = np.append(residuals,pResid/distance_err[i0])\n",
    "    return residuals\n",
    "\n",
    "\n",
    "x0 = np.array([70.,0.3])\n",
    "ps = [x0]\n",
    "bnds = ((0, 1000), (0, 1.0))\n",
    "sol=opt.minimize(chi2, x0,bounds=bnds)#, tol=1e-6)\n",
    "print(sol)\n",
    "residuals=residuals(sol.x)\n",
    "chi2t=np.sum(residuals**2)\n",
    "print(\"Total chi2:\",chi2t,\"NDOF\",len(residuals)-2)\n",
    "print(\"Normalized chi2:\",chi2t/(len(residuals)-2))\n",
    "print(\"Probability of chi2:\",1-stats.chi2.cdf(chi2t,(len(residuals)-2)))\n",
    "\n",
    "#Let's plot it for good measure too\n",
    "x = np.linspace(0,len(residuals)*2)\n",
    "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2t, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "#xvals = np.linspace(0.,20.,100)\n",
    "#yvals = []\n",
    "#for pX in xvals:\n",
    "#    yvals.append(chi2([70,pX]))\n",
    "#plt.plot(xvals,yvals)\n",
    "#plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d017595a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "What can we say about this fit, is there something off?\n",
    "\n",
    "Let's plot the residuals and the fit function and also scan the likelihood for our parameter uncertainties. There is one thing off, can you figure it out?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a1577e93",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.8-runcell03\n",
    "\n",
    "#Plot it against the data\n",
    "xvals = np.linspace(0,1.4,100)\n",
    "yvals = []\n",
    "for pX in xvals:\n",
    "    yvals.append(lumidistance(pX,sol.x[0],sol.x[1]))\n",
    "\n",
    "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.plot(xvals,yvals)\n",
    "plt.show()\n",
    "\n",
    "#Histogram the residuals\n",
    "y0, bin_edges = np.histogram(residuals, bins=30)\n",
    "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "norm0=len(residuals)*(bin_edges[-1]-bin_edges[0])/30.\n",
    "plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,marker='.',drawstyle = 'steps-mid')\n",
    "k=np.arange(bin_edges[0],bin_edges[-1],0.05)\n",
    "normal=stats.norm.pdf(k,0,1)\n",
    "plt.plot(k,normal,'o-')\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(len(residuals)*0.5,len(residuals)*1.5)\n",
    "chi2d=stats.chi2.pdf(x,len(residuals-2)) # 40 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(chi2t, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e927de16",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_6_8'></a>   \n",
    "\n",
    "| [Top](#section_6_0) | [Restart Section](#section_6_8) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818f2a06",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.6.1</span>\n",
    "\n",
    "So given the Friedmann equations, we can add back the curvature term:\n",
    "\n",
    "$$ d(z) = ct^{\\prime} = (1+z)ct = (1+z)\\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}\\left(1+z^{\\prime}\\right)^{3} + \\Omega_{\\kappa}\\left(1+z^{\\prime}\\right)^{2}+ 1-\\Omega_{M}-\\Omega_{\\kappa}}}$$\n",
    "\n",
    "Adjust the `lumidistance()` function to fit for curvature. What is the value of $\\Omega_{\\kappa}$ if we perform a new fit to the data? What do you think this this is implying? \n",
    "\n",
    "Enter your answer as a number with precision 1e-2.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b91a5ef6",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def hubble_curve(z,Om,OmK):\n",
    "    return #YOUR CODE HERE\n",
    "\n",
    "def lumidistance_curve(z,h0,Om,OmK):\n",
    "    return #YOUR CODE HERE\n",
    "\n",
    "\n",
    "model  = lmfit.Model(lumidistance_curve)\n",
    "p = model.make_params(h0=70,Om=0.2,OmK=0.0)\n",
    "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n",
    "lmfit.report_fit(result)\n",
    "result.plot();\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0a4728b9",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "def hubble_curve(z,Om,OmK):\n",
    "    pVal=Om*(1+z)**3+OmK*(1+z)**2+(1.-Om-OmK)\n",
    "    return np.sqrt(pVal)\n",
    "\n",
    "def lumidistance_curve(z,h0,Om,OmK):\n",
    "    integral=0\n",
    "    nint=100\n",
    "    for i0 in range(nint):\n",
    "        zp=z*float(i0)/100.\n",
    "        dz=z/float(nint)\n",
    "        pVal=1./(1e-5+hubble_curve(zp,Om,OmK))\n",
    "        integral += pVal*dz\n",
    "    d=(1.+z)*integral*(1e6*3e5/h0)\n",
    "    return d\n",
    "\n",
    "\n",
    "model  = lmfit.Model(lumidistance_curve)\n",
    "p = model.make_params(h0=70,Om=0.2,OmK=0.0)\n",
    "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n",
    "lmfit.report_fit(result)\n",
    "result.plot();"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e86fd67",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "-0.19 +/- 0.40\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "We find the curvature is consistent with 0, with a slight bias towards a negative curvature value. Interestingly, we also find that the Hubble's constant doesn't change as much, but the matter changes some more. However, the overall scale of change is relatively small. Interestingly, the chi2 value is almost exactly the same, implying that adding these parameters does not really change the overall performance. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd21490",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.7 Understanding Best Fit (Revisited)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_6) | [Exercises](#exercises_7_7) | [Next Section](#section_7_8) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a679e9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6daf70",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9c666",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a8628",
   "metadata": {
    "tags": [
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdcea4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Uncertainty on more than one parameter</h3>\n",
    "\n",
    "At the end of last week, we started to fit more than one parameter. That is, we minimized some error metric for more than one parameter. Let's take a closer look. As before, we'll fit the function \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x,a,b) = a + b \\sin(x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "to the Auger data, i.e. finding optimal choices of parameters $a$ and $b$ given some set of sample observations $y_i$ and their sample uncertainties $\\sigma_i$. One way to choose optimal parameter values is by optimizing our likelihood over both $a$ and $b$. As a reminder, likelihood defines  how probable our observed data is as a function of parameter values and a hypothesized form. At extrema, we'd expect our likelihood is maximized or \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\frac{\\partial \\mathcal{L}}{\\partial a} = 0 \\\\\n",
    " \\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As we saw in L5.7, maxima of the likelihood function correspond to minima of the relevant $\\chi^2$ statistic of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\chi^{2}(x|a,b) = \\sum_{i=1}^{N} \\frac{(y_{i}-f(x_{i},a,b))^2}{\\sigma_i^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "At the extrema,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial a} = 0 \\\\\n",
    "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial b} = 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "In a little bit, we're going to use this to manually write code to fit the function using the generic optimizer in `scipy`. But first, let's review how things look using `lmfit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bc5f6",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-runcell01\n",
    "\n",
    "############ This is all code from a previous lesson\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import lmfit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize as opt\n",
    "\n",
    "def rad(iTheta):\n",
    "    return iTheta/180. * math.pi\n",
    "\n",
    "def rad1(iTheta):\n",
    "    return iTheta/180. * math.pi-math.pi\n",
    "\n",
    "def load(label):\n",
    "    dec=np.array([])\n",
    "    ra=np.array([])\n",
    "    az=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile,delimiter=' ')\n",
    "        for pRow in plots:\n",
    "            if '#' in pRow[0] or pRow[0]=='':\n",
    "                continue\n",
    "            dec = np.append(dec,rad(float(pRow[2])))\n",
    "            ra  = np.append(ra,rad1(float(pRow[3])))\n",
    "            az  = np.append(az,rad(float(pRow[4])))\n",
    "    return dec,ra,az\n",
    "\n",
    "def prephist(iRA):\n",
    "    y0, bin_edges = np.histogram(iRA, bins=30)\n",
    "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    y0 = y0.astype('float')\n",
    "    return x0,y0,1./(y0**0.5)\n",
    "\n",
    "label8='data/events_a8_1space.dat'\n",
    "\n",
    "# Uncomment if using cloud runtime (which one?)\n",
    "#label8='/content/drive/MyDrive/events_a8_1space.dat'\n",
    "#label8='/content/events_a8_1space.dat'\n",
    "\n",
    "dec,ra8,az=load(label8)\n",
    "xhist,yhist,xweights=prephist(ra8)\n",
    "\n",
    "\n",
    "########## Tlast fit code\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    return a+b*np.sin(x)\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=10)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262677b7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3> What is the $\\chi^{2}$-value from a p-value? </h3>\n",
    "\n",
    "So what is the importance of this fit? This is just equal to a $\\chi^{2}$ distribution, as we ahve seen before. Before we delve into the details of the importance of the fit, lets just remind ourselves how to compute the p-value from a gaussian and the $\\chi^{2}$ value from the p-value or a sigma value assuming a Gaussian. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06284815",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-runcell02\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "sigma = 1\n",
    "#print(stats.norm.cdf(sigma)-stats.norm.cdf(-sigma))\n",
    "\n",
    "def pval(iVal):\n",
    "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n",
    "\n",
    "def chi2Val(iGausSigma,iNDOF):\n",
    "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n",
    "    return val\n",
    "\n",
    "print(chi2Val(2,28))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d50f61",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_1'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e84a0a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.7.1</span>\n",
    "\n",
    "Compute the Chi-square value for 2 sigma and 2 degrees of freedom? Enter your answer as number with precision 1e-2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca364a97",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def exp_func(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe71584",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def pval(iVal):\n",
    "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n",
    "\n",
    "def chi2Val(iGausSigma,iNDOF):\n",
    "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n",
    "    return val\n",
    "\n",
    "print(chi2Val(2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223fcdf",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "$Χ^{2} = 6.18$\n",
    "\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Use the same function as explained in L7.1 lecture video. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545d620",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_8'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.8 Minimizing on a Surface (1D Scan)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_7) | [Exercises](#exercises_7_8) | [Next Section](#section_7_9) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e038e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, what we are going to do is re-run the fit. Here we'll go through the $\\chi^2$ minimization process semi manually, using `scipy.opt`. The code is explained in the lecture.\n",
    "\n",
    "Strategically, what we are trying to do is write the $\\chi^{2}$ value and use our normal minimizer tools to find the minimum parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171ebcb",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.2-runcell01\n",
    "\n",
    "#This is the new code\n",
    "def chi2(iX): #iX is an array iX[0]=a and iX[1]=b\n",
    "    '''Note that this function depends on xhist and yhist being defined already.\n",
    "    This is bad practice in general, but we do it here for convenience.\n",
    "    After all, this code isn't reused elsewhere.'''\n",
    "    assert len(iX) == 2\n",
    "    lTot=0\n",
    "    for val in range(len(yhist)):\n",
    "        xtest=fnew(xhist[val],iX[0],iX[1])\n",
    "        lTot += (1./(1e-5+xtest))*(yhist[val]-xtest)**2\n",
    "    return lTot\n",
    "\n",
    "#First we minimize\n",
    "x0 = np.array([1000,10]) # initial conditions\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7b26f",
   "metadata": {},
   "source": [
    "Now, given that we have found the minimum, lets look at how the $\\chi^{2}$ value changes from the minimum. The important thing is to remember that from [Wilk's Thereom](https://en.wikipedia.org/wiki/Wilks%27_theorem) the uncertainty of a parameter can be obtained by tkaing $\\Delta \\chi^{2}$ from the minimum of a value of 1. Lets go ahead and plot the minimum and look at what $\\Delta \\chi^{2}=1$ looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefadb2",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.2-runcell02\n",
    "\n",
    "#Scan near the minimum of each value\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n",
    "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n",
    "\n",
    "#Now lets fix one parameter at the minimum, and profile the other\n",
    "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"a-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()\n",
    "\n",
    "#Now for the other parameter\n",
    "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"b-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15952b19",
   "metadata": {},
   "source": [
    "Where the red and blue lines cross correspnods to the one $\\sigma$ fluctation up and down from the minimum. As a result, we can code up this algorithm and compute the best fit and the uncertainty on the best fit parameter. Let's go ahead and code this up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77713d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Now lets use a numerical solver to find the points at which a function crosses zero (root solver)\n",
    "def chi2minX(xval, delta_chi2=1):\n",
    "    val=chi2([xval,sol.x[1]])\n",
    "    minval=chi2(sol.x) + delta_chi2\n",
    "    return val-minval\n",
    "\n",
    "def chi2minY(yval, delta_chi2=1):\n",
    "    val=chi2([sol.x[0],yval])\n",
    "    minval=chi2(sol.x) + delta_chi2\n",
    "    return val-minval\n",
    "\n",
    "def chi2uncX(sol):\n",
    "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n",
    "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n",
    "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n",
    "    print(\"Reminder the Poisson uncertainty would be:\",math.sqrt(np.mean(yhist)/(len(xhist))))\n",
    "    return solX1, solX2\n",
    "\n",
    "def chi2uncY(sol):\n",
    "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n",
    "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n",
    "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n",
    "    return solY1, solY2\n",
    "\n",
    "solX1, solX2 = chi2uncX(sol)\n",
    "solY1, solY2 = chi2uncY(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5fec8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "So what have we done? \n",
    "\n",
    "We've used the optimizer to minimize the $\\chi^{2}$ values in 2D (a,b), and we have obtained an uncertainty by looking at $\\Delta \\chi^{2}$. You can see that our semi manual manipulation got us to the same parameter estimates as the `lmfit` example. Additionally, by looking at slices of $\\chi^{2}$ in each of the parameters we obtained the same uncertainties. \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e2f50",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_2'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a88c0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.8.1</span>\n",
    "\n",
    "What are the 2$\\sigma$ bounds (that is, 95.45% confidence interval values) of $a$ and $b$ in the fit above? Enter your answer as a list of numbers number with precision 1 (the nearest whole number): `[a_lower, a_upper, b_lower, b_upper]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab0b9b",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "\n",
    "#For a given parameter, assume everything else is determined, so we're\n",
    "# working with 1 degree of freedom.\n",
    "# First, we want to know the chi-square value at which 2 standard deivations (95.45% of the probability)\n",
    "# in the chi-square distribution is for values less extreme than that value.\n",
    "ndof=1\n",
    "pval=0.9545\n",
    "val = stats.chi2.ppf(pval,ndof)\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minXfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "def minYfunc(x):\n",
    "    return chi2minY(x, delta_chi2=val)\n",
    "\n",
    "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n",
    " #insert code here\n",
    " return valmin,valmax\n",
    "\n",
    "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n",
    "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n",
    "print(f\"b bounds: [{b1}, {b2}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b18f81",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "from scipy.optimize.zeros import RootResults\n",
    "from scipy.optimize.optimize import OptimizeResult\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# For a given parameter, assume everything else is determined, so we're\n",
    "# working with 1 degree of freedom.\n",
    "# First, we want to know the chi-square value at which 95.45% of the probability\n",
    "# in the chi-square distribution is for values less extreme than that value.\n",
    "# We do this as before:\n",
    "ndof=1\n",
    "pval=0.9545\n",
    "val = stats.chi2.ppf(pval,ndof)\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "# Now, we independently perturb a or b from their minimized chi-square values\n",
    "# until this Delta chi-square value is achieved, just like above.\n",
    "\n",
    "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n",
    "    '''Does what chi2uncX and chi2uncY above do, with unc_prop_guess defining the\n",
    "    proportional size of search space (e.g. 0.2 in chi2uncY, or 0.02 in chi2uncX)\n",
    "    However, returns the parameter values where the delta chi2 is reached'''\n",
    "    solX1=opt.root_scalar(min_func, bracket=[sol.x[sol_index], sol.x[sol_index]*(1+unc_prop_guess)], method='brentq')\n",
    "    solX2=opt.root_scalar(min_func, bracket=[sol.x[sol_index]*(1-unc_prop_guess), sol.x[sol_index]], method='brentq')\n",
    "    return solX1.root, solX2.root\n",
    "\n",
    "def minXfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "def minYfunc(x):\n",
    "    return chi2minY(x, delta_chi2=val)\n",
    "\n",
    "# Here we set our delta_chi2 to the value in question, and guess how far away\n",
    "# the values of a and b will be to set unc_prop_guess (if not big enough, error)\n",
    "# then let the optimizer find the corresponding a and b uncertainties\n",
    "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n",
    "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n",
    "print(f\"b bounds: [{b1}, {b2}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd0b90",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "a in roughly [1061, 1085] and b in roughly [-67, -33]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "We've assumed all but one parameter is fully determined, so we have 1 degree of freedom. For sake of explanation, assume it's $a$.\n",
    "\n",
    "We're looking for 2-sigma (95.45%) confidence interval bounds, that is, the values of $a$ for which $\\Delta \\chi^2$ is such that in a 1 DoF $\\chi^2$ distribution, 95% of $\\chi^2$ values are less extreme.\n",
    "From the inverse of the CDF, we find this $\\Delta \\chi^2$ is 4.\n",
    "\n",
    "Doing root finding to figure out how far we need to move $a$ or $b$ to achieve this increase in $\\chi^2$, we find the values we're looking for.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da90e3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_9'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.9 Minimizing on a Surface (2D Scan)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_8) | [Exercises](#exercises_7_9) | [Next Section](#section_7_10) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a163991",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "Now given our minimization worked in 1D for both variables $a$ and $b$, what about if we scan both variables simultaneously and then look at how the minimum behaves. To do this, we can start by first scaning around the best fit values of $a$ and $b$ and just plotting the value of our $\\chi^{2}$ minimization. From this space, we can start to relate this back to the 2D version of Wilk's theorem in the slides below. \n",
    "\n",
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aee4c4",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd5113",
   "metadata": {
    "tags": [
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffb143",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "To really visualize the whole thing lets make one more plot: how the $\\chi^2$ value depends on both parameters. We can do this using the `meshgrid` function and plotting it. In addition, we will add some color full lines for a $\\Delta \\chi^{2}$ given by various values, we will revisit what these values are in a little bit.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73905b8d",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-runcell01\n",
    "\n",
    "#define the 2D X and Y grid\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in a\n",
    "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in b\n",
    "X, Y = np.meshgrid(x, y) #2d grid\n",
    "#print(x[0],y[0],X[1,0],Y[1,0])\n",
    "\n",
    "# For z coordinate, evaluate chi2 at each x,y point in the grid.\n",
    "# note understanding this one-liner itself isn't too important\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "\n",
    "\n",
    "#and plot\n",
    "def plotColorsAndContours(X,Y,Z):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "    cb=fig.colorbar(c, ax=ax)\n",
    "    plt.xlabel(\"a\")\n",
    "    plt.ylabel(\"b\")\n",
    "    cb.set_label(\"$\\chi^{2}$\")\n",
    "    #Now lets plot the contours of Delta chi^2\n",
    "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    for i0 in range(len(levels)):\n",
    "        levels[i0] = levels[i0]+sol.fun\n",
    "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "    #plt.show()\n",
    "    \n",
    "plotColorsAndContours(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa7601",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "On the above we've plotted level curves representing different $\\chi^2$ increases from the minimum. In the last section, we obtained uncertainties for our fit parameters by allowing just 1 varaible to vary, then observing the $\\Delta\\chi^2$. We may ask, what is the uncertainty profile when we are letting 2 float simultaneously?\n",
    "\n",
    "Lets go back to our lecture slides' where we have the Taylor expansion result that led to the relation of Wilk's theorem. Now, we can write the expansion in terms of all variables $\\vec{\\theta}$:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can write this out in 2D as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\theta_{a}-\\theta_{a-min} & \\theta_{b}-\\theta_{b-min}\\end{array}\\right)\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\theta_{a}-\\theta_{a-min}\\\\\n",
    "\\theta_{b}-\\theta_{b-min}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the case where $\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\approx0$ we can simplify this approximation a lot.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\Delta\\theta_{a} & \\Delta\\theta_{b}\\end{array}\\right)\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\Delta\\theta_{a}\\\\\n",
    "\\Delta\\theta_{b}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This all becomes a 2D quadratic equation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\chi^{2}(x,\\vec{\\theta}) & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\Delta\\theta_{a}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}}+ & \\Delta\\theta_{b}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\\end{array}\\right)\\\\\n",
    " & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\left(\\begin{array}{cc}\n",
    "\\frac{\\Delta\\theta_{a}^{2}}{\\sigma_{\\theta_{a}}^{2}}+ & \\frac{\\Delta\\theta_{b}^{2}}{\\sigma_{\\theta_{b}}^{2}}\\end{array}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "I want to point out, we are now profiling two parameters at once in this 2D plot, which means the contribution to $\\chi^{2}$ involves 2 degrees of freedom. You can see this from the above equation, since its the sum of 2 gaussian distributed variables with width $1$ and mean $0$. The 1 $\\sigma$ confidence interval for 2 degrees of freedom is computed by taking $\\Delta \\chi^2(x,\\nu=2)=x~\\mathrm{where~}\\mathrm{cdf}\\left(\\chi^{2}(x,2)=0.683\\right)\\approx2.3$. This is the yellow contour. Contrast this with the case of 1 $\\sigma$ confidence on 1 degree of freedom, where $\\Delta\\chi^2 = 1$. \n",
    "\n",
    "Now, we can go one step further, and note that the above formula  $ a x^2 + b y^2 = c$ is just the form of an ellipse with a specific major and minor axis. As a result, we can define a 3D funciton given by \n",
    "\n",
    "$$ f(x,y) = \\left(\\frac{x-\\bar{a}}{\\sigma_{a}}\\right)^{2} + \\left(\\frac{y-\\bar{b}}{\\sigma_{b}}\\right)^{2} $$\n",
    "\n",
    "Anyway, let's plot the ellipses and compare it to our minimum contours. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04789bbb",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-runcell02\n",
    "\n",
    "#Lets plot the uncertainties  from hess_inv\n",
    "print(np.sqrt(2*sol.hess_inv))\n",
    "#the diagonals are approximately the errors\n",
    "\n",
    "#Make a the expression in the above equation x and x0 are 2 vectors\n",
    "def quadratic2D(x,x0,sigma0):\n",
    "    lVals=x-x0\n",
    "    lVals=(lVals**2)/(sigma0)/sigma0\n",
    "    return np.sum(lVals)\n",
    "\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "\n",
    "#Now plot the ellipse in 3D\n",
    "def plotEllipse(sigx,sigy):\n",
    "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n",
    "\n",
    "sigx=(solX2.root-solX1.root)/2.\n",
    "sigy=(solY2.root-solY1.root)/2.\n",
    "plotEllipse(sigx,sigy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f50cb",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Remarkably, our space, give by the formula above, yields the solid lines, which matches very closely to the dashed contours obtained by just writing the $\\chi^{2}$ value. This is a profound statement, it really is. \n",
    "\n",
    "Alright, since this gives us the same yellow line for our 2-variable 1$\\sigma$ confidence ellipse. If you look close you do see a difference. This makes us beg the question of what happens when $\\frac{\\partial^{2}\\chi^{2}}{\\partial \\theta_{a}\\theta_{b}}\\neq0$. We'll see soon, when we look at correlations on fitting a different set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afdca9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_3'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c1eae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.9.1</span>\n",
    "\n",
    "When we allow 2 parameters to vary, for a given confidence level (e.g. 1$\\sigma$) we end up with a confidence ellipse containing parameter values outside the confidence intervals on the individual parameters alone. Compute the 1$\\sigma$ (68.27%) confidence interval bounds for the parameter $a$, based on this ellipse from floating both $a$ and $b$. Enter your answer as a list of number with precision 1 (nearest whole number): `[a_lower, a_upper]`\n",
    "\n",
    "Why do we typically quote the uncertainty from the 1D variation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307e5b",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# We can follow the same procedure for finding the confidence interval based on 1D\n",
    "# but for 1-sigma we now use 2.3 \n",
    "# Use the code from the solution to Ex. 7.2.1,\n",
    "ndof=insert value here\n",
    "pval=0.6827#1 sigma p-value\n",
    "val = stats.chi2.ppf(pval,ndof) # 2 DoF this time!\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2152f",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "# We can follow the same procedure for finding the confidence interval based on 1D\n",
    "# but for 1-sigma we now use 2.3 instead of 1 for delta chi^2, since we have 2 DoF.\n",
    "# Using the code from the solution to Ex. 7.2.1,\n",
    "val = stats.chi2.ppf(0.6827,2) # 2 DoF this time!\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "a1, a2 = chi2unc(sol, 0, minfunc, 0.2)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa7ebc",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "a in roughly [1064, 1082]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Here we find the estimate on $a$ to have uncertainty around $\\pm 9$, as opposed to around $\\pm 6$ when we had only $a$ vary. For consistency, we usually quote the uncertainty on just one variable at a time.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199b6b8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_10'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.10 Correlations Between Fit Parameters: Part 1</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_9) | [Exercises](#exercises_7_10) | [Next Section](#section_7_11) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8b5a3",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "Ok, so now lets try to understand what happens when our off diagonal terms are off. Let's go back to Wilk's theorem and look at the Hessian for the $\\chi^{2}$, but now with non-zero off-diagonal parameters. \n",
    "\n",
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dc3b1",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.4-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7997c",
   "metadata": {
    "tags": [
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b152d6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "To understand the point of this, what we are going to do is make the 2D contour plot like the one above with the same contours as before, but now we will use a new function, that is just a reparametrization of the old one. This function will be\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = a x + b (1-x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This means that the values of $a$ and $b$ now have different meaning, but this is just a linear fit like before and it should give the same overall $\\chi^{2}$ value as before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139b15e",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.4-runcell01\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b*(1-x)\n",
    "    return a*x+pVal\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=1000)\n",
    "\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([1000,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "\n",
    "#Now lets fix one parameter at the minimum, and profile the other\n",
    "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"a-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()\n",
    "\n",
    "#Now for the other parameter\n",
    "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"b-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bff149",
   "metadata": {},
   "source": [
    "Now, critically lets now make the same ellipse plot with our new best fit values for $a$ and $b$, an dour new uncertainties. Additionally, we can also do the 1D uncertainty scans like we did above again, to see how what uncertainties we get on $a$ and $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotColorsAndContours(X,Y,Z)\n",
    "solX1, solX2 = chi2uncX(sol)\n",
    "solY1, solY2 = chi2uncY(sol)\n",
    "sigx=(solX2.root-solX1.root)/2.\n",
    "sigy=(solY2.root-solY1.root)/2.\n",
    "plotEllipse(sigx,sigy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b09e66",
   "metadata": {},
   "source": [
    "What we can see is that our ellipse differ pretty dramatically from above. Also, we see that our 1D uncertainties are very mis-estimated from what lmfit gives us. The reason for this is that `lmfit` takes into account the full correlation of the variables. Whereas just doing a 1D scan is equivalent to just moving on a single line along the $y$ and $x$ axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632189a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_4'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bdd8e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.10.1</span>\n",
    "\n",
    "Run the above fit with just a regular linear fit given by $f(x) = ax + b$. How does the $\\chi^{2}$ change? How do the correlations between the variables change? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae36ff3",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570965",
   "metadata": {
    "scrolled": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b\n",
    "    return -1*a*x+pVal\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=0)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([-20,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.5,sol.x[0]*1.5, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06786d7b",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "$\\chi^{2}$ is to within numerical variations the same. The correlation goes down to almost zero. \n",
    "\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "By reparametrizing the fit, we ahve allowed the function to be equally as expressive as the above funciton. However, we have changed the roles of $a$ and $b$ in the fit. With this new fit $b$ is in charge of getting the overall scale and $a$ is in charge of getting the trend over $\\theta$. Before we had both parameters were in charge of both scale and a combination of slope. Overall our fit has the same shape in the end, but the meaning of $a$ and $b$ is quite different and hence, the off-diagonal terms don't play a big role here. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6eb66",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_11'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.11 Correlations Between Fit Parameters: Part 2</h2>     \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_10) | [Exercises](#exercises_7_11) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503280a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now if we go back to our original correlated parameterization, we see that the 1D profile method for obtaining the uncertianties is not the same as the uncertainties we get from the fit.  This is clearly a problem, how can we address this issue? \n",
    "\n",
    "<h3>Dealing with Correlated uncertainties</h3>\n",
    "\n",
    "Looking at our parameters, our uncertainty estimate is smaller than what we observed above. The uncertainties quoted now differ from what we got by varying from the $\\chi^{2}$ minimum. What we are doing is moving up and down the 2D plot and looking at $\\Delta \\chi^{2}$. However, given the parameters are strongly correlated, you see that this doesn't capture the true uncertainty in the sense that we can still move further along $x$ and $y$ and still be within the yellow or even blue ellipse. It's quite clear when you overlay the uncertainty from the quadratic function, which draws circles not ellipses. \n",
    "\n",
    "What, then, is the right uncertainty? \n",
    "\n",
    "Notice our fit with `lmfit` outputs a parameter $C(a,b)=0.872$. This is in fact the correlation between the parameters of $a$ and $b$. We also see from our optimization function that we get something labelled as the `hess_inv`. This we can write noting the relation of the $\\chi^{2}$ Hessian and uncertainties as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which allows us to write the uncertainties as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\frac{2}{\\sigma^{2}}=\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}} \\\\\n",
    " \\sigma^{2}    = 2\\left(\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}}\\right)^{-1} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now in the case where the off diagonals of the Hessian are zero, the parameters were uncorrelated, so \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\rightarrow\\left(\\begin{array}{cc}\n",
    "\\frac{2}{\\sigma_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{2}{\\sigma_{b}^{2}}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But here we have something more complicated. However this is a natural way to define correlations. Lets first verify our intuition for our minimization scheme by taking our $2x2$ Hessian metrix and diagonalizing, computing the eigenvectors and the eigenvalues. We can diagonalize the matrix as: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A^{-1}2\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)^{-1}A=\\left(\\begin{array}{cc}\n",
    "\\sigma_{1}^{2} & 0\\\\\n",
    "0 & \\sigma_{2}^{2}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is important to note for any N dimensional Hessian, provided the determinant is not zero, we can find a basis of independent variables that are not correlated. That is to say we can always diagonalize our Hessian, and the eigenvectors of our Hessian are the independent values with variances given by the eigenvalues. These are our true uncertainty values (eigenvalues) and direction of variation (eigenvector).  \n",
    "\n",
    "When we run our minimizer, we get Hessian inverse, which we can play with. Let's go ahead and look at the Hessian from our minimizer. From the Hessian, we can import numpy's linear algebra toolkit and compute the eigen vectors and values. Finally, we can draw these vectors and values on our 2D scan plot above. Let's do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bcfc3",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell01\n",
    "\n",
    "print(np.sqrt(2*sol.hess_inv))\n",
    "#The diagonals are the uncertainty lmfit quotes\n",
    "\n",
    "#Really the best way to do this is to get the eigen values using an linear algebra problem\n",
    "import numpy.linalg as la\n",
    "w, v=la.eig(2*sol.hess_inv)\n",
    "print(\"values\",w,\"vectors\",v)\n",
    "\n",
    "#Now lets plot the eigenvectors\n",
    "from matplotlib.patches import Ellipse\n",
    "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a matplotlib Ellipse patch representing the covariance matrix\n",
    "    cov centred at centre and scaled by the factor nstd.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find and sort eigenvalues and eigenvectors into descending order\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = eigvals.argsort()[::-1]\n",
    "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "\n",
    "    # The anti-clockwise angle to rotate our ellipse by \n",
    "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n",
    "    theta = np.arctan2(vy, vx)\n",
    "\n",
    "    # Width and height of ellipse to draw\n",
    "    width, height = 2* nstd * np.sqrt(eigvals) #the two here is because its width/height not radius\n",
    "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n",
    "\n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+sol.fun\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cb363",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now the filled in circles make a direct correspondance with the uncertainties above. \n",
    "\n",
    "From the above we can now formulate a description of uncertainties in our system. When we had uncorrelated parameters we had a total uncertainty from our $\\chi^{2}$ given by \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now we have the full ellipse\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2+2\\sigma_{ab}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where we define that $\\sigma_{ab}=\\rm{COV(a,b)}$. There are a number of ways to call this variable, they all are equivalent, but lets be careful to write it out. We can write the error matrix in 2D as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\begin{array}{cc}\n",
    "\\sigma_{a}^{2} & {\\rm COV}(a,b)\\\\\n",
    "{\\rm COV}(a,b) & \\sigma_{b}^{2}\n",
    "\\end{array}\\right)=\\sum_{i=1}^{N}\\left(\\begin{array}{cc}\n",
    "\\left(a_{i}-\\bar{a}\\right)^{2} & \\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right)\\\\\n",
    "\\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right) & \\left(b_{i}-\\bar{b}\\right)^{2}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where on the right side we have written it in terms of the computation over events. Recall that for a linear regression the slope is just the $\\rm{COV(X,Y)/VAR(X)}$, so the covariance matrix is intricately tied with slope. \n",
    "\n",
    "We can also write it as the correlation matrix where we normalize by the uncertainties:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\rho=\\left(\\frac{1}{\\sigma_{a}}  \\frac{1}{\\sigma_{b}} \\right)^{T}\\left(\\begin{array}{cc}\n",
    "1 & \\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}}\\\\\n",
    "\\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}} & 1\n",
    "\\end{array}\\right)\\left(\\frac{1}{\\sigma_{a}}   \\frac{1}{\\sigma_{b}} \\right) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Recall that the covariance is what we originally used to compute the linear slope of the points, this is exactly the same here. In fact, instead of scanning the likelihood analytically we could have sampled the points, and done a linear regression. The resulting slope and line can be related to our eigenvectors. One last thing to mention is that if variables are correlated, we can use the correlation to propagate the uncertainties. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{f}^{2} = \\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_{x}^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^{2}\\sigma_{y}^2+\\left(\\frac{\\partial f}{\\partial x}\\right)\\left(\\frac{\\partial f}{\\partial y}\\right)\\sigma_{xy}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now lets check we can get the corelation matrix.\n",
    "\n",
    "As a small note our final fitted uncertainty is a little bit larger than the Poisson uncertainty. In fact for all fits there is a rule that our uncertainties on any parameter have to be larger than a certain number. This bound is known as the Cramér-Rao bound. I won't derive it or go into in detail, but the Cramér-Rao bound states.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Var}(\\theta|\\hat{\\theta}) \\geq \\frac{1}{\\mathcal{I}(\\theta)} \\\\\n",
    "\\mathcal{I}(\\theta) = E_{p(X|\\theta)}\\left[-\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\log\\left(p\\left(x|\\theta\\right)\\right)\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $\\mathrm{Var}(\\theta|\\hat{\\theta})$ is the variance on our fitted parameter and we call $\\mathcal{I}(\\theta)$ the [Fisher information](https://en.wikipedia.org/wiki/Fisher_information), which generalizes to the correlation matrix over binomial sampled distributions and, in its simplest form, is equal the inverse of the variance of a binomial distribution, which in turn is the more precise description of a poisson distribution. Hence the fact that our uncertainty is larger than the Poisson uncertainty. While I don't want to go into this more, this result is powerful because it means that there is limit to when you should stop searching for a best fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cee2ef",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell02\n",
    "\n",
    "#Now lets get the correlation C(a,b) (see below)\n",
    "w, v=np.linalg.eig(2*sol.hess_inv)\n",
    "print(2*sol.hess_inv)\n",
    "print(v)\n",
    "print(\"c(a,b)\",v[0,1]/v[0,0])\n",
    "print(\"A deceptively wrong way to get correlation: since its not normalized\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f1d14",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Finally, we are going to run our fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713300b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell03\n",
    "\n",
    "import lmfit\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b*(1-x)\n",
    "    return a*x+pVal\n",
    "\n",
    "#Randomly sample points in the above range\n",
    "def maketoy(iy):\n",
    "    toy=np.array([])\n",
    "    #go through the y-values and Poisson fluctuate\n",
    "    for i0 in range(len(iy)):\n",
    "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n",
    "        toy = np.append(toy,float(pVal))\n",
    "    return toy\n",
    "\n",
    "def fittoy(ibin,iy):\n",
    "    #generate toy\n",
    "    toy=maketoy(iy)\n",
    "    #now fit\n",
    "    model  = lmfit.Model(fnew)\n",
    "    p = model.make_params(a=1000,b=10)\n",
    "    xweights=np.array([])\n",
    "    #setup poison weight\n",
    "    for i0 in range(len(toy)):\n",
    "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n",
    "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n",
    "    return result.params[\"a\"].value,result.params[\"b\"].value\n",
    "\n",
    "ntoys=2000\n",
    "lAs=[]\n",
    "lBs=[]\n",
    "for i0 in range(ntoys):\n",
    "    pA,pB=fittoy(xhist,yhist)\n",
    "    lAs.append(pA)\n",
    "    lBs.append(pB)\n",
    "lAs = np.array(lAs)\n",
    "lBs = np.array(lBs)\n",
    "    \n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n",
    "plt.xlabel('a')\n",
    "plt.ylabel('b')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7f9f1",
   "metadata": {},
   "source": [
    "You can see that if we randomly sample from our distributions, then we get the ellipse variation in $A$ and $B$. The toys have a lot of useful elements. We can use it to get the variation in our parameters.  What we can also do is look at the variance of our parameters and compare it to the Hessian that we can diagonalize to get the fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets run a. linear regression\n",
    "def variance(isamples):\n",
    "    mean=isamples.mean()\n",
    "    n=len(isamples)\n",
    "    tot=0\n",
    "    for pVal in isamples:\n",
    "        tot+=(pVal-mean)**2\n",
    "    return tot/n\n",
    "\n",
    "def covariance(ixs,iys):\n",
    "    meanx=ixs.mean()\n",
    "    meany=iys.mean()\n",
    "    n=len(ixs)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/n\n",
    "\n",
    "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n",
    "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n",
    "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n",
    "print(\"Check with Hessian:\",2*sol.hess_inv)\n",
    "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86041",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_5'></a>   \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc2f20",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.11.1</span>\n",
    "\n",
    "Repeat the above uncorrelated fit $f(x) = a x + b$, plot the ellipse, compute the uncertainties in $a$ and $b$? Do they correspond with lmfit? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2d15a",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def exp_func(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf9943",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "def fnew(x,a,b):\n",
    "    pVal=b\n",
    "    return a*x+pVal\n",
    "\n",
    "##Copying andn pasting everything from above\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=0)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights,scale_covar=False)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([-20,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "w, v=la.eig(2*sol.hess_inv)\n",
    "print(\"values\",w,\"vectors\",v)\n",
    "print(\"matrix a:\",np.sqrt(w[0]),\"b:\",np.sqrt(w[1]))\n",
    "print(\"a:\",result.params[\"a\"].stderr,\"b:\",result.params[\"b\"].stderr)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.5,sol.x[0]*1.5, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "levels = [0.1,1.0,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+sol.fun\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2d54a",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "We find the uncertainties are comparable to lmfit. \n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "You will notice that to get the uncertainties we used this `scale_covar=False` formula, this computes the uncertainties from the Hessian in lmfit. Otherwise, it applies a correction to the uncertainties to account for the scenario when the fit $\\chi^{2}$ is large. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98847c2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_12'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.1 Supernovae and the universe in 2D</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_11) | [Exercises](#exercises_7_12) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f6e590f",
   "metadata": {},
   "source": [
    "Now, we would like to end our 2D plotting discussion and move on to geener pastures by doing the full supernovae analysis and making the famous 2D plot of dark matter density. \n",
    "\n",
    "Let's go ahead and load the supernova data once more time. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c4ec9356",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import math\n",
    "import numpy as np\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#Let's try to understand how good the fits we made in last Lesson are, let's load the supernova data again\n",
    "label='data/sn_z_mu_dmu_plow_union2.1.txt'\n",
    "\n",
    "def distanceconv(iMu):\n",
    "    power=iMu/5+1\n",
    "    return 10**power\n",
    "\n",
    "def distanceconverr(iMu,iMuErr):\n",
    "    power=iMu/5+1\n",
    "    const=math.log(10)/5.\n",
    "    return const*(10**power)*iMuErr\n",
    "\n",
    "def load(iLabel,iMaxZ=10.):\n",
    "    redshift=np.array([])\n",
    "    distance=np.array([])\n",
    "    distance_err=np.array([])\n",
    "    with open(iLabel,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter='\\t')\n",
    "        for row in plots:\n",
    "            if float(row[1]) > iMaxZ:\n",
    "                continue\n",
    "            redshift = np.append(redshift,float(row[1]))\n",
    "            distance = np.append(distance,distanceconv(float(row[2])))\n",
    "            distance_err = np.append(distance_err,distanceconverr(float(row[2]),float(row[3])))\n",
    "    return redshift,distance,distance_err  \n",
    "        \n",
    "#Now let's run the regression again\n",
    "def variance(isamples):\n",
    "    mean=isamples.mean()\n",
    "    n=len(isamples)\n",
    "    tot=0\n",
    "    for pVal in isamples:\n",
    "        tot+=(pVal-mean)**2\n",
    "    return tot/n\n",
    "\n",
    "def covariance(ixs,iys):\n",
    "    meanx=ixs.mean()\n",
    "    meany=iys.mean()\n",
    "    n=len(ixs)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/n\n",
    "\n",
    "def linear(ix,ia,ib):\n",
    "    return ia*ix+ib\n",
    "\n",
    "redshift,distance,distance_err=load(label)\n",
    "var=variance(redshift)\n",
    "cov=covariance(redshift,distance)\n",
    "A=cov/var\n",
    "const=distance.mean()-A*redshift.mean()\n",
    "xvals = np.linspace(0,1.5,100)\n",
    "yvals = []\n",
    "for pX in xvals:\n",
    "    yvals.append(linear(pX,A,const))\n",
    "\n",
    "plt.plot(xvals,yvals)\n",
    "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.xlabel(\"z(redshift)\")\n",
    "plt.ylabel(\"distance(pc)\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81be2c9d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "<h3>Remember Numerical properties of the universe</h3>\n",
    "\n",
    "Recall in Lecture 6 we took the following formula for the comoving distance of the redshift\n",
    "\n",
    "$$ d(z) = \\frac{c}{h_{0}}(1+z) \\sum_{i=0}^{i=100} \\frac{dz}{\\sqrt{\\Omega_{M}(1+z_{i}^{\\prime})^{3} + 1-\\Omega_{M}}}$$\n",
    "\n",
    "where $dz=z/100$ and $z_{i}= dz \\times i$. \n",
    "\n",
    "Then we went ahead and coded this up and put fit it to our supernova data. Let's run the fit and look at our previous diagnostics. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f958908",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_08",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L6.8-runcell01\n",
    "import lmfit\n",
    "\n",
    "def hubble(z,Om):\n",
    "    pVal=Om*(1+z)**3+(1.-Om)\n",
    "    return np.sqrt(pVal)\n",
    "\n",
    "def lumidistance(z,h0,Om):\n",
    "    integral=0\n",
    "    nint=100\n",
    "    for i0 in range(nint):\n",
    "        zp=z*float(i0)/100.\n",
    "        dz=z/float(nint)\n",
    "        pVal=1./(1e-5+hubble(zp,Om))\n",
    "        integral += pVal*dz\n",
    "    d=(1.+z)*integral*(1e6*3e5/h0)\n",
    "    return d\n",
    "\n",
    "model  = lmfit.Model(lumidistance)\n",
    "p = model.make_params(h0=70,Om=0.2)\n",
    "result = model.fit(data=distance, params=p, z=redshift, weights=1./distance_err)\n",
    "lmfit.report_fit(result)\n",
    "\n",
    "#Plot it against the data\n",
    "xvals = np.linspace(0,1.4,100)\n",
    "yvals = []\n",
    "for pX in xvals:\n",
    "    yvals.append(lumidistance(pX,result.params[\"h0\"],result.params[\"Om\"]))\n",
    "\n",
    "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.plot(xvals,yvals)\n",
    "plt.show()\n",
    "\n",
    "x = np.linspace(len(distance)*0.5,len(distance)*1.5)\n",
    "chi2d=stats.chi2.pdf(x,len(distance-2)) # 40 bins\n",
    "plt.plot(x,chi2d,label='chi2')\n",
    "plt.axvline(result.chisqr, c='red')\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "876ceb0f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_08"
    ]
   },
   "source": [
    "Ok, the fit looks really good. Now that we have done this, lets go ahead and extend this to a 2-dimensional plot, to do the 2D plot, we now need to simulatneously scan both the two varaibles that we are fitting for, which in this is Hubble's constant $h_{0}$ and the matter density $\\Omega_{m}$. \n",
    "\n",
    "To do this, we will run the minimizer and then we will look at the contours about the minimum. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "955926ab",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Plot it against the data\n",
    "def loglike(x):\n",
    "    lTot=0\n",
    "    for i0 in range(len(redshift)):\n",
    "        xtest=lumidistance(redshift[i0],x[0],x[1])\n",
    "        #lTot = lTot+(distance[i0]-xtest)**2\n",
    "        lTot = lTot+((1./distance_err[i0])**2)*(distance[i0]-xtest)**2\n",
    "    return lTot #*0.5 The above is 2 times loglike\n",
    "\n",
    "#x0 = np.array([70.,0.3])\n",
    "#ps = [x0]\n",
    "#bnds = ((0, 1000), (0, 1.5))\n",
    "#sol=opt.minimize(loglike, x0,bounds=bnds, tol=1e-6)\n",
    "#sol=opt.minimize(loglike, x0,bounds=bnds)\n",
    "\n",
    "#Finally lets get our 1D unctainies from the Hessian\n",
    "#unc=np.sqrt(2*sol.hess_inv.matmat(np.eye(2)))\n",
    "#print(\"Unc matrix:\",unc)\n",
    "#print(\"h0\",sol.x[0],\"+/-\",unc[0,0])\n",
    "#print(\"Om:\",sol.x[1],\"+/-\",unc[1,1])\n",
    "\n",
    "#And lets get the correlations\n",
    "#import numpy.linalg as la\n",
    "#w, v=la.eig(2*sol.hess_inv)\n",
    "#print(\"values\",w,\"vectors\",v)\n",
    "#Now lets get the correlation C(a,b) (see below)\n",
    "#print(\"c(a,b)\",v[0,1]/v[0,0])\n",
    "\n",
    "#Now lets scan the parameters\n",
    "x = np.linspace(result.params['h0']*0.9,result.params['h0']*1.1, 30)\n",
    "y = np.linspace(0.,0.5, 30)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "def pvalue(isigma):\n",
    "    return stats.norm.cdf(isigma)-stats.norm.cdf(-isigma)\n",
    "\n",
    "OneSigma   = stats.chi2.ppf(pvalue(1),2)\n",
    "TwoSigma   = stats.chi2.ppf(pvalue(2),2)\n",
    "ThreeSigma = stats.chi2.ppf(pvalue(3),2)\n",
    "print(\"Levels:\",OneSigma,TwoSigma,ThreeSigma)\n",
    "levels = [OneSigma,TwoSigma,ThreeSigma]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+loglike([result.params['h0'],result.params['Om']])\n",
    "\n",
    "Z = np.array([loglike([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "plt.xlabel('h0')\n",
    "plt.ylabel('$\\Omega_{M}$')\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0db1eebc",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Fitting for the curvature</h3>\n",
    "\n",
    "\n",
    "So given the Friedmann equations, we can add back the curvature term:\n",
    "\n",
    "$$ \n",
    "d(z) = ct^{\\prime} = (1+z)ct = (1+z)\\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}\\left(1+z^{\\prime}\\right)^{3} + \\Omega_{\\kappa}\\left(1+z^{\\prime}\\right)^{2}+ 1-\\Omega_{M}-\\Omega_{\\kappa}}}\n",
    "$$\n",
    "\n",
    "We can adjust the luminosity distance to add this back. Recall that \n",
    "\n",
    "$$\n",
    "\\Omega_{m}+\\Omega_{k}+\\Omega_{\\Lambda}=1\n",
    "$$\n",
    "So we only need to float two fo them. Let's go ahead and run the fit. \n",
    "\n",
    "$$ \n",
    "d(z) = ct^{\\prime} = (1+z)ct = (1+z)\\frac{c}{h_{0}}\\int_{0}^{z} \\frac{dz^{\\prime}}{\\sqrt{\\Omega_{M}\\left(1+z^{\\prime}\\right)^{3} + (1-\\Omega_{\\Lambda}-\\Omega_{M})\\left(1+z^{\\prime}\\right)^{2}+ \\Omega_{\\Lambda}}}\n",
    "$$\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b5955312",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#And finally lets actually float both the matter and dark energy\n",
    "def sqrtTerm2(z,Om,OmL):\n",
    "    pVal=Om*(1+z)**3+OmL+(1-OmL-Om)*(1+z)**2\n",
    "    return np.sqrt(pVal)\n",
    "\n",
    "def lumidistance2(x,h0,Om,OmL):\n",
    "    integral=0\n",
    "    nint=100\n",
    "    for i0 in range(nint):\n",
    "        pVal=1./(1e-5+sqrtTerm2(x*float(i0)/100.,Om,OmL))\n",
    "        integral += pVal*x/float(nint)\n",
    "    d=(1.+x)*integral*(1e6*3e5/h0)\n",
    "    return d\n",
    "\n",
    "def loglike2(x):\n",
    "    lTot=0\n",
    "    for i0 in range(len(redshift)):\n",
    "        xtest=lumidistance2(redshift[i0],x[0],x[1],x[2])\n",
    "        #lTot = lTot+(distance[i0]-xtest)**2\n",
    "        lTot = lTot+((1./distance_err[i0])**2)*(distance[i0]-xtest)**2\n",
    "    return lTot #*0.5 The above is 2 times loglike\n",
    "\n",
    "\n",
    "model  = lmfit.Model(lumidistance2)\n",
    "p = model.make_params(h0=70,Om=0.3,OmL=0.7)\n",
    "result = model.fit(data=distance, params=p, x=redshift, weights=1./distance_err)\n",
    "lmfit.report_fit(result)\n",
    "#plt.figure()\n",
    "#result.plot()\n",
    "\n",
    "#Plot it against the data\n",
    "xvals = np.linspace(0,1.4,100)\n",
    "yvals = []\n",
    "for pX in xvals:\n",
    "    yvals.append(lumidistance2(pX,result.params[\"h0\"],result.params[\"Om\"],result.params[\"OmL\"]))\n",
    "\n",
    "plt.errorbar(redshift,distance,yerr=distance_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.plot(xvals,yvals)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "123f89bc",
   "metadata": {},
   "source": [
    "Now lets see if we can make our plot. We will draw the log likelihood about the best fit from lmfit, and then we will draw the contours. Also lets not go overboard on the contours"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be6a80b",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#result.params[\"a\"].value,result.params[\"b\"].value\n",
    "#Now lets scan the parameters\n",
    "x = np.linspace(0,1.5,30)\n",
    "y = np.linspace(0,1.5,30)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "levels = [OneSigma,TwoSigma,ThreeSigma]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+loglike2([result.params[\"h0\"],result.params[\"Om\"],result.params[\"OmL\"]])\n",
    "Z = np.array([loglike2([result.params[\"h0\"],x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "plt.xlabel('$\\Omega_{m}$')\n",
    "plt.ylabel('$\\Omega_{\\Lambda}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c8b12b4",
   "metadata": {},
   "source": [
    "This finally gives us our 2D plot that, and you can see that it looks good"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c73eca",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.4 Principal Component Analysis</h2>     \n",
    "\n",
    "| [Top](#section_8_0) | [Previous Section](#section_8_3) | [Exercises](#exercises_8_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818dc421",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Finally, I would like to say that this method of finding the ellipse is our first deep learning method.\n",
    "This procedure of computing the covariance matrix, and finding the eigenvectors is known as  principal component analysis or PCA. Let's run it on our example and look from <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\" target=\"_blank\">Python Data Science Handbook by Jake VanderPlas</a>.\n",
    "\n",
    "First, what we can do is look at our old correlated fit. All this will do is get the eigenvectors and values for our 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd59fa6",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell01\n",
    "import numpy as np\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy.linalg as la\n",
    "#make some toy data\n",
    "lAs = np.random.normal(0,1,1000)\n",
    "lBs = np.random.normal(0,1,1000)+0.5*lAs\n",
    "#print(lAs)\n",
    "\n",
    "cov = np.cov([lAs,lBs])\n",
    "#eigen cov\n",
    "w, v=la.eig(cov)\n",
    "\n",
    "\n",
    "plt.plot(lAs,lBs,\".\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "X=(np.vstack([lAs,lBs])).T\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(\"PCA vectors\")\n",
    "print(pca.components_)\n",
    "print(\"PCA values\")\n",
    "print(pca.explained_variance_)\n",
    "print(\"Old Eigen\",\"vectors\",w,\"values\",v)\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',linewidth=2,shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(lAs, lBs, alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac5e41",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "Finally, we can try this on an ML dataset. Let's take images with many pixels and treat each pixel as a separate dimension. We can then run the decomposition on the image by decomposing the n-pixel by n-pixel correlation matrix. What we will do is take an image that is 62x47=2914 Pixels. From that we can compute the covariance matrix and we can start to diagonlize this. Since this is a large matrix (2914x2914), we have to use sophisticaed eigen decomposition strategies. In this case, we will use something called RandomizedPCA. \n",
    "\n",
    "What we are going to do is run PCA on the images, and take the top 200 eigenvectors from that. Then we are going to plot the top few of these eigen vectors and the cumulative explained variance, which is a metric for how much information can be gained by adding dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05851adf",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell02\n",
    "\n",
    "#Now let's do it ML style for fun\n",
    "#Load some faces of images\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "#Let's plot the eigenvectors\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.data[i].reshape(62, 47), cmap='bone')\n",
    "#print(62*47)\n",
    "#a\n",
    "#Fit them to PCA \n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "pca = RandomizedPCA(200)\n",
    "pca.fit(faces.data)\n",
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "#Let's plot the eigenvectors\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')\n",
    "plt.show()\n",
    "    \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0f6fe",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "What you can see above is that the top eigen vectors capture the shape of the face. You can see the shape variations between the different eigenvectors as you go through. Youc an see also that about 80% of the total info in the first 200 eigenvectors is captures in the first 25 components. \n",
    "\n",
    "Finally, what we can do is plot our world leaders just by taking the first 80 eigenvectors of our sample. What we have effectively done is compress our original image into just 80 values, thats all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7d1e7",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell03\n",
    "\n",
    "# Compute the components and projected faces\n",
    "pca = RandomizedPCA(10).fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('80-dim\\nreconstruction');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb371895",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Exercise using stellar spectra</h3>\n",
    "\n",
    "As a final exercise, we are going to isolate features from stellar spectra of the Sloan Digit Sky Survey. To that, let's first load data from the SDSS and try to understand what this looks like. We will use the `astroML` package, that allows us to pull astro data quickly. \n",
    "\n",
    "From this data, we will look at redshift correct galaxy spectra. The correction allows us to look at the large population of galaxies in this dataset and try to find the dominant features. Let's take a quick look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deda61",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell04\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "count=-1\n",
    "#let's plot this guy\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 5.5),subplot_kw={'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "nrows = 2; ncols = 10\n",
    "for i in range(ncols):\n",
    "    for j in range(nrows):\n",
    "        count=count+1\n",
    "        ax[j,i].plot(wavelengths,spectra_raw[count], '-k', lw=1)\n",
    "        ax[j,i].set_xlim(3100, 7999)\n",
    "        if j < nrows - 1:\n",
    "            ax[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            ax[j,i].set_xlabel(r'wavelength $(\\AA)$')\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2abffb",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "What you see is that the spectra vary, but there are a few lines from the galaxies that are particularly bright. Look at the one above 6000 Angstroms. That the Hydrogen, $\\beta$ emission line. You can see the whole table of lines <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.\n",
    "\n",
    "In the following problem, we will look closely at the dominant eigenvector and identify any spectral lines that it might include."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a15f0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.4.1</span>\n",
    "\n",
    "Your task is run PCA, look at the dominant eigenvector, and find the top two spectral lines. What do they correspond to (refer to the table <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.)?\n",
    "\n",
    "Select two spectral lines from the options below (in units of angstroms):\n",
    "\n",
    "- He ~4200\n",
    "- H$\\gamma$ ~4350\n",
    "- H$\\beta$ ~4860\n",
    "- O III ~4950\n",
    "- O III ~5006\n",
    "- H$\\alpha$ ~6560\n",
    "- N II ~6580\n",
    "- O I ~7000\n",
    "\n",
    "Does this make sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f135894",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L8.4.1\n",
    "\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "\n",
    "#select only the first 850 bins to avoid noise features\n",
    "spectra_raw = spectra_raw[:,0:850]\n",
    "wavelengths = wavelengths[0:850]\n",
    "pca = RandomizedPCA()\n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n",
    "#fit the pca to the spectra_raw data \n",
    "#plot the first eigenvector, pca.components_[0] over the full range of wavelengths\n",
    "#optionally constrain the wavelength range to zoom in on spectral lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856c822",
   "metadata": {
    "scrolled": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.4.1\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "spectra_raw = spectra_raw[:,0:850]\n",
    "wavelengths = wavelengths[0:850]\n",
    "\n",
    "pca = RandomizedPCA()\n",
    "pca.fit(spectra_raw)\n",
    "plt.plot(wavelengths,pca.components_[0] , '-k', lw=1)\n",
    "plt.xlabel(r'wavelength $(\\AA)$')\n",
    "#plt.xlim(6500,7000)\n",
    "plt.show()\n",
    "\n",
    "#print(wavelengths,pca.components_)\n",
    "#print(pca.explained_variance_ratio_)\n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae7266",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "\n",
    "- O III ~5006\n",
    "- H$\\alpha$ ~6560\n",
    "\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "These emissions lines are famous lines from Oxygen and Hydrogen. See for example <a href=\"https://en.wikipedia.org/wiki/Doubly_ionized_oxygen\" target=\"_blank\">here</a>.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fa5ae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 8.4.1a (ungraded)\n",
    ">\n",
    ">Plot all of the eigenvectors. What are the dominant features of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146fff0",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.4.1\n",
    "\n",
    "#Now let's look at the top 100 eigen vectors \n",
    "count=-1\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,bottom=0.1, top=0.95, hspace=0.05)\n",
    "nrows = 10; ncols = 10\n",
    "for i in range(ncols):\n",
    "    for j in range(nrows):\n",
    "        count=count+1\n",
    "        ax = fig.add_subplot(nrows, ncols, ncols * j + 1 + i)\n",
    "        ax.plot(wavelengths,pca.components_[count] , '-k', lw=1)\n",
    "        ax.set_xlim(3100, 7999)\n",
    "\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "        if j < nrows - 1:\n",
    "            ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            plt.xlabel(r'wavelength $(\\AA)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "75cae6b2",
   "metadata": {},
   "outputs": [],
   "source": [
    "components = pca.transform(spectra_raw)\n",
    "projected = pca.inverse_transform(components)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 5, figsize=(20, 10),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(5):\n",
    "    ax[0,i].plot(wavelengths,spectra_raw[count], '-k', lw=1)\n",
    "    ax[0,i].set_xlim(3100, 7999)\n",
    "    ax[1,i].plot(wavelengths,projected[count], '-k', lw=1)\n",
    "    ax[1,i].set_xlim(3100, 7999)\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('80-dim\\nreconstruction');\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5ebcf887",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
