{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "090cb21a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.316 Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 7: Correlations</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "82515af8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c2cd2d2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_1\">L7.1 Understanding Best Fit (Revisited)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_1\">L7.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_2\">L7.2 Minimizing on a Surface (1D Scan)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_2\">L7.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_3\">L7.3 Minimizing on a Surface (2D Scan)</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_3\">L7.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_4\">L7.4 Correlations Between Fit Parameters: Part 1</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_4\">L7.4 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_7_5\">L7.5 Correlations Between Fit Parameters: Part 2</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_7_5\">L7.5 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5783f1b7",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "In this lecture, we wil exploret the nature of correlated variables and how they impact our interpretation of results. Furthermore, we will understand how to go away from correlations to see how we can reduce their impact. Additionally, we will start to scan the parameters of our fit beyond just finding the minimum. By scanning, we start to learn the subtleties of each of these setups, and we start to get deep insight into the nature of what we are minimizing and how to interpret it. \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5a2ec6d3",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cells below to import the relevant libraries for this notebook. If you're using a Colab cloud runtime, also run the cell labeled \"for Colab users.\"\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "010c45e6",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f4182ec5",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell02\n",
    "\n",
    "# for Colab users\n",
    "!pip install lmfit\n",
    "\n",
    "# Get data and download to root of your Google Drive\n",
    "from google.colab import drive\n",
    "drive.mount('/content/drive')\n",
    "\n",
    "# If you're not comfortable giving permissions for this command, download the file from the URL, \n",
    "# then upload to your Google Drive root folder.\n",
    "!wget -P /content/drive/MyDrive/ https://raw.githubusercontent.com/mitx-8s50/data/main/L07/events_a8_1space.dat"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b305ef1b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell02 (ANOTHER WAY?)\n",
    "\n",
    "# for Colab users\n",
    "!pip install lmfit\n",
    "\n",
    "# Downloading data for your runtime\n",
    "!cd /\n",
    "!wget https://raw.githubusercontent.com/mitx-8s50/data/main/L07/events_a8_1space.dat"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "927ca505",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fc09dcc",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.0-runcell03\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcd21490",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.1 Understanding Best Fit (Revisited)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_0) | [Exercises](#exercises_7_1) | [Next Section](#section_7_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50a679e9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "8S50x"
    ]
   },
   "source": [
    "*The material in this section is discussed in the video **<a href=\"https://courses.mitxonline.mit.edu/learn/course/course-v1:MITxT+8.S50.1x+3T2022/block-v1:MITxT+8.S50.1x+3T2022+type@sequential+block@seq_LS7/block-v1:MITxT+8.S50.1x+3T2022+type@vertical+block@vert_LS7_vid1\" target=\"_blank\">HERE</a>.** You are encouraged to watch that video and use this notebook concurrently.*"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da6daf70",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5af9c666",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_01.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a60a8628",
   "metadata": {
    "tags": [
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_01.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07bdcea4",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Uncertainty on more than one parameter</h3>\n",
    "\n",
    "At the end of last week, we started to fit more than one parameter. That is, we minimized some error metric for more than one parameter. Let's take a closer look. As before, we'll fit the function \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x,a,b) = a + b \\sin(x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "to the Auger data, i.e. finding optimal choices of parameters $a$ and $b$ given some set of sample observations $y_i$ and their sample uncertainties $\\sigma_i$. One way to choose optimal parameter values is by optimizing our likelihood over both $a$ and $b$. As a reminder, likelihood defines  how probable our observed data is as a function of parameter values and a hypothesized form. At extrema, we'd expect our likelihood is maximized or \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\frac{\\partial \\mathcal{L}}{\\partial a} = 0 \\\\\n",
    " \\frac{\\partial \\mathcal{L}}{\\partial b} = 0 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As we saw in L5.7, maxima of the likelihood function correspond to minima of the relevant $\\chi^2$ statistic of the form\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "  \\chi^{2}(x|a,b) = \\sum_{i=1}^{N} \\frac{(y_{i}-f(x_{i},a,b))^2}{\\sigma_i^2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where we will make the assumption that $\\sigma_i = \\sqrt{f(x_{i},a,b)}$. At the extrema,\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial a} = 0 \\\\\n",
    "\\frac{\\partial \\chi^{2}(x|a,b) }{\\partial b} = 0\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "In a little bit, we're going to use this to manually write code to fit the function using the generic optimizer in `scipy`. But first, let's review how things look using `lmfit`. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "269bc5f6",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-runcell01\n",
    "\n",
    "############ This is all code from a previous lesson\n",
    "import numpy as np\n",
    "import csv\n",
    "import math\n",
    "import lmfit\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import optimize as opt\n",
    "\n",
    "def rad(iTheta):\n",
    "    return iTheta/180. * math.pi\n",
    "\n",
    "def rad1(iTheta):\n",
    "    return iTheta/180. * math.pi-math.pi\n",
    "\n",
    "def load(label):\n",
    "    dec=np.array([])\n",
    "    ra=np.array([])\n",
    "    az=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile,delimiter=' ')\n",
    "        for pRow in plots:\n",
    "            if '#' in pRow[0] or pRow[0]=='':\n",
    "                continue\n",
    "            dec = np.append(dec,rad(float(pRow[2])))\n",
    "            ra  = np.append(ra,rad1(float(pRow[3])))\n",
    "            az  = np.append(az,rad(float(pRow[4])))\n",
    "    return dec,ra,az\n",
    "\n",
    "def prephist(iRA):\n",
    "    y0, bin_edges = np.histogram(iRA, bins=30)\n",
    "    x0 = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    y0 = y0.astype('float')\n",
    "    return x0,y0,1./(y0**0.5)\n",
    "\n",
    "label8='data/events_a8_1space.dat'\n",
    "\n",
    "# Uncomment if using cloud runtime (which one?)\n",
    "#label8='/content/drive/MyDrive/events_a8_1space.dat'\n",
    "#label8='/content/events_a8_1space.dat'\n",
    "\n",
    "dec,ra8,az=load(label8)\n",
    "xhist,yhist,xweights=prephist(ra8)\n",
    "\n",
    "\n",
    "########## Tlast fit code\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    return a+b*np.sin(x)\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=10)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "262677b7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3> What is the $\\chi^{2}$-value from a p-value? </h3>\n",
    "\n",
    "So what is the importance of this fit? This is just equal to a $\\chi^{2}$ distribution, as we ahve seen before. Before we delve into the details of the importance of the fit, lets just remind ourselves how to compute the p-value from a gaussian and the $\\chi^{2}$ value from the p-value or a sigma value assuming a Gaussian. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "06284815",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.1-runcell02\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "sigma = 1\n",
    "print(stats.norm.cdf(sigma)-stats.norm.cdf(-sigma))\n",
    "\n",
    "def pval(iVal):\n",
    "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n",
    "\n",
    "def chi2Val(iGausSigma,iNDOF):\n",
    "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n",
    "    return val\n",
    "\n",
    "print(chi2Val(sigma,2))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1d50f61",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_1'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_1) | [Next Section](#section_7_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49e84a0a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.1.1</span>\n",
    "\n",
    "Compute the Chi-square value for 2 sigma and 2 degrees of freedom? Enter your answer as number with precision 1e-2. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ca364a97",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def exp_func(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fe71584",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def pval(iVal):\n",
    "    return stats.norm.cdf(iVal)-stats.norm.cdf(-iVal)\n",
    "\n",
    "def chi2Val(iGausSigma,iNDOF):\n",
    "    val=stats.chi2.ppf(pval(iGausSigma),iNDOF)\n",
    "    return val\n",
    "\n",
    "print(chi2Val(2,2))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c223fcdf",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "$Χ^{2} = 6.18$\n",
    "\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Use the same function as explained in L7.1 lecture video. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e545d620",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.2 Minimizing on a Surface (1D Scan)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_1) | [Exercises](#exercises_7_2) | [Next Section](#section_7_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "730e038e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, what we are going to do is re-run the fit. Here we'll go through the $\\chi^2$ minimization process semi manually, using `scipy.opt`. The code is explained in the lecture.\n",
    "\n",
    "Strategically, what we are trying to do is write the $\\chi^{2}$ value and use our normal minimizer tools to find the minimum parameters. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8171ebcb",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.2-runcell01\n",
    "\n",
    "#This is the new code\n",
    "def chi2(iX): #iX is an array iX[0]=a and iX[1]=b\n",
    "    '''Note that this function depends on xhist and yhist being defined already.\n",
    "    This is bad practice in general, but we do it here for convenience.\n",
    "    After all, this code isn't reused elsewhere.'''\n",
    "    assert len(iX) == 2\n",
    "    lTot=0\n",
    "    for val in range(len(yhist)):\n",
    "        xtest=fnew(xhist[val],iX[0],iX[1])\n",
    "        lTot += (1./(1e-5+xtest))*(yhist[val]-xtest)**2\n",
    "    return lTot\n",
    "\n",
    "#First we minimize\n",
    "x0 = np.array([1000,10]) # initial conditions\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eff7b26f",
   "metadata": {},
   "source": [
    "Now, given that we have found the minimum, lets look at how the $\\chi^{2}$ value changes from the minimum. The important thing is to remember that from [Wilk's Thereom](https://en.wikipedia.org/wiki/Wilks%27_theorem) the uncertainty of a parameter can be obtained by tkaing $\\Delta \\chi^{2}$ from the minimum of a value of 1. Lets go ahead and plot the minimum and look at what $\\Delta \\chi^{2}=1$ looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cefadb2",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.2-runcell02\n",
    "\n",
    "#Scan near the minimum of each value\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n",
    "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)\n",
    "\n",
    "#Now lets fix one parameter at the minimum, and profile the other\n",
    "plt.plot(x, chi2([x,sol.x[1]]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"a-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()\n",
    "\n",
    "#Now for the other parameter\n",
    "plt.plot(y, chi2([sol.x[0],y]),label='chi2');\n",
    "plt.axhline(sol.fun+1, c='red')\n",
    "plt.xlabel(\"b-value\")\n",
    "plt.ylabel(\"$\\chi^{2}$\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15952b19",
   "metadata": {},
   "source": [
    "Where the red and blue lines cross correspnods to the one $\\sigma$ fluctation up and down from the minimum. As a result, we can code up this algorithm and compute the best fit and the uncertainty on the best fit parameter. Let's go ahead and code this up. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "77713d2d",
   "metadata": {},
   "outputs": [],
   "source": [
    "######## Now lets use a numerical solver to find the points at which a function crosses zero (root solver)\n",
    "def chi2minX(xval, delta_chi2=1):\n",
    "    val=chi2([xval,sol.x[1]])\n",
    "    minval=chi2(sol.x) + delta_chi2\n",
    "    return val-minval\n",
    "\n",
    "def chi2minY(yval, delta_chi2=1):\n",
    "    val=chi2([sol.x[0],yval])\n",
    "    minval=chi2(sol.x) + delta_chi2\n",
    "    return val-minval\n",
    "\n",
    "def chi2uncX(sol):\n",
    "    solX1=opt.root_scalar(chi2minX,bracket=[sol.x[0], sol.x[0]*1.02],method='brentq')\n",
    "    solX2=opt.root_scalar(chi2minX,bracket=[sol.x[0]*0.98, sol.x[0]],method='brentq')\n",
    "    print(\"a:\",sol.x[0],\"+/-\",abs(solX2.root-solX1.root)/2.)\n",
    "    print(\"Reminder the Poisson uncertainty would be:\",math.sqrt(np.mean(yhist)/(len(xhist))))\n",
    "    return solX1, solX2\n",
    "\n",
    "def chi2uncY(sol):\n",
    "    solY1=opt.root_scalar(chi2minY,bracket=[sol.x[1],    sol.x[1]*1.2],method='brentq')\n",
    "    solY2=opt.root_scalar(chi2minY,bracket=[sol.x[1]*0.8, sol.x[1]],method='brentq')\n",
    "    print(\"b:\",sol.x[1],\"+/-\",abs(solY2.root-solY1.root)/2.)\n",
    "    return solY1, solY2\n",
    "\n",
    "solX1, solX2 = chi2uncX(sol)\n",
    "solY1, solY2 = chi2uncY(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b5fec8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "So what have we done? \n",
    "\n",
    "We've used the optimizer to minimize the $\\chi^{2}$ values in 2D (a,b), and we have obtained an uncertainty by looking at $\\Delta \\chi^{2}$. You can see that our semi manual manipulation got us to the same parameter estimates as the `lmfit` example. Additionally, by looking at slices of $\\chi^{2}$ in each of the parameters we obtained the same uncertainties. \n",
    "\n",
    "As a small note our final fitted uncertainty is a little bit larger than the Poisson uncertainty. In fact for all fits there is a rule that our uncertainties on any parameter have to be larger than a certain number. This bound is known as the Cramér-Rao bound. I won't derive it or go into in detail, but the Cramér-Rao bound states.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{Var}(\\theta|\\hat{\\theta}) \\geq \\frac{1}{\\mathcal{I}(\\theta)} \\\\\n",
    "\\mathcal{I}(\\theta) = E_{p(X|\\theta)}\\left[-\\frac{\\partial^{2}}{\\partial\\theta^{2}}\\log\\left(p\\left(x|\\theta\\right)\\right)\\right]\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $\\mathrm{Var}(\\theta|\\hat{\\theta})$ is the variance on our fitted parameter and we call $\\mathcal{I}(\\theta)$ the [Fisher information](https://en.wikipedia.org/wiki/Fisher_information), which generalizes the variance over binomial sampled distributions and, in its simplest form, is equal the inverse of the variance of a binomial distribution, which in turn is the more precise description of a poisson distribution. Hence the fact that our uncertainty is larger than the Poisson uncertainty. While I don't want to go into this more, this result is powerful because it means that there is limit to when you should stop searching for a best fit. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "088e2f50",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_2'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_2) | [Next Section](#section_7_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6b2a88c0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.2.1</span>\n",
    "\n",
    "What are the 2$\\sigma$ bounds (that is, 95.45% confidence interval values) of $a$ and $b$ in the fit above? Enter your answer as a list of numbers number with precision 1 (the nearest whole number): `[a_lower, a_upper, b_lower, b_upper]`"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98ab0b9b",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "\n",
    "#For a given parameter, assume everything else is determined, so we're\n",
    "# working with 1 degree of freedom.\n",
    "# First, we want to know the chi-square value at which 2 standard deivations (95.45% of the probability)\n",
    "# in the chi-square distribution is for values less extreme than that value.\n",
    "ndof=1\n",
    "pval=0.9545\n",
    "val = stats.chi2.ppf(pval,ndof)\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minXfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "def minYfunc(x):\n",
    "    return chi2minY(x, delta_chi2=val)\n",
    "\n",
    "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n",
    " #insert code here\n",
    " return valmin,valmax\n",
    "\n",
    "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n",
    "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n",
    "print(f\"b bounds: [{b1}, {b2}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63b18f81",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "from scipy.optimize.zeros import RootResults\n",
    "from scipy.optimize.optimize import OptimizeResult\n",
    "from typing import Callable, Tuple\n",
    "\n",
    "# For a given parameter, assume everything else is determined, so we're\n",
    "# working with 1 degree of freedom.\n",
    "# First, we want to know the chi-square value at which 95.45% of the probability\n",
    "# in the chi-square distribution is for values less extreme than that value.\n",
    "# We do this as before:\n",
    "ndof=1\n",
    "pval=0.9545\n",
    "val = stats.chi2.ppf(pval,ndof)\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "# Now, we independently perturb a or b from their minimized chi-square values\n",
    "# until this Delta chi-square value is achieved, just like above.\n",
    "\n",
    "def chi2unc(sol, sol_index, min_func, unc_prop_guess):\n",
    "    '''Does what chi2uncX and chi2uncY above do, with unc_prop_guess defining the\n",
    "    proportional size of search space (e.g. 0.2 in chi2uncY, or 0.02 in chi2uncX)\n",
    "    However, returns the parameter values where the delta chi2 is reached'''\n",
    "    solX1=opt.root_scalar(min_func, bracket=[sol.x[sol_index], sol.x[sol_index]*(1+unc_prop_guess)], method='brentq')\n",
    "    solX2=opt.root_scalar(min_func, bracket=[sol.x[sol_index]*(1-unc_prop_guess), sol.x[sol_index]], method='brentq')\n",
    "    return solX1.root, solX2.root\n",
    "\n",
    "def minXfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "def minYfunc(x):\n",
    "    return chi2minY(x, delta_chi2=val)\n",
    "\n",
    "# Here we set our delta_chi2 to the value in question, and guess how far away\n",
    "# the values of a and b will be to set unc_prop_guess (if not big enough, error)\n",
    "# then let the optimizer find the corresponding a and b uncertainties\n",
    "a1, a2 = chi2unc(sol, 0, minXfunc, 0.08)\n",
    "b1, b2 = chi2unc(sol, 1, minYfunc, 0.4)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n",
    "print(f\"b bounds: [{b1}, {b2}]\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "34cd0b90",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "a in roughly [1061, 1085] and b in roughly [-67, -33]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "We've assumed all but one parameter is fully determined, so we have 1 degree of freedom. For sake of explanation, assume it's $a$.\n",
    "\n",
    "We're looking for 2-sigma (95.45%) confidence interval bounds, that is, the values of $a$ for which $\\Delta \\chi^2$ is such that in a 1 DoF $\\chi^2$ distribution, 95% of $\\chi^2$ values are less extreme.\n",
    "From the inverse of the CDF, we find this $\\Delta \\chi^2$ is 4.\n",
    "\n",
    "Doing root finding to figure out how far we need to move $a$ or $b$ to achieve this increase in $\\chi^2$, we find the values we're looking for.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e6da90e3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.3 Minimizing on a Surface (2D Scan)</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_2) | [Exercises](#exercises_7_3) | [Next Section](#section_7_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4a163991",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "Now given our minimization worked in 1D for both variables $a$ and $b$, what about if we scan both variables simultaneously and then look at how the minimum behaves. To do this, we can start by first scaning around the best fit values of $a$ and $b$ and just plotting the value of our $\\chi^{2}$ minimization. From this space, we can start to relate this back to the 2D version of Wilk's theorem in the slides below. \n",
    "\n",
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "00aee4c4",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_03.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b4fd5113",
   "metadata": {
    "tags": [
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_03.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7ffb143",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "To really visualize the whole thing lets make one more plot: how the $\\chi^2$ value depends on both parameters. We can do this using the `meshgrid` function and plotting it. In addition, we will add some color full lines for a $\\Delta \\chi^{2}$ given by various values, we will revisit what these values are in a little bit.   "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73905b8d",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-runcell01\n",
    "\n",
    "#define the 2D X and Y grid\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100) #grid in x\n",
    "y = np.linspace(sol.x[1]*0.5,sol.x[1]*1.5, 100)#grid in y\n",
    "X, Y = np.meshgrid(x, y) #2d grid\n",
    "\n",
    "# For z coordinate, evaluate chi2 at each x,y point in the grid.\n",
    "# note understanding this one-liner itself isn't too important\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "\n",
    "#and plot\n",
    "def plotColorsAndContours(X,Y,Z):\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "    cb=fig.colorbar(c, ax=ax)\n",
    "    plt.xlabel(\"a\")\n",
    "    plt.ylabel(\"b\")\n",
    "    cb.set_label(\"$\\chi^{2}$\")\n",
    "    #Now lets plot the contours of Delta chi^2\n",
    "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    for i0 in range(len(levels)):\n",
    "        levels[i0] = levels[i0]+sol.fun\n",
    "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "    #plt.show()\n",
    "    \n",
    "plotColorsAndContours(X,Y,Z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b8fa7601",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "On the above we've plotted level curves representing different $\\chi^2$ increases from the minimum. In the last section, we obtained uncertainties for our fit parameters by allowing just 1 varaible to vary, then observing the $\\Delta\\chi^2$. We may ask, what is the uncertainty profile when we are letting 2 float simultaneously?\n",
    "\n",
    "Lets go back to our lecture slides' where we have the Taylor expansion result that led to the relation of Wilk's theorem. Now, we can write the expansion in terms of all variables $\\vec{\\theta}$:  \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "We can write this out in 2D as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\theta_{a}-\\theta_{a-min} & \\theta_{b}-\\theta_{b-min}\\end{array}\\right)\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\theta_{a}-\\theta_{a-min}\\\\\n",
    "\\theta_{b}-\\theta_{b-min}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "In the case where $\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\approx0$ we can simplify this approximation a lot.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\vec{\\theta})=\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\Delta\\theta_{a} & \\Delta\\theta_{b}\\end{array}\\right)\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\left(\\begin{array}{c}\n",
    "\\Delta\\theta_{a}\\\\\n",
    "\\Delta\\theta_{b}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This all becomes a 2D quadratic equation\n",
    "\n",
    "$$\n",
    "\\begin{align*}\n",
    "\\chi^{2}(x,\\vec{\\theta}) & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\frac{1}{2}\\left(\\begin{array}{cc}\n",
    "\\Delta\\theta_{a}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}}+ & \\Delta\\theta_{b}^{2}\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\\end{array}\\right)\\\\\n",
    " & =\\chi_{min}^{2}(x,\\vec{\\theta})+\\left(\\begin{array}{cc}\n",
    "\\frac{\\Delta\\theta_{a}^{2}}{\\sigma_{\\theta_{a}}^{2}}+ & \\frac{\\Delta\\theta_{b}^{2}}{\\sigma_{\\theta_{b}}^{2}}\\end{array}\\right)\n",
    "\\end{align*}\n",
    "$$\n",
    "\n",
    "I want to point out, we are now profiling two parameters at once in this 2D plot, which means the contribution to $\\chi^{2}$ involves 2 degrees of freedom. You can see this from the above equation, since its the sum of 2 gaussian distributed variables with width $1$ and mean $0$. The 1 $\\sigma$ confidence interval for 2 degrees of freedom is computed by taking $\\Delta \\chi^2(x,\\nu=2)=x~\\mathrm{where~}\\mathrm{cdf}\\left(\\chi^{2}(x,2)=0.683\\right)\\approx2.3$. This is the yellow contour. Contrast this with the case of 1 $\\sigma$ confidence on 1 degree of freedom, where $\\Delta\\chi^2 = 1$. \n",
    "\n",
    "Now, we can go one step further, and note that the above formula  $ a x^2 + b y^2 = c$ is just the form of an ellipse with a specific major and minor axis. As a result, we can define a 3D funciton given by \n",
    "\n",
    "$$ f(x,y) = \\left(\\frac{x-\\bar{a}}{\\sigma_{a}}\\right)^{2} + \\left(\\frac{y-\\bar{b}}{\\sigma_{b}}\\right)^{2} $$\n",
    "\n",
    "Anyway, let's plot the ellipses and compare it to our minimum contours. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "04789bbb",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.3-runcell02\n",
    "\n",
    "#Lets plot the uncertainties  from hess_inv\n",
    "print(np.sqrt(2*sol.hess_inv))\n",
    "#the diagonals are approximately the errors\n",
    "\n",
    "#Make a the expression in the above equation x and x0 are 2 vectors\n",
    "def quadratic2D(x,x0,sigma0):\n",
    "    lVals=x-x0\n",
    "    lVals=(lVals**2)/(sigma0)/sigma0\n",
    "    return np.sum(lVals)\n",
    "\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "\n",
    "#Now plot the ellipse in 3D\n",
    "def plotEllipse(sigx,sigy):\n",
    "    levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    ZQ = np.array([quadratic2D([x,y],sol.x,[sigx,sigy]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "    c = plt.contour(X, Y, ZQ, levels,colors=['red', 'blue', 'yellow','green'],linestyles='dashed')\n",
    "\n",
    "sigx=(solX2.root-solX1.root)/2.\n",
    "sigy=(solY2.root-solY1.root)/2.\n",
    "plotEllipse(sigx,sigy)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f9f50cb",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Remarkably, our space, give by the formula above, yields the solid lines, which matches very closely to the dashed contours obtained by just writing the $\\chi^{2}$ value. This is a profound statement, it really is. \n",
    "\n",
    "Alright, since this gives us the same yellow line for our 2-variable 1$\\sigma$ confidence ellipse. If you look close you do see a difference. This makes us beg the question of what happens when $\\frac{\\partial^{2}\\chi^{2}}{\\partial \\theta_{a}\\theta_{b}}\\neq0$. We'll see soon, when we look at correlations on fitting a different set of parameters."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "07afdca9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_3'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_3) | [Next Section](#section_7_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "284c1eae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.3.1</span>\n",
    "\n",
    "When we allow 2 parameters to vary, for a given confidence level (e.g. 1$\\sigma$) we end up with a confidence ellipse containing parameter values outside the confidence intervals on the individual parameters alone. Compute the 1$\\sigma$ (68.27%) confidence interval bounds for the parameter $a$, based on this ellipse from floating both $a$ and $b$. Enter your answer as a list of number with precision 1 (nearest whole number): `[a_lower, a_upper]`\n",
    "\n",
    "Why do we typically quote the uncertainty from the 1D variation?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a307e5b",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# We can follow the same procedure for finding the confidence interval based on 1D\n",
    "# but for 1-sigma we now use 2.3 \n",
    "# Use the code from the solution to Ex. 7.2.1,\n",
    "ndof=insert value here\n",
    "pval=0.6827#1 sigma p-value\n",
    "val = stats.chi2.ppf(pval,ndof) # 2 DoF this time!\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7ac2152f",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "# We can follow the same procedure for finding the confidence interval based on 1D\n",
    "# but for 1-sigma we now use 2.3 instead of 1 for delta chi^2, since we have 2 DoF.\n",
    "# Using the code from the solution to Ex. 7.2.1,\n",
    "val = stats.chi2.ppf(0.6827,2) # 2 DoF this time!\n",
    "print(\"Delta Chi-square:\", val)\n",
    "\n",
    "def minfunc(x):\n",
    "    return chi2minX(x, delta_chi2=val)\n",
    "\n",
    "a1, a2 = chi2unc(sol, 0, minfunc, 0.2)\n",
    "\n",
    "print(f\"a bounds: [{a1}, {a2}]\")\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0baa7ebc",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "a in roughly [1064, 1082]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Here we find the estimate on $a$ to have uncertainty around $\\pm 9$, as opposed to around $\\pm 6$ when we had only $a$ vary. For consistency, we usually quote the uncertainty on just one variable at a time.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8199b6b8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.4 Correlations Between Fit Parameters: Part 1</h2>  \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_3) | [Exercises](#exercises_7_4) | [Next Section](#section_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "60a8b5a3",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "Ok, so now lets try to understand what happens when our off diagonal terms are off. Let's go back to Wilk's theorem and look at the Hessian for the $\\chi^{2}$, but now with non-zero off-diagonal parameters. \n",
    "\n",
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1c7dc3b1",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.4-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L07/slides_L07_04.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4dc7997c",
   "metadata": {
    "tags": [
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L07/slides_L07_04.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16b152d6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "To understand the point of this, what we are going to do is make the 2D contour plot like the one above with the same contours as before, but now we will use a new function, that is just a reparametrization of the old one. This function will be\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = a x + b (1-x)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This means that the values of $a$ and $b$ now have different meaning, but this is just a linear fit like before and it should give the same overall $\\chi^{2}$ value as before. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0139b15e",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.4-runcell01\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b*(1-x)\n",
    "    return a*x+pVal\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=1000)\n",
    "\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([1000,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.99,sol.x[0]*1.01, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "24bff149",
   "metadata": {},
   "source": [
    "Now, critically lets now make the same ellipse plot with our new best fit values for $a$ and $b$, an dour new uncertainties. Additionally, we can also do the 1D uncertainty scans like we did above again, to see how what uncertainties we get on $a$ and $b$. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "397437db",
   "metadata": {},
   "outputs": [],
   "source": [
    "plotColorsAndContours(X,Y,Z)\n",
    "solX1, solX2 = chi2uncX(sol)\n",
    "solY1, solY2 = chi2uncY(sol)\n",
    "sigx=(solX2.root-solX1.root)/2.\n",
    "sigy=(solY2.root-solY1.root)/2.\n",
    "plotEllipse(sigx,sigy)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "19b09e66",
   "metadata": {},
   "source": [
    "What we can see is that our ellipse differ pretty dramatically from above. Also, we see that our 1D uncertainties are very mis-estimated from what lmfit gives us. The reason for this is that `lmfit` takes into account the full correlation of the variables. Whereas just doing a 1D scan is equivalent to just moving on a single line along the $y$ and $x$ axes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5632189a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_4'></a>     \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_4) | [Next Section](#section_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "657bdd8e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.4.1</span>\n",
    "\n",
    "Run the above fit with just a regular linear fit given by $f(x) = ax + b$. How does the $\\chi^{2}$ change? How do the correlations between the variables change? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cae36ff3",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23570965",
   "metadata": {
    "scrolled": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b\n",
    "    return -1*a*x+pVal\n",
    "\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=0)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([-20,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "print(sol)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.5,sol.x[0]*1.5, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "plotColorsAndContours(X,Y,Z)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "06786d7b",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "$\\chi^{2}$ is to within numerical variations the same. The correlation goes down to almost zero. \n",
    "\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "By reparametrizing the fit, we ahve allowed the function to be equally as expressive as the above funciton. However, we have changed the roles of $a$ and $b$ in the fit. With this new fit $b$ is in charge of getting the overall scale and $a$ is in charge of getting the trend over $\\theta$. Before we had both parameters were in charge of both scale and a combination of slope. Overall our fit has the same shape in the end, but the meaning of $a$ and $b$ is quite different and hence, the off-diagonal terms don't play a big role here. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3b6eb66",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_7_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L7.5 Correlations Between Fit Parameters: Part 2</h2>     \n",
    "\n",
    "| [Top](#section_7_0) | [Previous Section](#section_7_4) | [Exercises](#exercises_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b503280a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now if we go back to our original correlated parameterization, we see that the 1D profile method for obtaining the uncertianties is not the same as the uncertainties we get from the fit.  This is clearly a problem, how can we address this issue? \n",
    "\n",
    "<h3>Dealing with Correlated uncertainties</h3>\n",
    "\n",
    "Looking at our parameters, our uncertainty estimate is smaller than what we observed above. The uncertainties quoted now differ from what we got by varying from the $\\chi^{2}$ minimum. What we are doing is moving up and down the 2D plot and looking at $\\Delta \\chi^{2}$. However, given the parameters are strongly correlated, you see that this doesn't capture the true uncertainty in the sense that we can still move further along $x$ and $y$ and still be within the yellow or even blue ellipse. It's quite clear when you overlay the uncertainty from the quadratic function, which draws circles not ellipses. \n",
    "\n",
    "What, then, is the right uncertainty? \n",
    "\n",
    "Notice our fit with `lmfit` outputs a parameter $C(a,b)=0.872$. This is in fact the correlation between the parameters of $a$ and $b$. We also see from our optimization function that we get something labelled as the `hess_inv`. This we can write noting the relation of the $\\chi^{2}$ Hessian and uncertainties as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x_{i},\\vec{\\theta})=\\chi^{2}_{min}(x_{i},\\vec{\\theta})+\\frac{1}{2}(\\theta_{i}-\\theta_{j})^{T}\\frac{\\partial^{2}}{\\partial \\theta_{i}\\theta_{0}}\\chi^{2}_{min}(x_{i},\\vec{\\theta}_{0})(\\theta_{j}-\\theta_{0})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Which allows us to write the uncertainties as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\frac{2}{\\sigma^{2}}=\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}} \\\\\n",
    " \\sigma^{2}    = 2\\left(\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{i}\\partial\\theta_{j}}\\right)^{-1} \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now in the case where the off diagonals of the Hessian are zero, the parameters were uncorrelated, so \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)\\rightarrow\\left(\\begin{array}{cc}\n",
    "\\frac{2}{\\sigma_{a}^{2}} & 0\\\\\n",
    "0 & \\frac{2}{\\sigma_{b}^{2}}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "But here we have something more complicated. However this is a natural way to define correlations. Lets first verify our intuition for our minimization scheme by taking our $2x2$ Hessian metrix and diagonalizing, computing the eigenvectors and the eigenvalues. We can diagonalize the matrix as: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "A^{-1}2\\left(\\begin{array}{cc}\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}^{2}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}}\\\\\n",
    "\\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{a}\\partial\\theta_{b}} & \\frac{\\partial^{2}\\chi^{2}}{\\partial\\theta_{b}^{2}}\n",
    "\\end{array}\\right)^{-1}A=\\left(\\begin{array}{cc}\n",
    "\\sigma_{1}^{2} & 0\\\\\n",
    "0 & \\sigma_{2}^{2}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is important to note for any N dimensional Hessian, provided the determinant is not zero, we can find a basis of independent variables that are not correlated. That is to say we can always diagonalize our Hessian, and the eigenvectors of our Hessian are the independent values with variances given by the eigenvalues. These are our true uncertainty values (eigenvalues) and direction of variation (eigenvector).  \n",
    "\n",
    "When we run our minimizer, we get Hessian inverse, which we can play with. Let's go ahead and look at the Hessian from our minimizer. From the Hessian, we can import numpy's linear algebra toolkit and compute the eigen vectors and values. Finally, we can draw these vectors and values on our 2D scan plot above. Let's do it:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f37bcfc3",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell01\n",
    "\n",
    "print(np.sqrt(2*sol.hess_inv))\n",
    "#The diagonals are the uncertainty lmfit quotes\n",
    "\n",
    "#Really the best way to do this is to get the eigen values using an linear algebra problem\n",
    "import numpy.linalg as la\n",
    "w, v=la.eig(2*sol.hess_inv)\n",
    "print(\"values\",w,\"vectors\",v)\n",
    "\n",
    "#Now lets plot the eigenvectors\n",
    "from matplotlib.patches import Ellipse\n",
    "def get_cov_ellipse(cov, centre, nstd, **kwargs):\n",
    "    \"\"\"\n",
    "    Return a matplotlib Ellipse patch representing the covariance matrix\n",
    "    cov centred at centre and scaled by the factor nstd.\n",
    "\n",
    "    \"\"\"\n",
    "    # Find and sort eigenvalues and eigenvectors into descending order\n",
    "    eigvals, eigvecs = np.linalg.eigh(cov)\n",
    "    order = eigvals.argsort()[::-1]\n",
    "    eigvals, eigvecs = eigvals[order], eigvecs[:, order]\n",
    "\n",
    "    # The anti-clockwise angle to rotate our ellipse by \n",
    "    vx, vy = eigvecs[:,0][0], eigvecs[:,0][1]\n",
    "    theta = np.arctan2(vy, vx)\n",
    "\n",
    "    # Width and height of ellipse to draw\n",
    "    width, height = 2* nstd * np.sqrt(eigvals) #the two here is because its width/height not radius\n",
    "    return Ellipse(xy=centre, width=width, height=height,angle=np.degrees(theta), **kwargs)\n",
    "\n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+sol.fun\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9b2cb363",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Now the filled in circles make a direct correspondance with the uncertainties above. \n",
    "\n",
    "From the above we can now formulate a description of uncertainties in our system. When we had uncorrelated parameters we had a total uncertainty from our $\\chi^{2}$ given by \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now we have the full ellipse\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{tot}^{2} = \\sigma_{a}^2+\\sigma_{b}^2+2\\sigma_{ab}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where we define that $\\sigma_{ab}=\\rm{COV(a,b)}$. There are a number of ways to call this variable, they all are equivalent, but lets be careful to write it out. We can write the error matrix in 2D as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\left(\\begin{array}{cc}\n",
    "\\sigma_{a}^{2} & {\\rm COV}(a,b)\\\\\n",
    "{\\rm COV}(a,b) & \\sigma_{b}^{2}\n",
    "\\end{array}\\right)=\\sum_{i=1}^{N}\\left(\\begin{array}{cc}\n",
    "\\left(a_{i}-\\bar{a}\\right)^{2} & \\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right)\\\\\n",
    "\\left(a_{i}-\\bar{a}\\right)\\left(b_{i}-\\bar{b}\\right) & \\left(b_{i}-\\bar{b}\\right)^{2}\n",
    "\\end{array}\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where on the right side we have written it in terms of the computation over events. Recall that for a linear regression the slope is just the $\\rm{COV(X,Y)/VAR(X)}$, so the covariance matrix is intricately tied with slope. \n",
    "\n",
    "We can also write it as the correlation matrix where we normalize by the uncertainties:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\rho=\\left(\\frac{1}{\\sigma_{a}}  \\frac{1}{\\sigma_{b}} \\right)^{T}\\left(\\begin{array}{cc}\n",
    "1 & \\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}}\\\\\n",
    "\\frac{{\\rm COV}(a,b)}{\\sigma_{a}\\sigma_{b}} & 1\n",
    "\\end{array}\\right)\\left(\\frac{1}{\\sigma_{a}}   \\frac{1}{\\sigma_{b}} \\right) \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Recall that the covariance is what we originally used to compute the linear slope of the points, this is exactly the same here. In fact, instead of scanning the likelihood analytically we could have sampled the points, and done a linear regression. The resulting slope and line can be related to our eigenvectors. One last thing to mention is that if variables are correlated, we can use the correlation to propagate the uncertainties. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\sigma_{f}^{2} = \\left(\\frac{\\partial f}{\\partial x}\\right)^2\\sigma_{x}^2 + \\left(\\frac{\\partial f}{\\partial y}\\right)^{2}+\\left(\\frac{\\partial f}{\\partial x}\\right)\\left(\\frac{\\partial f}{\\partial y}\\right)\\sigma_{xy}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now lets check we can get the corelation matrix."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19cee2ef",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell02\n",
    "\n",
    "#Now lets get the correlation C(a,b) (see below)\n",
    "w, v=np.linalg.eig(2*sol.hess_inv)\n",
    "print(v)\n",
    "print(\"c(a,b)\",v[0,1]/v[0,0])\n",
    "print(\"A deceptively wrong way to get correlation: since its not normalized\",sol.hess_inv[0,1]/sol.hess_inv[0,0])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "687f1d14",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "Finally, we are going to run our fit "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a713300b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L7.5-runcell03\n",
    "\n",
    "import lmfit\n",
    "\n",
    "def fnew(x,a,b):\n",
    "    pVal=b*(1-x)\n",
    "    return a*x+pVal\n",
    "\n",
    "#Randomly sample points in the above range\n",
    "def maketoy(iy):\n",
    "    toy=np.array([])\n",
    "    #go through the y-values and Poisson fluctuate\n",
    "    for i0 in range(len(iy)):\n",
    "        pVal = np.random.normal (iy[i0],np.sqrt([iy[i0]]))\n",
    "        toy = np.append(toy,float(pVal))\n",
    "    return toy\n",
    "\n",
    "def fittoy(ibin,iy):\n",
    "    #generate toy\n",
    "    toy=maketoy(iy)\n",
    "    #now fit\n",
    "    model  = lmfit.Model(fnew)\n",
    "    p = model.make_params(a=1000,b=10)\n",
    "    xweights=np.array([])\n",
    "    #setup poison weight\n",
    "    for i0 in range(len(toy)):\n",
    "        xweights = np.append(xweights,1./math.sqrt(toy[i0]))\n",
    "    result = model.fit(data=toy,x=ibin, params=p, weights=xweights)\n",
    "    return result.params[\"a\"].value,result.params[\"b\"].value\n",
    "\n",
    "ntoys=1000\n",
    "lAs=np.array([])\n",
    "lBs=np.array([])\n",
    "for i0 in range(ntoys):\n",
    "    pA,pB=fittoy(xhist,yhist)\n",
    "    lAs = np.append(lAs,pA)\n",
    "    lBs = np.append(lBs,pB)\n",
    "\n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x, 1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.plot(lAs,lBs,c='black',marker='.',linestyle = 'None')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ec7f9f1",
   "metadata": {},
   "source": [
    "You can see that if we randomly sample from our distributions, then we get the ellipse variation in $A$ and $B$. The toys have a lot of useful elements. We can use it to get the variation in our parameters.  What we can also do is look at the variance of our parameters and compare it to the Hessian that we can diagonalize to get the fit. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c2c4d75",
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets run a. linear regression\n",
    "def variance(isamples):\n",
    "    mean=isamples.mean()\n",
    "    n=len(isamples)\n",
    "    tot=0\n",
    "    for pVal in isamples:\n",
    "        tot+=(pVal-mean)**2\n",
    "    return tot/n\n",
    "\n",
    "def covariance(ixs,iys):\n",
    "    meanx=ixs.mean()\n",
    "    meany=iys.mean()\n",
    "    n=len(ixs)\n",
    "    tot=0\n",
    "    for i0 in range(len(ixs)):\n",
    "        tot+=(ixs[i0]-meanx)*(iys[i0]-meany)\n",
    "    return tot/n\n",
    "\n",
    "print(\"A:\",lAs.mean(),\"+/-\",lAs.std())\n",
    "print(\"B:\",lBs.mean(),\"+/-\",lBs.std())\n",
    "print(\"Cov:\",covariance(lAs,lBs),\"A Variance:\",variance(lAs),\"B Variance:\",variance(lBs))\n",
    "print(\"Check with Hessian:\",2*sol.hess_inv)\n",
    "print(\"Cor:\",covariance(lAs,lBs)/math.sqrt(variance(lAs)*variance(lBs)),\"A Variance:\",1.,\"B Variance:\",1.)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "49b86041",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_7_5'></a>   \n",
    "\n",
    "| [Top](#section_7_0) | [Restart Section](#section_7_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13dc2f20",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-7.5.1</span>\n",
    "\n",
    "Repeat the above uncorrelated fit $f(x) = a x + b$, plot the ellipse, compute the uncertainties in $a$ and $b$? Do they correspond with lmfit? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "19b2d15a",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def exp_func(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "41cf9943",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "def fnew(x,a,b):\n",
    "    pVal=b\n",
    "    return a*x+pVal\n",
    "\n",
    "##Copying andn pasting everything from above\n",
    "model  = lmfit.Model(fnew)\n",
    "p = model.make_params(a=1000,b=0)\n",
    "result = model.fit(data=yhist,x=xhist, params=p, weights=xweights,scale_covar=False)\n",
    "lmfit.report_fit(result)\n",
    "plt.figure()\n",
    "result.plot()\n",
    "plt.show()\n",
    "\n",
    "x0 = np.array([-20,1000])\n",
    "ps = [x0]\n",
    "sol=opt.minimize(chi2, x0)\n",
    "w, v=la.eig(2*sol.hess_inv)\n",
    "print(\"values\",w,\"vectors\",v)\n",
    "print(\"matrix a:\",np.sqrt(w[0]),\"b:\",np.sqrt(w[1]))\n",
    "print(\"a:\",result.params[\"a\"].stderr,\"b:\",result.params[\"b\"].stderr)\n",
    "\n",
    "x = np.linspace(sol.x[0]*0.5,sol.x[0]*1.5, 100)\n",
    "y = np.linspace(sol.x[1]*0.99,sol.x[1]*1.01, 100)\n",
    "X, Y = np.meshgrid(x, y)\n",
    "Z = np.array([chi2([x,y]) for (x,y) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "err_ellipse=get_cov_ellipse(2*sol.hess_inv,sol.x,1)\n",
    "fig, ax = plt.subplots(1, 1)\n",
    "c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "fig.colorbar(c, ax=ax)\n",
    "levels = [0.1,1.0,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "for i0 in range(len(levels)):\n",
    "    levels[i0] = levels[i0]+sol.fun\n",
    "c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "ax.add_artist(err_ellipse)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "89d2d54a",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "We find the uncertainties are comparable to lmfit. \n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "You will notice that to get the uncertainties we used this `scale_covar=False` formula, this computes the uncertainties from the Hessian in lmfit. Otherwise, it applies a correction to the uncertainties to account for the scenario when the fit $\\chi^{2}$ is large. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "802238db",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.316x Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 8: Fitting Neutrino Data</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7348d65b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3e2dfdd9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_1\">L8.1 Neutrino Oscillations</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_1\">L8.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_2\">L8.2 Loading the Data</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_2\">L8.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_3\">L8.3 Fitting the Master Function to the Data</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_3\">L8.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_8_4\">L8.4 Principal Component Analysis</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_8_4\">L8.4 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "548823b4",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "In this Lesson we will consider the following questions:\n",
    "\n",
    "- Why Do Neutrinos Oscillate?\n",
    "- What does this mean in terms of mesurement?\n",
    "\n",
    "We will also briefly explore the topic of principal component analysis.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "444b6f96",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.0-runcell02\n",
    "\n",
    "import numpy as np                 #https://numpy.org/doc/stable/\n",
    "from scipy import optimize as opt  #https://docs.scipy.org/doc/scipy/reference/optimize.html\n",
    "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html\n",
    "import lmfit                       #https://lmfit.github.io/lmfit-py/ \n",
    "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "import uproot                      #https://uproot.readthedocs.io/en/latest/\n",
    "from sklearn.decomposition import PCA                   #https://scikit-learn.org/stable/modules/generated/sklearn.decomposition.PCA.html\n",
    "from sklearn.datasets import fetch_lfw_people           #https://scikit-learn.org/stable/modules/generated/sklearn.datasets.fetch_lfw_people.html\n",
    "from sklearn.decomposition import PCA as RandomizedPCA  \n",
    "from astroML.datasets import sdss_corrected_spectra"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98847c2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.1 Neutrino Oscillations</h2>  \n",
    "\n",
    "| [Top](#section_8_0) | [Previous Section](#section_8_0) | [Exercises](#exercises_8_1) | [Next Section](#section_8_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "236193f6",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L08/slides_L08_01.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55fd95cf",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.1-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L08/slides_L08_01.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ff58d903",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.1.1</span>\n",
    "\n",
    "For a neutrino beam that has a detector 1000km away, like is present with the DUNE neutrino experiment, what is the optimal energy (in GeV) to observe muon neutrino disappearance given a neutrino energy > 0.5 GeV?\n",
    "\n",
    "Use the master formula below, which outputs the probability of oscillating from a muon neutrino to a muon neutrino, as a function of energy (in units of GeV). Find the minimum of the function to determine where the muon neutrino disappears.\n",
    "\n",
    "Enter your answer as a number with precision 1e-2. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d911ae4b",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L8.1.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def master_formula(E,L=1000):\n",
    "    deltam=1*1e-3\n",
    "    sin2theta23=0.57\n",
    "    xval=1.27*deltam*scale1*L/E\n",
    "    #val=1-4*scale2*sin2theta23*(1-scale2*sin2theta23)*np.sin(xval)**2\n",
    "    val=1-4*sin2theta23*(1-sin2theta23)*(np.sin(xval)**2)\n",
    "    return val\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "85f765e5",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.1.1\n",
    "\n",
    "def master_formula(E,L=1000):\n",
    "    deltam=1*1e-3\n",
    "    sin2theta23=0.57\n",
    "    xval=1.27*deltam*L/E\n",
    "    #val=1-4*scale2*sin2theta23*(1-scale2*sin2theta23)*np.sin(xval)**2\n",
    "    val=1-4*sin2theta23*(1-sin2theta23)*(np.sin(xval)**2)\n",
    "    return val\n",
    "\n",
    "evals=np.arange(0.5,10,0.01)\n",
    "mixvals=master_formula(evals)\n",
    "plt.plot(evals,mixvals)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Energy (GeV)\")\n",
    "print(\"Minimum Energy is \",evals[np.argmin(mixvals)])\n",
    "plt.show()\n",
    "\n",
    "evals=np.arange(0,3000,10)\n",
    "mixvals=master_formula(1,evals)\n",
    "plt.plot(evals,mixvals)\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.xlabel(\"Distance\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "664551c8",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "0.81 GeV \n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "\n",
    "We want to get the maximum amount of muon neutrino disappearance. This is done by finding the minium of the master formula over the full energy range. We see this minimum occurs at 0.81 GeV. Alternatively, we could have varied the length at a fixed energy. \n",
    "    \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7ff65cc6",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    ">#### Follow-up 8.1.1a (ungraded)\n",
    ">\n",
    ">What other parameters of this experiment could we vary to observe the neutrino disappearance?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b727f3a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.2 Loading the Data</h2>  \n",
    "\n",
    "| [Top](#section_8_0) | [Previous Section](#section_8_1) | [Exercises](#exercises_8_2) | [Next Section](#section_8_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f02585e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Fitting Neutrino data</h3>\n",
    "\n",
    "In this part of the lecture, I would like to fit data from one of the recent neutrino experiments. The data consists of events at various energies that are observed from neutrino-matter interactions in the NO$\\nu$A experiment in Minnesota. Details about this experiment can be found <a href=\"https://inspirehep.net/files/0a3cd74d55753d242b2a364ce70a5e0e\" target=\"_blank\">here</a>.\n",
    "\n",
    "There are 3 type of neutrinos, the electron, muon and $\\tau$ neutrino. These neutrinos all interact in roughly the same way, through the weak interaction. Additionally, these neutrinos are all known to be very light. Lastly, it is found that these neutrinos are capable of changing from one type to another over time. What that means is that an electron neutrino can oscillate into a muon neutrino or a $\\tau$ neutrino, and so on. The fact that they oscillate is a bit of a mystery, but what we do know is that this means the way mass is generated for the neutrinos is a different mechanism to the way it interacts.  \n",
    "\n",
    "To understand the data, we need to consider the key components of this experiment, which is that we first create a beam of neutrinos at Fermilab in Illinois, and we then fire this beam at the NO$\\nu$A experiment in Minnesota. At NO$\\nu$A we check to see what we observe. Since neutrinos interact very weakly, we do this by counting interactions of muon neutrinos in two separate detectors, one Fermilab and the other at NO$\\nu$A. Between Fermilab and NO$\\nu$A, some of these muon neutrinos will oscillate into other types of neutrino through quantum mechanical mixing. This is a great way to test the properties of quantum mechanics. You can read more about that <a href=\"https://arxiv.org/abs/1602.00041\" target=\"_blank\">here</a>.\n",
    "\n",
    "That being said, what we expect to compare is the shape of the observed events from the input beam, with the shape of the output beam. Since neutrinos interact very weakly, the way we perform this is we put a large detector near the input beam, and we measure the rate of muon neutrinos, and then we put an even larger detector at the output beam, and we measure the rate. Let's take a look at this data. \n",
    "\n",
    "The data is in root format, like the project. We will use uproot to load the data and see what it is like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fd7586f0",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.2-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "import uproot\n",
    "\n",
    "file = uproot.open(\"data/NOvA_2020_data_histograms.root\")\n",
    "\n",
    "#print(file.classnames())\n",
    "\n",
    "def plot(iLabel,iFile,iColor):\n",
    "    bin_edges = iFile[iLabel].axis().edges()\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    plt.xlabel(\"E (GeV)\")\n",
    "    plt.ylabel(\"$N_{events}$\")\n",
    "    plt.errorbar(bin_centers,iFile[iLabel].values(),yerr=iFile[iLabel].errors(),marker='.',linestyle = '', color = iColor,label=iLabel)    \n",
    "    \n",
    "plot(\"neutrino_mode_numu_quartile1\",file,'black')\n",
    "plot(\"antineutrino_mode_numu_quartile1\",file,'red')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "    "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d793458a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "So, we see two neutrino samples with four quartiles. The quartiles turn out to be different quality selections on the data. Quartile 1 is the most sensitive quartile, whereas Quartile 2, 3, and 4 are progressively less sensitive. How these quartiles are chosen depends on the beam, detector performance, and quality of the reconstruction. For our measurement, we can sum them all up and treat them as one measurement. \n",
    "\n",
    "The other label we see is the anti-neutrino and neutrino labels for the type of beam. The beam at Fermilab can be run in two different modes. One mode is neutrino mode. In this mode, particles are fired into the beam that mostly decay into regular neutrinos. The other mode is anti-neutrino mode, in that scenario particles are fired into the beam that decay into anti-neutrinos. \n",
    "\n",
    "Suffice it to say there is no guarantee that anti-neutrinos and neutrinos oscillate in the same way, so we keep these samples separate. We can look at the separate quartiles, but let's do that later. \n",
    "\n",
    "Instead, let's look at another root file that has the predictions for what we expect the neutrino beam to look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fad2131",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.2-runcell02\n",
    "\n",
    "filePred = uproot.open(\"data/NOvA_2020_data_release_predictions_with_systs_all_hists.root\")\n",
    "\n",
    "#print(filePred.classnames())\n",
    "plot(\"prediction_components_numu_fhc_Quartile1/NoOscillations_Total_pred\",filePred,'black')\n",
    "plot(\"prediction_components_numu_rhc_Quartile1/NoOscillations_Total_pred\",filePred,'red')\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "960998ae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "This file is messy. I won't go into the details but let me put some labels here, to reconcile things. First of all, we see prediction_components_rhc_Quartile. Quartile is the same as before. RHC, and its counterpart FHC standard for \"Reverse Horn Current\" (RHC) and \"Foward Horn Current\".   The FHC configuration focuses charged particles with positive polarity (pions, $\\pi^{+}$ and Kaons, $K^{+}$) which decay to give a neutrino beam ($\\nu_{\\mu}$) whereas, the RHC configuration focuses charged particles with negative polarity (pions,$\\pi^{-}$ and Kaons, $K^{-}$) that decay to give an anti-neutrino enhanced beam ($\\bar{\\nu}_{\\mu}$). \n",
    "\n",
    "Furthermore, the predictions are done under the assumption that there are no oscillations. Hence, the \"NoOscillations\" label."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6731562c",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Neutrino Oscillations</h3>\n",
    "\n",
    "To understand how to fit this data, we follow from the master formula for neutrino oscillations. For those familiar with quantum mechanics, let's write out what the neutrino particle eigen-state is:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " |\\nu_{\\mu}\\rangle = U^{*}_{\\mu 1}|\\nu_{1}\\rangle + U^{*}_{\\mu 2}|\\nu_{2}\\rangle + U^{*}_{\\mu 3}|\\nu_{3}\\rangle\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Where $U_{\\mu i}$ is the muon row of the oscillation matrix. When you evolve each piece $i$ of this state over time $t$ and allow the neutrino of energy $E$ and mass $m_{i}$ to move forward a length $L$, you will get that (skipping some steps) \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " |\\nu_{i}(L)\\rangle = e^{-iEt-\\vec{p}\\cdot\\vec{x}}|\\nu_{i}\\rangle \\\\\n",
    "               \\approx e^{-i\\frac{Lm^{2}_{i}}{2E}}|\\nu_{i}(L=0)\\rangle\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "and, thus, even for identical energies $E$, separate mass eigenstates $\\nu_{1}$, $\\nu_{2}$ and $\\nu_{3}$ will evolve at different rates because of the $m_{i}$ term. What that means is that the probability for neutrinos to still be there can be written by the following master formula: \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " P_{\\mu\\rightarrow\\mu} & = &  \\left|\\langle\\nu_{\\mu}(L)|\\nu_{\\mu}(0)\\rangle\\right|^{2} \\\\\n",
    "                       & \\approx & 1-\\sin^{2}\\theta_{23}\\sin^{2}\\left(\\frac{1.27\\Delta m^{2}_{23}}{E} L\\right)  \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $\\sin^{2}\\theta_{23}$ is the parameter that describes the rate of oscillation between muon neutrinos and $\\tau$ neutrinos, and $m^{2}_{23}=m_{3}^2-m_{2}^2$ is the mass difference between the $\\tau$ and muon neutrino. You may ask, why is the electron neutrino not involved. It turns out that its rate of oscillations is too small to impact this measurement. \n",
    "\n",
    "Given that, what we can do then is take our original data, divide it by our no oscillation expectation and fit it. In this case, what we would like to extract is not just one parameter, but two parameters $\\theta_{23}$ and $m^{2}_{23}$. Let's see if we can get them. \n",
    "\n",
    "First, let's prepare our ratio data, starting by constructing the ratio for each quantile.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2506d164",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.2-runcell03\n",
    "\n",
    "nquartiles=4\n",
    "label=\"neutrino_mode_numu_quartile\"\n",
    "predlabel0=\"prediction_components_numu_fhc_Quartile\"\n",
    "predlabel1=\"/NoOscillations_Total_pred\"\n",
    "bin_edges=file[label+\"1\"].axis().edges()\n",
    "x = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "def ratio(iQuartile,iPlot=False):\n",
    "    ytop=file[label+str(i0+1)].values()\n",
    "    #ytop_err=file[label+str(i0+1)].values()\n",
    "    ytop_err=np.sqrt(file[label+str(i0+1)].values())\n",
    "    ybot=filePred[predlabel0+str(i0+1)+predlabel1].values()\n",
    "    #ybot_err=file[label+str(i0+1)].values() we will skip this since the error is much smaller\n",
    "    y = ytop/ybot\n",
    "    y_err = ytop_err/ybot #we will ignore the ybot error since it is tiny\n",
    "    if iPlot:\n",
    "        plt.errorbar(x,y,yerr=y_err,marker='.',linestyle = '',label=\"Quartile \"+str(i0+1))\n",
    "    return y,y_err\n",
    "    \n",
    "for i0 in range(nquartiles):\n",
    "    ratio(i0,True)\n",
    "    \n",
    "plt.xlabel(\"E(GeV)\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a6cf8051",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "Now, what we would like to do is combine these ratios together. However, we need to do an average weighted by their uncertainties. To do that, we will define for the i-th bin in the ratio $r_{i}$ for the j-th quartile, the <a href=\"https://en.wikipedia.org/wiki/Weighted_arithmetic_mean\" target=\"_blank\">weighted mean</a>\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\bar{r}_{i} = \\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} r_{ij}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is the maximum likelihood mean for normally distributed independent variables (see above). The weighted variance is then given by propagation of errors as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " \\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)^{2}\\sum_{j=1}^{4} \\frac{1}{\\sigma^{4}_{ij}} \\sigma^{2}_{ij} \\\\\n",
    "  \\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)^{2}\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} \\\\\n",
    "\\sigma^{2}_{i} = \\left(\\frac{1}{\\sum_{j=1}^{4} \\frac{1}{\\sigma^{2}_{ij}} }\\right)\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's go ahead and combine them."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63cd4ba8",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.2-runcell04\n",
    "def combinedRatio():\n",
    "    y,y_err = ratio(0,False)\n",
    "    y_arrs=np.array([y])\n",
    "    weight_arrs=np.array([y_err])\n",
    "    for i0 in range(nquartiles):\n",
    "        y,y_err = ratio(i0,False)\n",
    "        y_arrs=np.vstack([y_arrs,y])\n",
    "        weights=1./(y_err**2)\n",
    "        weights[weights == np.inf] = 0.1\n",
    "        weight_arrs = np.vstack([weight_arrs,weights])\n",
    "    #Now do the weighted \n",
    "    yout=np.average(y_arrs,weights=weight_arrs,axis=0)\n",
    "    weights=np.sum(weight_arrs,axis=0)\n",
    "    return yout,1/weights**0.5,weights**0.5\n",
    "\n",
    "label=\"neutrino_mode_numu_quartile\"\n",
    "predlabel0=\"prediction_components_numu_fhc_Quartile\"\n",
    "y,yerr,weights=combinedRatio()    \n",
    "\n",
    "label=\"antineutrino_mode_numu_quartile\"\n",
    "predlabel0=\"prediction_components_numu_rhc_Quartile\"\n",
    "y_anti,yerr_anti,weights_anti=combinedRatio()    \n",
    "\n",
    "plt.errorbar(x,y,yerr=yerr,marker='.',linestyle = '',label=\"neutrino\")\n",
    "plt.errorbar(x,y_anti,yerr=yerr_anti,marker='.',linestyle = '',label=\"anti-neutrino\")\n",
    "plt.xlabel(\"E(GeV)\")\n",
    "plt.ylabel(\"Ratio\")\n",
    "plt.legend()\n",
    "plt.ylim(0,1.5)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e8542f60",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_8_2'></a>     \n",
    "\n",
    "| [Top](#section_8_0) | [Restart Section](#section_8_2) | [Next Section](#section_8_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d6e8f87",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.2.1</span>\n",
    "\n",
    "In this problem, we want to show that the weighted average with weight given by $\\frac{1}{\\sigma^2}$ minmizes the uncertainty. For this case, consider a weighted average of two numbers $x$ and $y$. We can define the weighted average $\\bar{x}$ as \n",
    "\n",
    "$$\\bar{x}=f x + (1-f) y$$\n",
    "\n",
    "where $0 \\leq f \\leq 1$ is our weight factor. The uncertainty on $\\bar{x}$ can be written as \n",
    "\n",
    "\n",
    "$$\\sigma^2_{\\bar{x}}=f^2 \\sigma_{x}^2 + (1-f)^2 \\sigma_{y}^2$$ \n",
    "\n",
    "To minimize the uncertainty all we need to do is \n",
    "\n",
    "$$ \\frac{d\\sigma_{\\bar{x}}^2}{df} = 0$$\n",
    "\n",
    "What is the value of $f$ that minimizes the uncertainty? Express your answer using `sigmax` for $\\sigma_{x}$ and `sigmay` for $\\sigma_{y}$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e9e45709",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "\n",
    "$$ \\frac{d\\sigma_{\\bar{x}}^2}{df} = 2f\\sigma_{x}^2 - 2(1-f)\\sigma_{y}^2 = 2f(\\sigma_{x}^2+\\sigma_{y}^2) - 2\\sigma_{y}^2 = 0$$\n",
    "\n",
    "$$ f = \\frac{\\sigma_{y}^2}{\\sigma_{x}^2+\\sigma_{y}^2} = \\frac{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}}{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}} \\frac{\\sigma_{y}^2}{\\sigma_{x}^2+\\sigma_{y}^2} $$\n",
    "\n",
    "$$ f = \\frac{\\frac{1}{\\sigma_{x}^2}}{\\frac{1}{\\sigma_{y}^2} + \\frac{1}{\\sigma_{x}^2}} $$\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The point being is that weighting by $1/\\sigma^2$ gives you the best weighted mean. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77205726",
   "metadata": {
    "tags": [
     "md",
     "learner"
    ]
   },
   "source": [
    ">#### Follow-up 8.2.1a (ungraded)\n",
    ">\n",
    ">Show that this yields the weighted average formula that we previously derived. Try on your own, or see the solution to the problem above.\n",
    ">\n",
    ">Hint: You might have to multiply your final result by:\n",
    ">\n",
    ">$$\\frac{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}}{\\frac{1}{\\sigma_{x}^2\\sigma_{y}^2}}$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "51599a46",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.3 Fitting the Master Function to the Data</h2>  \n",
    "\n",
    "| [Top](#section_8_0) | [Previous Section](#section_8_2) | [Exercises](#exercises_8_3) | [Next Section](#section_8_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "811a7e8f",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this section, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L08/slides_L08_03.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7147b619",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L08/slides_L08_03.html', width=970, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5d8263ac",
   "metadata": {
    "tags": [
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are discussed in the related video. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L08/slides_L08_01.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L08/slides_L08_03.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6f4c14be",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Ok, now that we have the points, let's finally fit the data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "789833e8",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell01\n",
    "\n",
    "import lmfit\n",
    "deltam=1*1e-3\n",
    "L=810\n",
    "sin2theta23=1.0\n",
    "def func(x,scale1,scale2):\n",
    "    xval=1.27*deltam*scale1*L/x\n",
    "    #val=1-4*scale2*sin2theta23*(1-scale2*sin2theta23)*np.sin(xval)**2\n",
    "    val=1-4*scale2*(1-scale2)*(np.sin(xval)**2)\n",
    "    return val\n",
    "\n",
    "def fit(iX,iY,iWeight):\n",
    "    model  = lmfit.Model(func)\n",
    "    p = model.make_params(scale1=1.0,scale2=0.6)\n",
    "    result = model.fit(x=iX[iY > 0],data=iY[iY > 0], params=p, weights=iWeight[iY > 0])\n",
    "    lmfit.report_fit(result)\n",
    "    result.plot()\n",
    "    print(\"Fit1 chi2 probability: \",stats.chi2.cdf(result.chisqr,result.nfree))\n",
    "\n",
    "fit(x,y,weights)\n",
    "fit(x,y_anti,weights_anti)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8e98b81",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Profiling Neutrino Parameters</h3>\n",
    "\n",
    "So, we see that neutrinos oscillate. However, what if we want to understand how the values of the parameters vary. Let's do a quick scan of the parameters, computing the likelihood for each. \n",
    "\n",
    "We can write the 2 times the log likelihood in terms of the $\\chi^{2}$: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\chi^{2}(x|\\vec{\\theta}) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-f(x_{i}|\\vec{\\theta}))^2}{\\sigma_{i}^{2}} \\\\\n",
    "-2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right) &=& \\sum_{i=1}^{N} \\frac{(x_{i}-f(x_{i}|\\vec{\\theta}))^2}{\\sigma_{i}^{2}} \\\\\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2fdcfd65",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell02\n",
    "\n",
    "def twoLogLike(var,iX=x,iY=y,iWeights=weights):\n",
    "    lTot=0\n",
    "    xtest=func(iX,var[1],var[0])\n",
    "    lTot = weights*(iY-xtest)\n",
    "    return np.sum(lTot**2)\n",
    "\n",
    "from scipy import optimize as opt\n",
    "x0 = np.array([1,1])\n",
    "sol=opt.minimize(twoLogLike, x0)\n",
    "\n",
    "def plotScan(sol):\n",
    "    #Look the same answers, now let's plot the chi2\n",
    "    xscan = np.linspace(sol.x[0]*0.6,sol.x[0]*2.5, 100)\n",
    "    yscan = np.linspace(sol.x[1]*0.6,sol.x[1]*2.0, 100)\n",
    "    X, Y = np.meshgrid(xscan, yscan)\n",
    "    #levels = [0.1,1,2.3,4,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    levels = [4,6,9, 16, 25, 36, 49, 64, 81, 100]\n",
    "    for i0 in range(len(levels)):\n",
    "        levels[i0] = levels[i0]+sol.fun\n",
    "    Z = np.array([twoLogLike([xscan,yscan]) for (xscan,yscan) in zip(X.ravel(), Y.ravel())]).reshape(X.shape)\n",
    "    fig, ax = plt.subplots(1, 1)\n",
    "    c = ax.pcolor(X,Y,Z,cmap='RdBu')\n",
    "    fig.colorbar(c, ax=ax)\n",
    "    c = plt.contour(X, Y, Z, levels,colors=['red', 'blue', 'yellow','green'])\n",
    "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n",
    "    plt.ylabel(\"$\\Delta m^{2}_{23}$\")\n",
    "    plt.show()\n",
    "\n",
    "plotScan(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d1fc31a0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "So now, we see two circles. What exactly does this mean? Let's profile one variable at a time. First we can look at a profile of $\\sin^{2}\\theta_{23}$ for the case where we mix $\\Delta m^{2}_{23}$ to the best fit value. Then we can do the opposite, fixing $\\sin^{2}\\theta_{23}$ to one of the two minima and profiling  $\\Delta m^{2}_{23}$.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "619e57ac",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell03\n",
    "\n",
    "#Now let's fix one parameter at the minimum, and profile the other\n",
    "def scanAxes(sol):\n",
    "    xscan = np.linspace(sol.x[0]*0.8,sol.x[0]*2.2, 100)\n",
    "    yscan = np.linspace(sol.x[1]*0.8,sol.x[1]*1.2, 100)\n",
    "\n",
    "    xLog = np.array([])\n",
    "    for pX in xscan:\n",
    "        xLog = np.append(xLog,twoLogLike(var=[pX,sol.x[1]]))\n",
    "\n",
    "    yLog = np.array([])\n",
    "    for pY in yscan:\n",
    "        yLog = np.append(yLog,twoLogLike(var=[sol.x[0],pY]))\n",
    "\n",
    "    plt.plot(xscan, xLog,label='loglike');\n",
    "    plt.axhline(sol.fun+1, c='red')\n",
    "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n",
    "    plt.ylabel(\"2$\\Delta$LL\")\n",
    "    plt.show()\n",
    "\n",
    "    #Now for the other parameter\n",
    "    plt.plot(yscan,yLog,label='LL');\n",
    "    plt.axhline(sol.fun+1, c='red')\n",
    "    plt.xlabel(\"$\\Delta m^{2}_{23}$\")\n",
    "    plt.ylabel(\"2$\\Delta$LL\")\n",
    "    plt.show()\n",
    "    \n",
    "scanAxes(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "528d02c3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "So, for $\\sin^{2}(\\theta_{23})$ there are actually two minima that have the same minimal value. This is a degeneracy in the ways neutrinos oscillate that this data cannot resolve. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f36f87f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "This is a complex fit that is hard to  intepret. Let's do the scan for anti-neutrino to see if a difference in parameters between anti and regular neutrino.  A difference in the parameters would mean that anti particles behave differently that regular particles. This is known as <a href=\"https://en.wikipedia.org/wiki/CP_violation\" target=\"_blank\">CP-violation</a> and can possibly explain why the universe is made of predominatly matter!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37605587",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell04\n",
    "\n",
    "#answer\n",
    "def twoLogLike(var,iX=x,iY=y_anti,iWeights=weights_anti):\n",
    "    lTot=0\n",
    "    xtest=func(iX,var[1],var[0])\n",
    "    lTot = weights*(iY-xtest)\n",
    "    return np.sum(lTot**2)\n",
    "\n",
    "x0 = np.array([1,1])\n",
    "sol=opt.minimize(twoLogLike, x0)\n",
    "plotScan(sol)\n",
    "scanAxes(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7abc357f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "These look almost exactly like the regular neutrino, so sadly we don't see any CP-violation."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d69c8fbe",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Combining measurements with constraints from the world</h3>\n",
    "\n",
    "Now, let's say we want to combine this measurement with another measurement. The simplest way to imagine this is that we are minimizing our fit with an additional bin, which is the likelihood that our measurement has deviated from the world average. Our likelihood now will be the product of the probabilities of the best fit parameters, with the new results from NO$\\nu$A.  We can write this as\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right) = 2 \\log\\left(\\mathcal{L}(x|\\vec{\\theta})\\right)_{\\rm original} + \n",
    "\\frac{\\left(\\sin \\theta_{23} - \\sin \\theta_{23}^{\\rm best}\\right)^{2}}{\\sigma^{2}_{\\sin \\theta_{23}}} + \\frac{\\left(\\Delta m^{2}_{23} - \\Delta m^{2~\\rm{best}}_{23}\\right)^{2}} {\\sigma^{2~\\rm{best}}_{\\Delta m^{2}_{12}}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This is just equivalent to multiplying the p-values of our fit with a gaussian about the best fit parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78ad2eca",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell05\n",
    "\n",
    "#Now what if we try to add the world's measurement of these parameters into our fit\n",
    "#https://pdg.lbl.gov/2020/listings/rpp2020-list-neutrino-mixing.pdf\n",
    "def twoLogLike(var,iX=x,iY=y,iWeights=weights):\n",
    "    lTot=0\n",
    "    xtest=func(iX,var[1],var[0])\n",
    "    lTot = weights*(iY-xtest)\n",
    "    lTot = np.sum(lTot**2)\n",
    "    sin2worldavg=0.547\n",
    "    sin2uncavg=0.021\n",
    "    constraintsin2=((var[0]-sin2worldavg)**2)/(sin2uncavg**2)\n",
    "    deltamworldavg=2.453\n",
    "    deltamuncavg=0.034\n",
    "    constraintdeltam=((var[1]-deltamworldavg)**2)/(deltamuncavg**2)\n",
    "    return lTot+constraintsin2+constraintdeltam\n",
    "\n",
    "x0 = np.array([1,1])\n",
    "sol=opt.minimize(twoLogLike, x0)\n",
    "plotScan(sol)\n",
    "scanAxes(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9049b24b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "Now you can see that the degneracy is resolved and the best fit parameters are tightly constrained to a very specific set of parameters. Now, we can go ahead and zoom in our best fit set of parameters. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23420b84",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.3-runcell06\n",
    "\n",
    "def scanAxes(sol):\n",
    "    xscan = np.linspace(sol.x[0]*0.9,sol.x[0]*1.2, 100)\n",
    "    yscan = np.linspace(sol.x[1]*0.9,sol.x[1]*1.1, 100)\n",
    "\n",
    "    xLog = np.array([])\n",
    "    for pX in xscan:\n",
    "        xLog = np.append(xLog,twoLogLike(var=[pX,sol.x[1]]))\n",
    "\n",
    "    yLog = np.array([])\n",
    "    for pY in yscan:\n",
    "        yLog = np.append(yLog,twoLogLike(var=[sol.x[0],pY]))\n",
    "\n",
    "    plt.plot(xscan, xLog,label='loglike');\n",
    "    plt.axhline(sol.fun+1, c='red')\n",
    "    plt.xlabel(\"$\\sin^{2}\\Theta_{23}$\")\n",
    "    plt.ylabel(\"2$\\Delta$LL\")\n",
    "    plt.show()\n",
    "\n",
    "    #Now for the other parameter\n",
    "    plt.plot(yscan,yLog,label='LL');\n",
    "    plt.axhline(sol.fun+1, c='red')\n",
    "    plt.xlabel(\"$\\Delta m^{2}_{23}$\")\n",
    "    plt.ylabel(\"2$\\Delta$LL\")\n",
    "    plt.show()\n",
    "    \n",
    "scanAxes(sol)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "21acadc6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_8_3'></a>     \n",
    "\n",
    "| [Top](#section_8_0) | [Restart Section](#section_8_3) | [Next Section](#section_8_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7948684e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.3.1</span>\n",
    "\n",
    "Looking at the world's best fit values, does the NOvA data improve the best fit and, if so, by how much? Does this improvement make sense?\n",
    "\n",
    "**PHIL: Similar to L7, can you make these instructions more explicit. How do you want them to approach this?**\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a2651a68",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L8.3.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def exp_func(x):\n",
    "    return 0\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab4832d0",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.3.1\n",
    "\n",
    "xscan = np.linspace(sol.x[0]*0.9,sol.x[0]*1.2, 1000)\n",
    "yscan = np.linspace(sol.x[1]*0.9,sol.x[1]*1.1, 1000)\n",
    "\n",
    "pXMin=-10\n",
    "pXMax=-10\n",
    "for i0,pX in enumerate(xscan):\n",
    "    xLog = twoLogLike(var=[pX,sol.x[1]])\n",
    "    if xLog < sol.fun+1 and pXMin == -10:\n",
    "        pXMin = 0.5*(xscan[i0-1]+xscan[i0]) \n",
    "    if xLog > sol.fun+1 and pXMin != -10 and pXMax == -10:\n",
    "        pXMax = 0.5*(xscan[i0-1]+xscan[i0]) \n",
    "\n",
    "pYMin=-10\n",
    "pYMax=-10\n",
    "for i0,pX in enumerate(yscan):\n",
    "    xLog = twoLogLike(var=[sol.x[0],pX])\n",
    "    if xLog < sol.fun+1 and pYMin == -10:\n",
    "        pYMin = 0.5*(yscan[i0-1]+yscan[i0]) \n",
    "    if xLog > sol.fun+1 and pYMin != -10 and pYMax == -10:\n",
    "        pYMax = 0.5*(yscan[i0-1]+yscan[i0]) \n",
    "\n",
    "        \n",
    "print(sol.x[0],\"unc sin:\",(pXMax-pXMin)/2.,\"Prev: 0.547+/-0.021\")\n",
    "print(sol.x[1],\"unc m23:\",(pYMax-pYMin)/2.,\"Prev: 2.453+/-0.034\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7dff2cb",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "The uncertainty gets better in m23, but worse in sin2theta23. \n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Adding additional data can change our overall results in interesting ways. If there is tension between the parameters, uncertainties can move the central value and best fit, this is what we see going on here. The NO$\\nu$A results are clearly pulling on the results. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "94c73eca",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_8_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L8.4 Principal Component Analysis</h2>     \n",
    "\n",
    "| [Top](#section_8_0) | [Previous Section](#section_8_3) | [Exercises](#exercises_8_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "818dc421",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Finally, I would like to say that this method of finding the ellipse is our first deep learning method.\n",
    "This procedure of computing the covariance matrix, and finding the eigenvectors is known as  principal component analysis or PCA. Let's run it on our example and look from <a href=\"https://jakevdp.github.io/PythonDataScienceHandbook/05.09-principal-component-analysis.html\" target=\"_blank\">Python Data Science Handbook by Jake VanderPlas</a>.\n",
    "\n",
    "First, what we can do is look at our old correlated fit. All this will do is get the eigenvectors and values for our 2D plot."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bdd59fa6",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell01\n",
    "from sklearn.decomposition import PCA\n",
    "import numpy.linalg as la\n",
    "#make some toy data\n",
    "lAs = np.random.normal(0,1,1000)\n",
    "lBs = np.random.normal(0,1,1000)+0.5*lAs\n",
    "cov = np.cov([lAs,lBs])\n",
    "#eigen cov\n",
    "w, v=la.eig(cov)\n",
    "\n",
    "\n",
    "plt.plot(lAs,lBs,\".\")\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"y\")\n",
    "plt.show()\n",
    "\n",
    "X=(np.vstack([lAs,lBs])).T\n",
    "pca = PCA(n_components=2)\n",
    "pca.fit(X)\n",
    "print(\"PCA vectors\")\n",
    "print(pca.components_)\n",
    "print(\"PCA values\")\n",
    "print(pca.explained_variance_)\n",
    "print(\"Old Eigen\",\"vectors\",w,\"values\",v)\n",
    "\n",
    "def draw_vector(v0, v1, ax=None):\n",
    "    ax = ax or plt.gca()\n",
    "    arrowprops=dict(arrowstyle='->',linewidth=2,shrinkA=0, shrinkB=0)\n",
    "    ax.annotate('', v1, v0, arrowprops=arrowprops)\n",
    "\n",
    "# plot data\n",
    "plt.scatter(lAs, lBs, alpha=0.2)\n",
    "for length, vector in zip(pca.explained_variance_, pca.components_):\n",
    "    v = vector * 3 * np.sqrt(length)\n",
    "    draw_vector(pca.mean_, pca.mean_ + v)\n",
    "plt.axis('equal');"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3fac5e41",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "Finally, we can try this on an ML dataset. Let's take images with many pixels and treat each pixel as a separate dimension. We can then run the decomposition on the image by decomposing the n-pixel by n-pixel correlation matrix. What we will do is take an image that is 62x47=2914 Pixels. From that we can compute the covariance matrix and we can start to diagonlize this. Since this is a large matrix (2914x2914), we have to use sophisticaed eigen decomposition strategies. In this case, we will use something called RandomizedPCA. \n",
    "\n",
    "What we are going to do is run PCA on the images, and take the top 200 eigenvectors from that. Then we are going to plot the top few of these eigen vectors and the cumulative explained variance, which is a metric for how much information can be gained by adding dimensions. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "05851adf",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell02\n",
    "\n",
    "#Now let's do it ML style for fun\n",
    "#Load some faces of images\n",
    "from sklearn.datasets import fetch_lfw_people\n",
    "faces = fetch_lfw_people(min_faces_per_person=60)\n",
    "\n",
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "#Let's plot the eigenvectors\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(faces.data[i].reshape(62, 47), cmap='bone')\n",
    "    \n",
    "#Fit them to PCA \n",
    "from sklearn.decomposition import PCA as RandomizedPCA\n",
    "pca = RandomizedPCA(200)\n",
    "pca.fit(faces.data)\n",
    "fig, axes = plt.subplots(3, 8, figsize=(9, 4),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "#Let's plot the eigenvectors\n",
    "for i, ax in enumerate(axes.flat):\n",
    "    ax.imshow(pca.components_[i].reshape(62, 47), cmap='bone')\n",
    "plt.show()\n",
    "    \n",
    "plt.plot(np.cumsum(pca.explained_variance_ratio_))\n",
    "plt.xlabel('number of components')\n",
    "plt.ylabel('cumulative explained variance');\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dcb0f6fe",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "What you can see above is that the top eigen vectors capture the shape of the face. You can see the shape variations between the different eigenvectors as you go through. Youc an see also that about 80% of the total info in the first 200 eigenvectors is captures in the first 25 components. \n",
    "\n",
    "Finally, what we can do is plot our world leaders just by taking the first 80 eigenvectors of our sample. What we have effectively done is compress our original image into just 80 values, thats all!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cfb7d1e7",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell03\n",
    "\n",
    "# Compute the components and projected faces\n",
    "pca = RandomizedPCA(80).fit(faces.data)\n",
    "components = pca.transform(faces.data)\n",
    "projected = pca.inverse_transform(components)\n",
    "\n",
    "# Plot the results\n",
    "fig, ax = plt.subplots(2, 10, figsize=(10, 2.5),subplot_kw={'xticks':[], 'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "for i in range(10):\n",
    "    ax[0, i].imshow(faces.data[i].reshape(62, 47), cmap='binary_r')\n",
    "    ax[1, i].imshow(projected[i].reshape(62, 47), cmap='binary_r')\n",
    "    \n",
    "ax[0, 0].set_ylabel('full-dim\\ninput')\n",
    "ax[1, 0].set_ylabel('80-dim\\nreconstruction');\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bb371895",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Exercise using stellar spectra</h3>\n",
    "\n",
    "As a final exercise, we are going to isolate features from stellar spectra of the Sloan Digit Sky Survey. To that, let's first load data from the SDSS and try to understand what this looks like. We will use the `astroML` package, that allows us to pull astro data quickly. \n",
    "\n",
    "From this data, we will look at redshift correct galaxy spectra. The correction allows us to look at the large population of galaxies in this dataset and try to find the dominant features. Let's take a quick look at the data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "83deda61",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L8.4-runcell04\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "count=-1\n",
    "#let's plot this guy\n",
    "fig, ax = plt.subplots(2, 10, figsize=(20, 5.5),subplot_kw={'yticks':[]},gridspec_kw=dict(hspace=0.1, wspace=0.1))\n",
    "nrows = 2; ncols = 10\n",
    "for i in range(ncols):\n",
    "    for j in range(nrows):\n",
    "        count=count+1\n",
    "        ax[j,i].plot(wavelengths,spectra_raw[count], '-k', lw=1)\n",
    "        ax[j,i].set_xlim(3100, 7999)\n",
    "        if j < nrows - 1:\n",
    "            ax[j,i].xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            ax[j,i].set_xlabel(r'wavelength $(\\AA)$')\n",
    "            \n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fd2abffb",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "What you see is that the spectra vary, but there are a few lines from the galaxies that are particularly bright. Look at the one above 6000 Angstroms. That the Hydrogen, $\\beta$ emission line. You can see the whole table of lines <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.\n",
    "\n",
    "In the following problem, we will look closely at the dominant eigenvector and identify any spectral lines that it might include."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8039b9bd",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_8_4'></a>   \n",
    "\n",
    "| [Top](#section_8_0) | [Restart Section](#section_8_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "290a15f0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-8.4.1</span>\n",
    "\n",
    "Your task is run PCA, look at the dominant eigenvector, and find the top two spectral lines. What do they correspond to (refer to the table <a href=\"http://astronomy.nmsu.edu/drewski/tableofemissionlines.html\" target=\"_blank\">here</a>.)?\n",
    "\n",
    "Select two spectral lines from the options below (in units of angstroms):\n",
    "\n",
    "- He ~4200\n",
    "- H$\\gamma$ ~4350\n",
    "- H$\\beta$ ~4860\n",
    "- O III ~4950\n",
    "- O III ~5006\n",
    "- H$\\alpha$ ~6560\n",
    "- N II ~6580\n",
    "- O I ~7000\n",
    "\n",
    "Does this make sense?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f135894",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L8.4.1\n",
    "\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "\n",
    "#select only the first 850 bins to avoid noise features\n",
    "spectra_raw = spectra_raw[:,0:850]\n",
    "wavelengths = wavelengths[0:850]\n",
    "pca = RandomizedPCA()\n",
    "\n",
    "\n",
    "#YOUR CODE HERE\n",
    "#fit the pca to the spectra_raw data \n",
    "#plot the first eigenvector, pca.components_[0] over the full range of wavelengths\n",
    "#optionally constrain the wavelength range to zoom in on spectral lines"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9856c822",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.4.1\n",
    "\n",
    "from astroML.datasets import sdss_corrected_spectra\n",
    "\n",
    "data = sdss_corrected_spectra.fetch_sdss_corrected_spectra()\n",
    "wavelengths = sdss_corrected_spectra.compute_wavelengths(data)\n",
    "spectra_raw = data['spectra']\n",
    "spectra_raw = spectra_raw[:,0:850]\n",
    "wavelengths = wavelengths[0:850]\n",
    "\n",
    "pca = RandomizedPCA()\n",
    "pca.fit(spectra_raw)\n",
    "plt.plot(wavelengths,pca.components_[0] , '-k', lw=1)\n",
    "plt.xlabel(r'wavelength $(\\AA)$')\n",
    "#plt.xlim(6500,7000)\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "50ae7266",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "\n",
    "- O III ~5006\n",
    "- H$\\alpha$ ~6560\n",
    "\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "These emissions lines are famous lines from Oxygen and Hydrogen. See for example <a href=\"https://en.wikipedia.org/wiki/Doubly_ionized_oxygen\" target=\"_blank\">here</a>.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1f2fa5ae",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 8.4.1a (ungraded)\n",
    ">\n",
    ">Plot all of the eigenvectors. What are the dominant features of each?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5146fff0",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L8.4.1\n",
    "\n",
    "#Now let's look at the top 100 eigen vectors \n",
    "count=-1\n",
    "fig = plt.figure(figsize=(20, 20))\n",
    "fig.subplots_adjust(left=0.05, right=0.95, wspace=0.05,bottom=0.1, top=0.95, hspace=0.05)\n",
    "nrows = 10; ncols = 10\n",
    "for i in range(ncols):\n",
    "    for j in range(nrows):\n",
    "        count=count+1\n",
    "        ax = fig.add_subplot(nrows, ncols, ncols * j + 1 + i)\n",
    "        ax.plot(wavelengths,pca.components_[count] , '-k', lw=1)\n",
    "        ax.set_xlim(3100, 7999)\n",
    "\n",
    "        ax.yaxis.set_major_formatter(plt.NullFormatter())\n",
    "        ax.xaxis.set_major_locator(plt.MultipleLocator(1000))\n",
    "        if j < nrows - 1:\n",
    "            ax.xaxis.set_major_formatter(plt.NullFormatter())\n",
    "        else:\n",
    "            plt.xlabel(r'wavelength $(\\AA)$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "402331df",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "celltoolbar": "Tags",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
