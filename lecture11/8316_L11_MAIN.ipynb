{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "<i>This code was authored by the 8.316 Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "# LECTURE 11\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_10_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.0 Overview of Learning Objectives</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In this lecture we will explore the following objectives:\n",
    "\n",
    "- Fitting For the Higgs Boson\n",
    "- Combining p-values\n",
    "- Convolutions\n",
    "- Building Interpolated distributions\n",
    "- Dealing with non-analytic forms\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cell below to import the relevant libraries for this notebook."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L11.0-runcell01\n",
    "\n",
    "import numpy as np\n",
    "import lmfit\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "from scipy import stats"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L11.0-runcell02\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_11_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.1 Fitting for the Higgs boson signal Background </h2>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "With all of these pieces together, I would like to compute the significance of the Higgs boson discovery in one of its main channels. To do this, we are going to use all of the tools that we have been going over. Let's first look at the data. For the Higgs boson data, there are 2 years of data each with 5 categories. Here is what all of them look like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import lmfit \n",
    "\n",
    "def pol0(x,p0):\n",
    "    pols=[p0]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def pol1(x,p0,p1):\n",
    "    pols=[p0,p1]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def pol2(x, p0, p1,p2):\n",
    "    pols=[p0,p1,p2]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def pol3(x, p0, p1,p2,p3):\n",
    "    pols=[p0,p1,p2,p3]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def pol4(x, p0, p1,p2,p3,p4):\n",
    "    pols=[p0,p1,p2,p3,p4]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def pol5(x, p0, p1,p2,p3,p4,p5):\n",
    "    pols=[p0,p1,p2,p3,p4,p5]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def fitModel(iX,iY,iWeights,iFunc):\n",
    "    model  = lmfit.Model(iFunc)\n",
    "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0)\n",
    "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n",
    "    #result = lmfit.minimize(binnedLikelihood, params, args=(iX,iY,(iY**0.5),iFunc))\n",
    "    output = model.eval(params=result.params,x=iX)\n",
    "    return output\n",
    "\n",
    "import scipy.stats as stats \n",
    "\n",
    "def residual2(iY,iFunc,iYErr):\n",
    "    residval = (iY-iFunc)\n",
    "    return np.sum(residval**2)\n",
    "    \n",
    "def ftest(iY,iYerr,f1,f2,ndof1,ndof2):\n",
    "    r1=residual2(iY,f1,iYerr)\n",
    "    r2=residual2(iY,f2,iYerr)\n",
    "    sigma2group=(r1-r2)/(ndof2-ndof1)\n",
    "    sigma2=r2/(len(iY)-ndof2)\n",
    "    return sigma2group/sigma2\n",
    "\n",
    "def chi2(iY,iFunc,iYErr,iNDOF):\n",
    "    resid = (iY-iFunc)/iYErr\n",
    "    chi2value = np.sum(resid**2)\n",
    "    chi2prob=1-stats.chi2.cdf(chi2value,len(iY)-iNDOF)\n",
    "    print(\"Mean:\",resid.mean(),\"\\tSTD:\",resid.std(),\"chi2 prob:\",chi2prob)\n",
    "    return chi2value/(len(iY)-iNDOF)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def load(iLabel,iRange=False):\n",
    "    x = np.array([])\n",
    "    y = np.array([])\n",
    "    label=iLabel\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter=' ')\n",
    "        for row in plots:\n",
    "            if not iRange and (float(row[1]) > 150 or float(row[1]) < 110):\n",
    "                continue\n",
    "            x = np.append(x,float(row[1]))\n",
    "            y = np.append(y,float(row[2]))\n",
    "            #add poisson uncertainties                                                                                                 \n",
    "    weights = 1./y**0.5 \n",
    "    return x,y,y**0.5,weights\n",
    "\n",
    "def plot(ax,iLabel):\n",
    "    x,y,y_err,weights=load(iLabel)\n",
    "    #Now we plot it. \n",
    "    ax.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=iLabel)\n",
    "    #ax.x_label(\"$m_{\\gamma\\gamma}$\")\n",
    "    #ax.y_label(\"$N_{events}$\")\n",
    "    ax.legend()\n",
    "    #ax.show()\n",
    "    \n",
    "fig, axs = plt.subplots(2, 3)\n",
    "#2012 data    \n",
    "plot(axs[0,0],\"data/out.txt\")\n",
    "plot(axs[0,1],\"data/out2.txt\")\n",
    "plot(axs[0,2],\"data/out3.txt\")\n",
    "plot(axs[1,0],\"data/out4.txt\")\n",
    "plot(axs[1,1],\"data/out5.txt\")\n",
    "plt.show()\n",
    "\n",
    "fig, axs = plt.subplots(2, 3)\n",
    "#2011 data    \n",
    "plot(axs[0,0],\"data/out_2011.txt\")\n",
    "plot(axs[0,1],\"data/out2_2011.txt\")\n",
    "plot(axs[0,2],\"data/out3_2011.txt\")\n",
    "plot(axs[1,0],\"data/out4_2011.txt\")\n",
    "plot(axs[1,1],\"data/out5_2011.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As you can see, from the above plots, there are way more points in the 2012 data. Lets take the category with the largest number of points, and perform an f-test on it, we can neglect the signal for now, but we will get back to that in a sec.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fitAll(iLabel,iPlot=False):\n",
    "    x,y,y_err,weights=load(iLabel)\n",
    "    result0 = fitModel(x,y,weights,pol0)\n",
    "    result1 = fitModel(x,y,weights,pol1)\n",
    "    result2 = fitModel(x,y,weights,pol2)\n",
    "    result3 = fitModel(x,y,weights,pol3)\n",
    "    result4 = fitModel(x,y,weights,pol4)\n",
    "    result5 = fitModel(x,y,weights,pol5)\n",
    "\n",
    "    if iPlot:\n",
    "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n",
    "        plt.plot(x,result0,label=\"pol0\")\n",
    "        plt.plot(x,result1,label=\"pol1\")\n",
    "        plt.plot(x,result2,label=\"pol2\")\n",
    "        plt.plot(x,result3,label=\"pol3\")\n",
    "        plt.plot(x,result4,label=\"pol4\")\n",
    "        plt.plot(x,result5,label=\"pol5\")\n",
    "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "        plt.ylabel(\"$N_{events}$\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x,y,y_err,result0,result1,result2,result3,result4,result5\n",
    "\n",
    "def ftestAll(iLabel):\n",
    "    x,y,y_err,result0,result1,result2,result3,result4,result5=fitAll(iLabel)\n",
    "    f10=ftest(y,y_err,result0,result1,1,2)\n",
    "    f21=ftest(y,y_err,result1,result2,2,3)\n",
    "    f32=ftest(y,y_err,result2,result3,3,4)\n",
    "    f43=ftest(y,y_err,result3,result4,4,5)\n",
    "    f54=ftest(y,y_err,result4,result5,4,5)\n",
    "    print(\"f 1 to 0:\",1-stats.f.cdf(f10,1,len(y)-1))\n",
    "    print(\"f 2 to 1:\",1-stats.f.cdf(f21,1,len(y)-2))\n",
    "    print(\"f 3 to 2:\",1-stats.f.cdf(f32,1,len(y)-3))\n",
    "    print(\"f 4 to 3:\",1-stats.f.cdf(f43,1,len(y)-4))\n",
    "    print(\"f 5 to 4:\",1-stats.f.cdf(f54,1,len(y)-5))\n",
    "    \n",
    "fitAll(\"data/out.txt\",True)\n",
    "ftestAll(\"data/out.txt\")\n",
    "\n",
    "fitAll(\"data/out2.txt\",True)\n",
    "ftestAll(\"data/out2.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from this looks like a 4th order polynomial gives an f-test above roughly 5% for both the category with the largest yield and the second largest yield. This seems reaonsable for us to use as our background function. Let's proceed with a signal function. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.1.1 </span>\n",
    "\n",
    "When we searched for the Higgs boson, we did this blind. What that means is we did look at the plots of the fits. However, we did do f-tests, and even $\\chi^{2}$ goodness of fit tests. Knowing f-tests are good compute the $\\chi^{2}$ of the best fit for the above. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def chi2testAll(iLabel):\n",
    "    x,y,y_err,result0,result1,result2,result3,result4,result5=fitAll(iLabel)\n",
    "    ###Now we test the chi2\n",
    "    chi2(y,result0,y_err,1)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "\n",
    "def chi2testAll(iLabel):\n",
    "    x,y,y_err,result0,result1,result2,result3,result4,result5=fitAll(iLabel)\n",
    "    ###Now we test the chi2\n",
    "    chi2(y,result0,y_err,1)\n",
    "    chi2(y,result1,y_err,2)\n",
    "    chi2(y,result2,y_err,3)\n",
    "    chi2(y,result3,y_err,4)\n",
    "    chi2(y,result4,y_err,5)\n",
    "    chi2(y,result5,y_err,6)\n",
    "\n",
    "chi2testAll(\"data/out.txt\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "The chi2 is already good \n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "These sort of fits, we call **background only** all have a good quality of fit, and the f-test gives us different information. For signals like this where the background fraction is small, these sort of tests can be done by just looking to see if the chi2 background only fit is good. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.1.2</span>\n",
    "\n",
    "If the signal is large why would this be a form of unblinding (ie a seeing about our signal signfiicance before we look at the data)?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Yes\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "When you are looking for a large signal with high purity, and you are doing a blind analysis, you have to be careful to only look at the signal region or any indicator like the $\\chi^{2}$ goodness of fit when you are read to start looking at the signal. \n",
    "    \n",
    "</div>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_11_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.2 Fitting for the Higgs boson signal Background </h2>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, to fit a Higgs signal, what we want to do is a hypothesis test like we did above. Except now, we will cast our hypothesis, slightly differently to before. \n",
    "\n",
    "**Null Hypothesis** The Higgs signal has a mass of $m_{\\gamma\\gamma}$ at a specific $m_{0}$, and a fixed width 1.2 GeV. \n",
    "\n",
    "**Alternative Hypothesis** The Higgs signal is not there. \n",
    "\n",
    "The reason for the fixed width is that we know the Higgs width from our prediction of the Higgs (is 4 MeV), we also know the detector resolution by measuring photons in the experiment at other regions. In this case, we actually use $Z\\rightarrow ee$ where the electrons are treated as photons. \n",
    "\n",
    "Also, in this case, we are going to fix the mass at 125 GeV, that way the only variable we are floating is the amplitude, and the significance we can quote by taking 2 $\\Delta\\log(\\mathcal{L})$ and noting this should follow a $\\chi^{2}_{1}$ distribution with 1 degree of freedom."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def sigpol4(x,p0,p1,p2,p3,p4,amp,mass,sigma):\n",
    "    bkg=pol4(x,p0,p1,p2,p3,p4)\n",
    "    sig=amp*stats.norm.pdf(x,mass,sigma)\n",
    "    return sig+bkg\n",
    "\n",
    "def sigpol3(x,p0,p1,p2,p3,p4,amp,mass,sigma):\n",
    "    bkg=pol3(x,p0,p1,p2,p3)\n",
    "    sig=amp*stats.norm.pdf(x,mass,sigma)\n",
    "    return sig+bkg\n",
    "\n",
    "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n",
    "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n",
    "    sig=amp*stats.norm.pdf(x,mass,sigma)\n",
    "    return sig+bkg\n",
    "\n",
    "def fitModel(iX,iY,iWeights,iM,iFunc):\n",
    "    model  = lmfit.Model(iFunc)\n",
    "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0,amp=0,mass=iM,sigma=1.2)\n",
    "    try:\n",
    "        p[\"mass\"].vary=False\n",
    "        p[\"sigma\"].vary=False\n",
    "    except:\n",
    "      a=1\n",
    "      #print(\"Mass and Sigma not in fit\")\n",
    "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n",
    "    output = model.eval(params=result.params,x=iX)\n",
    "    return output,result.residual\n",
    "\n",
    "def fitSig(iLabel,iM,SBfunc,Bfunc,iPlot=False):\n",
    "    x,y,y_err,weights=load(iLabel)\n",
    "    resultSB,likeSB=fitModel(x,y,weights,iM,SBfunc)\n",
    "    resultB, likeB =fitModel(x,y,weights,iM,Bfunc)\n",
    "    if iPlot:\n",
    "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n",
    "        plt.plot(x,resultSB,label=\"S+B\")\n",
    "        plt.plot(x,resultB, label=\"B\")\n",
    "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "        plt.ylabel(\"$N_{events}$\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return np.sum(likeB**2)-np.sum(likeSB**2)\n",
    "\n",
    "NLL=fitSig(\"data/out.txt\",125,sigpol4,pol4,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out2.txt\",125,sigpol4,pol4,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out3.txt\",125,sigpol4,pol4,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out4.txt\",125,sigpol4,pol4,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out5.txt\",125,sigpol4,pol4,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Wow, we see a fairly significant Higgs bump at 125, but lets scan the mass and make the so called p-value plot. This is just a plot of the significance as a function of mass. What we will do is move the mass distribution peform the same p-value calculation with the fixed signal, and we will make the plot. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n",
    "    NLL=fitSig(iLabel,pMass,iSBFunc,iBFunc,False)\n",
    "    NLLp = 1-stats.chi2.cdf(NLL,1)\n",
    "    return NLLp\n",
    "\n",
    "def pvaluePlot(iLabel,iSBFunc,iBFunc):\n",
    "    pvalue = np.array([])\n",
    "    massrange=np.linspace(110,150,120)\n",
    "    for pMass in massrange:\n",
    "        pvalue = np.append(pvalue,pvalueCalc(iLabel,pMass,iSBFunc,iBFunc))\n",
    "    return massrange,pvalue\n",
    "\n",
    "m0,p0 = pvaluePlot(\"data/out.txt\",sigpol4,pol4)\n",
    "m1,p1 = pvaluePlot(\"data/out2.txt\",sigpol4,pol4)\n",
    "m2,p2 = pvaluePlot(\"data/out3.txt\",sigpol4,pol4)\n",
    "m3,p3 = pvaluePlot(\"data/out4.txt\",sigpol4,pol4)\n",
    "m4,p4 = pvaluePlot(\"data/out5.txt\",sigpol4,pol4)\n",
    "\n",
    "plt.plot(m0,p0,label=\"Category 1\")\n",
    "plt.plot(m1,p1,label=\"Category 2\")\n",
    "plt.plot(m2,p2,label=\"Category 3\")\n",
    "plt.plot(m3,p3,label=\"Category 4\")\n",
    "plt.plot(m4,p4,label=\"Category 5\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.2.1</span>\n",
    "\n",
    "\n",
    "Compute the Higgs boson p-value signficance plot for category 1 with a 5th order polynomial? How does it compare. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n",
    "    sig=#\n",
    "    bkg=#\n",
    "    return sig+bkg\n",
    "\n",
    "m0,p0 = pvaluePlot(\"out.txt\",sigpol5,pol5)\n",
    "m1,p1 = pvaluePlot(\"out2.txt\",sigpol5,pol5)\n",
    "m2,p2 = pvaluePlot(\"out3.txt\",sigpol5,pol5)\n",
    "m3,p3 = pvaluePlot(\"out4.txt\",sigpol5,pol5)\n",
    "m4,p4 = pvaluePlot(\"out5.txt\",sigpol5,pol5)\n",
    "\n",
    "plt.plot(m0,p0,label=\"Category 1\")\n",
    "plt.plot(m1,p1,label=\"Category 2\")\n",
    "plt.plot(m2,p2,label=\"Category 3\")\n",
    "plt.plot(m3,p3,label=\"Category 4\")\n",
    "plt.plot(m4,p4,label=\"Category 5\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pol5(x, p0, p1,p2,p3,p4,p5):\n",
    "    pols=[p0,p1,p2,p3,p4,p5]\n",
    "    y = np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def sigpol5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n",
    "    bkg=pol5(x,p0,p1,p2,p3,p4,p5)\n",
    "    sig=amp*stats.norm.pdf(x,mass,sigma)\n",
    "    return sig+bkg\n",
    "\n",
    "\n",
    "#answer\n",
    "NLL=fitSig(\"data/out.txt\",125,sigpol5,pol5,True)\n",
    "\n",
    "m03,p03 = pvaluePlot(\"data/out.txt\",sigpol5,pol5)\n",
    "plt.plot(m0,p0,label=\"Category 1 4th order\")\n",
    "plt.plot(m03,p03,label=\"Category 1 5th order\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#Now just for fun, lets run the full p-value plot\n",
    "m0,p0 = pvaluePlot(\"data/out.txt\",sigpol5,pol5)\n",
    "m1,p1 = pvaluePlot(\"data/out2.txt\",sigpol5,pol5)\n",
    "m2,p2 = pvaluePlot(\"data/out3.txt\",sigpol5,pol5)\n",
    "m3,p3 = pvaluePlot(\"data/out4.txt\",sigpol5,pol5)\n",
    "m4,p4 = pvaluePlot(\"data/out5.txt\",sigpol5,pol5)\n",
    "\n",
    "plt.plot(m0,p0,label=\"Category 1\")\n",
    "plt.plot(m1,p1,label=\"Category 2\")\n",
    "plt.plot(m2,p2,label=\"Category 3\")\n",
    "plt.plot(m3,p3,label=\"Category 4\")\n",
    "plt.plot(m4,p4,label=\"Category 5\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Less Significant\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "When you fit with more degrees of freedom, you start to lose sensitivity. The choice of freedom of the function is critical to ensure good sensitivity. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_11_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.3 Combining p-values</h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "Now, if we have 5 experiments each giving a p-value at a specific mass point. How, do we combine these p-values. The strategy is to realize these are each independent experiments. Moreover, it can be shown that if you have a flat prior in probability, and if you take the log of this prior and multiply by 2 $2\\log(p)$ this distribution is approximately that of a $\\chi^{2}$ distribution of 2 degrees of freedom.  That is to say if you have N categories each with equal sensitivity to a signal (flat prior in probability to be in any category), the the sum of the $2\\log(p)$ of the categories gives us a $\\chi^{2}$ distribution. \n",
    "\n",
    "\n",
    "We can see this analytically by noting that $\\chi^{2}$ distribution is given by \n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\nu) = \\frac{1}{2^{\\nu/2}\\Gamma(\\nu/2)} x^{\\nu/2-1}e^{-x/2}\n",
    "\\end{equation}\n",
    "\n",
    "For $\\nu=2$, we have it is just a an exponential distribution given by:\n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^{2}(x,\\nu=2) = \\frac{1}{2}e^{-x/2}\n",
    "\\end{equation}\n",
    "\n",
    "Now for a distribution that is flat is distribution from 0 to 1, such as the $p-value$ of a random measurement. Then if we take the log of that, we find\n",
    "\\begin{equation}\n",
    " y = -2\\log(x)\\rightarrow e^{-\\frac{y}{2}}=x \\\\\n",
    " dx = -\\frac{1}{2}e^{\\frac{-y}{2}} dy \\\\\n",
    "\\end{equation}\n",
    "To equate probability distributions, we want to solve for scenario where the probabilities over a range are equal, namely, to get it as a function of $y$, we can write, noting $p(x)=1$\n",
    "\\begin{equation}\n",
    " p(y) dy = p(x) dx \\\\\n",
    " p(y) = p(x) \\frac{dx}{dy}\\\\\n",
    " p(y) = p(f^{-1}(y)) \\frac{dx}{dy}\\\\\n",
    " p(y) = \\frac{dx}{dy} \\\\\n",
    " p(y)=e^{-\\frac{y}{2}} \\\\\n",
    "\\end{equation}\n",
    "or for $x$ a flat probability distribution from 0 to 1, we have that $y$ has to be distributed such that $p(y)=e^{-\\frac{y}{2}}$ "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "xvals=np.arange(0,18,0.01)\n",
    "samples = np.random.uniform(0,1,10000)\n",
    "plt.hist(-2.*np.log(samples),bins=30,density=True,label='y=log(x) $x\\in[0,1]$')\n",
    "plt.plot(xvals,stats.chi2.pdf(xvals,2),label='$\\chi^{2}_{2}(x)$')\n",
    "plt.xlabel(\"y=log(x)\")\n",
    "plt.ylabel(\"N\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now going back to our calculation, we can immediately relate \n",
    "\\begin{equation}\n",
    "\\chi^{2}_{\\nu=2} = -2 \\log(p_{i})\n",
    "\\end{equation}\n",
    "Now, lets say we have $n$ measurements each with probability $p_{i}$ for the i-th category. If we take the $2\\log(p_{i})$ and sum the distributions, we have a sum of $\\chi^{2}$ distributions of 2 degrees of freedom. This is just a $\\chi^{2}_{\\nu=2n}$ distribution. \n",
    "\n",
    "\\begin{equation}\n",
    "\\chi^{2}_{\\nu=2n} = -2 \\sum_{i=1}^{n} \\log(p_{i})\n",
    "\\end{equation}\n",
    "\n",
    "From this relation, we can immediately get the combined p-value by checking up the p-value of a $\\chi^{2}_{\\nu=2n}$ distribution.  Lets see this in action!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvalueCalc(iLabel,pMass,iSBFunc,iBFunc):\n",
    "    logp=0\n",
    "    for pLabel in iLabel:\n",
    "        NLL=fitSig(pLabel,pMass,iSBFunc,iBFunc,False)\n",
    "        NLLp = 1.-stats.chi2.cdf(NLL,1)\n",
    "        logp = logp - 2.*np.log(NLLp)\n",
    "    pPVal  = 1-stats.chi2.cdf(logp,2*len(iLabel))\n",
    "    return pPVal\n",
    "\n",
    "files=[\"data/out.txt\",\"data/out2.txt\",\"data/out3.txt\",\"data/out4.txt\",\"data/out5.txt\"]\n",
    "mC,pC = pvaluePlot(files,sigpol4,pol4)\n",
    "\n",
    "for pVal in range(4):\n",
    "    sigmas = 1-stats.norm.cdf(pVal+1)\n",
    "    plt.axhline(y=sigmas, color='r', linestyle='-')\n",
    "plt.plot(m0,p0,label=\"Category 1\")\n",
    "plt.plot(m1,p1,label=\"Category 2\")\n",
    "plt.plot(m2,p2,label=\"Category 3\")\n",
    "plt.plot(m3,p3,label=\"Category 4\")\n",
    "plt.plot(m4,p4,label=\"Category 5\")\n",
    "plt.plot(mC,pC,label=\"Combined Category\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "plt.plot(mC,pC,label=\"Category 1\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.yscale(\"log\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Ok, now that we have done that, lets compare our result for the 8TeV measurement in the original paper [here](https://arxiv.org/pdf/1407.0558.pdf). If you look at that plot, you will see that there is a significance of almost 4 standard deviations for the 8TeV data, whereas we only have 3 standard deviations. The reason is that the analysis in the paper is more complicated. To compare our result with something closer, look at the Higgs discovery [paper](https://arxiv.org/pdf/1207.7235.pdf). Here, we are more sensitive, but we also have more data. \n",
    "\n",
    "The way the analysis is more complicated is because of the fact that the combination we are doing here is naive. We are assuming all the categories contribute equally. In reality, we know that from simulation of the signal that the relative contributions of each category is not flat, but weighted. As a consequnce, the trick in this section really only works to get an \"approximate\" sensitivity. The real way to do this is to simultaneously fit all categories with the known, relative signal strength of each of the categories. "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.3.1</span>\n",
    "\n",
    "Noting that the p-value on the y-axis is just a translation of the likelihood, and the p-value is computed from a $\\chi^{2}$ distribution, convert from $\\chi^{2}$ probability back to 2$\\log(\\mathcal{L})$, from this compute the best fit mass for the Higgs boson? How does it compare to the true value\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#Answer, the strategy here is to realize that p-value plot is also 2*Log(L) of our best fit, thus, we just need to go 1 standard deviation from the minimum in likelihood\n",
    "logC = stats.chi2.ppf(1-pC,1)\n",
    "logC = -logC+np.max(logC)\n",
    "plt.plot(mC,logC,label=\"Combined\")\n",
    "plt.ylim(0,2)\n",
    "plt.xlim(124,126)\n",
    "#plt.hlines(1,120,130)\n",
    "plt.xlabel(\"m$_{\\gamma\\gamma}$ (GeV)\")\n",
    "plt.ylabel(\"2$\\Delta$log(L) \")\n",
    "plt.show()\n",
    "#Its roughly 124.8-125.5 => 125.2+/-0.35 (Its almost spot on!)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Our best fit mass is from 124.8 to 125.5 => 125.2 +/- 0.35 (almost spont on!)\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "We can obtain this by noting that the p-value is jsut a translation of the minimum likelihood. So we just need to convert from p-value back to likelihood and scan 2$\\log(\\mathcal{L})$, and viola. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_10_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.4 Function choice </h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, we chose a background function based on a [power law](https://en.wikipedia.org/wiki/Power_law) others you can consider are the [laurent polynomials](https://en.wikipedia.org/wiki/Laurent_polynomial). This will just be taking $x^{-j}$ instead of $x^{j}$. The point for this is that the choice that we made to fit the function had some level of aribratriness. What would our fit performance be if we just chose another fit function somewhat at ranodm? In fact, more generally, if we have another fit function that works, which one is best? \n",
    "\n",
    "This section we will try to address this arbitrariness by noting how these fits behave in general. Let's take a deeper dive into what is going on. we are goin to make a set of progressive more complciated functions\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def bkg1(x,p0,p1):\n",
    "    pols=[p0,p1]\n",
    "    y=np.polyval(pols,x)\n",
    "    #y=np.exp(-p0*(x-p1))\n",
    "    return y\n",
    "\n",
    "def bkg2(x, p0,p1,p2):\n",
    "    #pols=[p0,p1,p2]\n",
    "    #y=np.polyval(pols,x)\n",
    "    y=p0*np.exp(-p1*(x-p2))\n",
    "    return y\n",
    "\n",
    "def bkg3(x, p0, p1,p2,p3):\n",
    "    #pols=[p0,p1,p2,p3]\n",
    "    y=p0*np.exp(-p1*(x-p2))+p3\n",
    "    return y\n",
    "\n",
    "def bkg4(x, p0, p1,p2,p3,p4):\n",
    "    pols=[p3,p4]\n",
    "    y=p0*np.exp(-p1*(x-p2))+np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def bkg5(x, p0, p1,p2,p3,p4,p5):\n",
    "    pols=[p3,p4,p5]\n",
    "    y=p0*np.exp(-p1*(x-p2))+np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def bkg6(x, p0, p1,p2,p3,p4,p5,p6):\n",
    "    pols=[p3,p4,p5,p6]\n",
    "    y=p0*np.exp(-p1*(x-p2))+np.polyval(pols,x)\n",
    "    return y\n",
    "\n",
    "def fitModel(iX,iY,iWeights,iFunc):\n",
    "    model  = lmfit.Model(iFunc)\n",
    "    p = model.make_params(p0=0,p1=0.05,p2=100,p3=0.05,p4=0,p5=0,p6=0)\n",
    "    try:\n",
    "        p['p1'].vary=False\n",
    "    except:\n",
    "        a=1\n",
    "    try:\n",
    "        p['p2'].vary=False\n",
    "    except:\n",
    "        a=1\n",
    "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n",
    "    #lmfit.report_fit(result)\n",
    "    output = model.eval(params=result.params,x=iX)\n",
    "    return output\n",
    "\n",
    "def fitAll(iLabel,iPlot=False):\n",
    "    x,y,y_err,weights=load(iLabel)\n",
    "    result0 = fitModel(x,y,weights,pol0)\n",
    "    result1 = fitModel(x,y,weights,bkg1)\n",
    "    result2 = fitModel(x,y,weights,bkg2)\n",
    "    result3 = fitModel(x,y,weights,bkg3)\n",
    "    result4 = fitModel(x,y,weights,bkg4)\n",
    "    result5 = fitModel(x,y,weights,bkg5)\n",
    "    result6 = fitModel(x,y,weights,bkg6)\n",
    "    \n",
    "    f65=ftest(y,y_err,result5,result6,4,5)\n",
    "    print(\"f 6 to 5:\",1-stats.f.cdf(f65,1,len(y)-1))\n",
    "    if iPlot:\n",
    "        plt.errorbar(x,y,y_err, lw=2,fmt=\".k\", capsize=0,label=\"data\")\n",
    "        plt.plot(x,result0,label=\"bkg0\")\n",
    "        plt.plot(x,result1,label=\"bkg1\")\n",
    "        plt.plot(x,result2,label=\"bkg2\")\n",
    "        plt.plot(x,result3,label=\"bkg3\")\n",
    "        plt.plot(x,result4,label=\"bkg4\")\n",
    "        plt.plot(x,result5,label=\"bkg5\")\n",
    "        plt.plot(x,result6,label=\"bkg5\")\n",
    "        plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "        plt.ylabel(\"$N_{events}$\")\n",
    "        plt.legend()\n",
    "        plt.show()\n",
    "    return x,y,y_err,result0,result1,result2,result3,result4,result5\n",
    "\n",
    "#just showing 4 since its the hardest to fit\n",
    "fitAll(\"data/out.txt\",True)\n",
    "ftestAll(\"data/out.txt\")\n",
    "chi2testAll(\"data/out.txt\")\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So from above, a the second order constant + power law and a linear term is sufficient to fit he data both with a good $\\chi^{2}$ and a good f-test. Notice the f-test is not perfect higher orders. In any case, lets go ahaed and chose this function, and see how this changes our significance? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def fitModel(iX,iY,iWeights,iM,iFunc):\n",
    "    model  = lmfit.Model(iFunc)\n",
    "    p = model.make_params(p0=0,p1=0,p2=0,p3=0,p4=0,p5=0,amp=0,mass=iM,sigma=1.2)\n",
    "    try:\n",
    "        p[\"mass\"].vary=False\n",
    "        p[\"sigma\"].vary=False\n",
    "    except:\n",
    "      a=1\n",
    "      #print(\"Mass and Sigma not in fit\")\n",
    "    result = model.fit(data=iY,params=p,x=iX,weights=iWeights)\n",
    "    output = model.eval(params=result.params,x=iX)\n",
    "    return output,result.residual\n",
    "\n",
    "def sigbkg5(x,p0,p1,p2,p3,p4,p5,amp,mass,sigma):\n",
    "    bkg=bkg5(x,p0,p1,p2,p3,p4,p5)\n",
    "    sig=amp*stats.norm.pdf(x,mass,sigma)\n",
    "    return sig+bkg\n",
    "\n",
    "#answer\n",
    "NLL=fitSig(\"data/out.txt\",125,sigbkg5,bkg5,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out2.txt\",125,sigbkg5,bkg5,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out3.txt\",125,sigbkg5,bkg5,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out4.txt\",125,sigbkg5,bkg5,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n",
    "\n",
    "NLL=fitSig(\"data/out5.txt\",125,sigbkg5,bkg5,True)\n",
    "print(\"2NLL:\",NLL,\"p-value\",1-stats.chi2.cdf(NLL,1))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Our p-value for this one category went down from 0.004 to 0.048. This means we are getting less excess. Could this mean we are more sensitive? Let's do the full combined p-value scan with this assumptions, and compare.  Note that the scan below now starts to take a little bit of time (few min). \n",
    "\n",
    "While you are waiting to run the code, maybe you can start to guess which function will be more sensitive? \n",
    "\n",
    "Moreover, given that, is there anything we gain from having multiple functions? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def pvaluePlot(iLabel,iSBFunc,iBFunc):\n",
    "    pvalue = np.array([])\n",
    "    massrange=np.linspace(120,130,10)\n",
    "    for pMass in massrange:\n",
    "        print(\"Mass:\",pMass)\n",
    "        pvalue = np.append(pvalue,pvalueCalc(iLabel,pMass,iSBFunc,iBFunc))\n",
    "    return massrange,pvalue\n",
    "\n",
    "\n",
    "files=[\"data/out.txt\",\"data/out2.txt\",\"data/out3.txt\",\"data/out4.txt\",\"data/out5.txt\"]\n",
    "mCB2,pCB2 = pvaluePlot(files,sigbkg5,bkg5)\n",
    "\n",
    "for pVal in range(4):\n",
    "    sigmas = 1-stats.norm.cdf(pVal+1)\n",
    "    plt.axhline(y=sigmas, color='r', linestyle='-')\n",
    "plt.plot(mC,pC,label=\"Combined Category (polynomial)\")\n",
    "plt.plot(mCB2,pCB2,label=\"Combined Category (Power law)\")\n",
    "plt.ylim((0.0001,1))\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"p-value\")\n",
    "plt.yscale(\"log\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Now, in light of this, there has been a recent development known as discrete profiling that has allowed to extend the scope of our interpretations. The idea here is that if we account for our degrees of freedom to be different, we can profile a whole library of functions simultaneously, and we can compare all of them by looking at their corrected likelihood. \n",
    "\n",
    "To do this, we are going to do something a little ad-hoc here and just look at one explempary category so that we can really understand how the envelope changes our measurements. What we will do below, is replace our p-value with a likelihood scan, then we will scan around the Higgs mass and show the likelihood"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def logCalc(iLabel,pMass,iSBFunc,iBFunc,iNDOF):\n",
    "    lNLL=iNDOF\n",
    "    for pLabel in iLabel:\n",
    "        pNLL=fitSig(pLabel,pMass,iSBFunc,iBFunc,False)\n",
    "        lNLL -= pNLL\n",
    "    return lNLL\n",
    "\n",
    "def NLLPlot(iLabel,iSBFunc,iBFunc,iNDOF):\n",
    "    NLL = np.array([])\n",
    "    massrange=np.linspace(123,127,40)\n",
    "    for pMass in massrange:\n",
    "        NLL = np.append(NLL,logCalc(iLabel,pMass,iSBFunc,iBFunc,iNDOF))\n",
    "    return massrange,NLL\n",
    "\n",
    "files=[\"data/out2.txt\"]\n",
    "mCP2,lLLP = NLLPlot(files,sigpol4,pol4,5)\n",
    "mCB2,lLLB = NLLPlot(files,sigbkg5,bkg5,4)\n",
    "\n",
    "plt.plot(mCP2,lLLP,label=\"Combined Category (polynomial)\")\n",
    "plt.plot(mCB2,lLLB,label=\"Combined Category (Power law)\")\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$-2\\Delta LL$\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The true minimum is defined by the envelope in the corrected likelihood, that really gives us the full information that enables us to quote a result. Discrete profiling thus enables us to get even more sensitive if our envelope narrows. However, its often that it widens, this is the uncertainty coming in from function choice. The point being is that by being more expressive by adding more function choices, we naturally give the fit more flexibility to fit well. If there are more options that fit well, then our uncertainties get larger."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.4.1</span>\n",
    "\n",
    "Based on the above envelope compute the Higgs mass? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "minall=np.minimum(lLLB.flatten(),lLLP.flatten())\n",
    "minall=np.min(minall)\n",
    "plt.plot(mCB2,np.minimum(lLLB,lLLP)-minall,label=\"Min LL\")\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$-2\\Delta LL$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Our best fit mass is from 124.4 to 125.3 => 124.8 +/- 0.35 (almost spont on!)\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "The minimum is not now a quadratic and has a bulge on the left. This shows we have added a systematic for function choice and made the uncertainties slightly larger. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_10_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.5 Convolutions</h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "\n",
    "Convlutions are a critical component of evey good statistical analysis. Its a way to multiply distributions together. \n",
    "Lets build convlutions up by scratch and then go from there. \n",
    "\n",
    "The core concept of a convolution is that you are effectively multiplying distributions. Given two functions $f(x)$ and $g(x)$, we can define convolutions by \n",
    "\\begin{eqnarray}\n",
    "(f*g)(z) &=& \\int^{\\infty}_{-\\infty} f(z-t)g(t)dt\n",
    "\\end{eqnarray}\n",
    "For data analysis, we usually think about this in the context of probability distribuitons $g$ and $f$. From here, we construct a new probability distribution $(f*g)$. Anyway, lets take a look at how it works. \n",
    "\n",
    "![alternative text](convolution.gif)\n",
    "\n",
    "To do this, lets first define some functions to convolve. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#First lets define a triangular distribution\n",
    "def triangle(x,mean=5):\n",
    "    Norm=mean*mean\n",
    "    val=np.where(x <= mean,np.maximum(x,np.zeros(len(x))), np.maximum(2*mean-x,np.zeros(len(x))))\n",
    "    return val/Norm\n",
    "\n",
    "#Now define the gaussian\n",
    "def gaussian(x,mean=0,sigma=1):\n",
    "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets do a convolution by hand\n",
    "def convolve(f1,f2,x,sigma=1,iMin=-10,iMax=10,iN=2000):\n",
    "    step=(iMax-iMin)/iN\n",
    "    pInt=0\n",
    "    for i0 in range(iN):\n",
    "            pX   = np.repeat(i0*step+iMin,len(x))\n",
    "            pVal = f1(x-pX,sigma=sigma)*f2(pX)\n",
    "            pInt += pVal*step\n",
    "    return pInt"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy import stats\n",
    "\n",
    "#now lets plot\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-10, 15, 100)\n",
    "tri_out=triangle(x_in)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,triangle,x_in)\n",
    "conv2_out=convolve(gaussian,triangle,x_in,sigma=2)\n",
    "\n",
    "ax.plot(x_in,tri_out,label='triangle')\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So why are these convolutions so important? \n",
    "\n",
    "We can get a picture of how important these are \n",
    "\n",
    "![alternative text](gauss_blur.png)\n",
    "\n",
    "Effectively, when we have poor resolution of things coming from an uncertainty in how we measure something. Distributions, which are originally nice and sharp can get smeared out. The easy way to describe all of these is through a convolution. There is a rich literature to discribe convolutions. \n",
    "\n",
    "A critical element to remember about the convolutions is that in Fourier space convolutions can be performed as straight multiplications and then inversing the fourier transform.  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.5.1</span>\n",
    "\n",
    "What happens if you convolve a Gaussian with a straight line: $y=0$? You can use the code from the previous cell to plot it.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "def line(x):\n",
    "  return x \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-10, 15, 100)\n",
    "line_out=line(x_in)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,line,x_in)\n",
    "conv2_out=convolve(gaussian,line,x_in,sigma=5)\n",
    "\n",
    "ax.plot(x_in,line_out,label='line')\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "See code above\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The convolution will stretch out the line by weighting points with a gaussian. The stretching only occurs on the boundary where the convolution is underway. As a result, the core line doesn't actually change in shape. \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.5.2</span>\n",
    "\n",
    "What happens if you convolve a Gaussian with a Gaussian? (you can again use the code before it)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-10, 15, 100)\n",
    "line_out=line(x_in)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,gaussian,x_in)\n",
    "conv2_out=convolve(gaussian,gaussian,x_in,sigma=5)\n",
    "\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.plot(x_in,conv2_out,label='convolved($\\sigma=5$)')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "See code above\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "A Gauassian convolved with a gaussian just gives you another Gaussian, you can prove this by fourier transforming the convolution and noting that according to the [Convolution theorem](https://en.wikipedia.org/wiki/Convolution_theorem) a convolution is just equal to a product of fourier transforms. \n",
    "\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### challenge question\n",
    "\n",
    "What does $f(x)=\\sin(x)$ convoled with a gaussian look like? How does it change by the resolution of the gaussian?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    return np.sin(x)*x\n",
    "\n",
    "#now lets plot\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-10, 15, 100)\n",
    "func_out=func(x_in)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,func,x_in)\n",
    "conv2_out=convolve(gaussian,func,x_in,sigma=2)\n",
    "\n",
    "ax.plot(x_in,func_out,label='sin(x)')\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.plot(x_in,conv2_out,label='convolved($\\sigma=2$)')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#answer\n",
    "def func(x):\n",
    "    return np.sin(x)*x\n",
    "\n",
    "#now lets plot\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-10, 15, 100)\n",
    "sin_out=sin(x_in)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,sin,x_in,sigma=2)\n",
    "\n",
    "\n",
    "ax.plot(x_in,sin_out,label='triangle')\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.5.3</span>\n",
    "\n",
    "Often we use convolutions to \"Smear\" our distributions analytically without having to build a simulation. In the code below we sample points from a gaussian. Take the original function below ( a box disribution), add points sampled by a gaussian, and show that the distribution matches the convolution."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "lNToys=1000\n",
    "nbins=100\n",
    "sigma=1\n",
    "smeared=np.random.normal(0,sigma,(lNToys,nbins))\n",
    "x_in=np.linspace(-20, 20,(nbins))\n",
    "smeared_x_in=#your code here, make 1000 example x distributiosn from above\n",
    "\n",
    "def func(x):\n",
    "    return 0+0.1*np.where(x < -10,0,1) - 0.1*np.where(x < 10,0,1)\n",
    "\n",
    "box_out=func(x_in)\n",
    "smeared_box_out=#your code here, average over 1000 sampled distributions\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "#>>>RUN\n",
    "lNToys=1000\n",
    "nbins=100\n",
    "sigma=1\n",
    "smeared=np.random.normal(0,sigma,(lNToys,nbins))\n",
    "x_in=np.linspace(-20, 20,(nbins))\n",
    "smeared_x_in=x_in+smeared\n",
    "\n",
    "def func(x):\n",
    "    return 0+0.1*np.where(x < -10,0,1) - 0.1*np.where(x < 10,0,1)\n",
    "\n",
    "#now lets plot\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(-20, 20, 100)\n",
    "box_out=func(x_in)\n",
    "smeared_box_out=np.mean(func(smeared_x_in),axis=0)\n",
    "gaus_out=gaussian(x_in)\n",
    "conv_out=convolve(gaussian,func,x_in)\n",
    "\n",
    "\n",
    "ax.plot(x_in,box_out,label='box(x)')\n",
    "ax.plot(x_in,smeared_box_out,label='smeared box(x)')\n",
    "ax.plot(x_in,gaus_out,label='gaussian')\n",
    "ax.plot(x_in,conv_out,label='convolved')\n",
    "ax.set(xlabel='x(t)', ylabel='y(t)',title='Convolutions')\n",
    "ax.grid()\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "See code above\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "This is a first demonstration of a monte carlo technique to approximate a resolution. In Monte Carlo, we randomly sample distributions and then simulate them to mimic the effect that we would like our output to look like. In this case, we are smearing our inputs and applying a function on our smeared inputs. Knowing when to integrate and when to sample is something that is very powerful as we go forward. \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_11_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.6 Building Interpolated Distributions</h2>  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "To find the right fit function, we used a library of functions to profile and find the minimum. Thanks to the f-statistic, we don't need to just throw one function at the problem, we can throw many. In fact, modern searches aiming for the most sensitivity will send a library of functions to fit a signal and not just one. For a more detailed analysis of how you would do this look at this [paper](https://arxiv.org/pdf/1408.6865.pdf). \n",
    "\n",
    "However, what we can also do is actually build functions by just throwing it to the data. To give you two ways to do this, let's first try a spline interpolated function. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#Now lets load some data and do gaussian kernals with it\n",
    "x,y,y_err,weights=load(\"data/out_2011.txt\")\n",
    "\n",
    "\n",
    "from scipy import interpolate\n",
    "tck = interpolate.splrep(x, y) #setup the spline\n",
    "x2 = np.linspace(110, 150) #range\n",
    "y2 = interpolate.splev(x2, tck)#apply the spline\n",
    "\n",
    "plt.plot(x, y, 'go',label='data')\n",
    "plt.plot(x2, y2, 'b',label='spline')\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "So what happened is, we just took the data and turned it into a smooth function that we can evaulate anywhere. This is done by chunking up the data into little pieces and fitting higher order polynomials to each. We end up with a function. \n",
    "\n",
    "Since its a function, we can do whatever we want with it. Let's smooth this function out by convolving it with a Gaussian distribution. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#spline convolve by hand\n",
    "def splineconvolve(tck,f2,x,iMin=-15,iMax=15,iN=500):\n",
    "    step=float((iMax-iMin))/float(iN)\n",
    "    pInt=0\n",
    "    for i0 in range(iN):\n",
    "            pX   = i0*step+iMin\n",
    "            pVal = interpolate.splev(x-pX,tck)*f2(pX)\n",
    "            pInt += pVal*step\n",
    "    return pInt\n",
    "\n",
    "def gaussian(x,mean=0,sigma=5):\n",
    "    return 1./(sigma * np.sqrt(2 * np.pi)) * np.exp( - (x - mean)**2 / (2 * sigma**2)) \n",
    "\n",
    "fig, ax = plt.subplots()\n",
    "x_in=np.linspace(122, 142, 10)\n",
    "conv_out=[]\n",
    "for val in x_in:\n",
    "    pConv_out=splineconvolve(tck,gaussian,val)\n",
    "    conv_out.append(pConv_out)\n",
    "\n",
    "#now we cna plot it\n",
    "plt.plot(x, y, 'go',label='data')\n",
    "plt.plot(x2, y2, 'b',label='spline')\n",
    "plt.plot(x_in,conv_out,c='orange',label='convolution')\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "In fact, in recent times interpolation with all sorts of functions, not just polynomials have become very popular. Just to show you another one that is used often, lets try Gaussian processes. Gaussian processes, aim to fit guassian distributiosn to describe the data. The strategy, conceptually, is like the f-test. Keep adding Gaussian's to fit the data until it is well described. See [here](https://en.wikipedia.org/wiki/Gaussian_process) for a much deeper explanation for how this would work. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install george\n",
    "import george\n",
    "from george import kernels\n",
    "\n",
    "kernel = np.var(y) * kernels.Matern52Kernel(5.0)\n",
    "#kernel = np.var(y) * kernels.Matern52Kernel(125.0)\n",
    "gp = george.GP(kernel)\n",
    "gp.compute(x, y_err)\n",
    "x_pred = np.linspace(110, 150, 100)\n",
    "pred, pred_var = gp.predict(y, x_pred, return_var=True)\n",
    "\n",
    "plt.fill_between(x_pred, pred - np.sqrt(pred_var), pred + np.sqrt(pred_var),color=\"k\", alpha=0.2,label=\"gaussian Process\")\n",
    "plt.plot(x_pred, pred, \"k\", lw=1.5, alpha=0.5)\n",
    "plt.errorbar(x, y, yerr=y_err, fmt=\".k\", capsize=0)\n",
    "plt.plot(x2, y2, 'b',label='spline')\n",
    "plt.plot(x_in,conv_out,c='orange',label='convolution')\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "As with the spline, there are parameters for how it averages over the points. Try the kernal 125 above, you will see that this is is effectively changing the size of the window of interpolation. This will allow us to either smooth or unsmooth our distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-11.6.1</span>\n",
    "\n",
    "When we we have a spline function, we can now do coordinate tansforms of whole distributions, take the above spline, shift it by a 5 GeV and plot it again? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "xs  = ###Your code here\n",
    "ys  = ###Your code here\n",
    "\n",
    "#now we cna plot it\n",
    "plt.plot(x, y, 'go',label='data')\n",
    "plt.plot(x2, y2, 'r',label='spline')\n",
    "plt.plot(xs, ys, 'b',label='spline shift')\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "fig, ax = plt.subplots()\n",
    "\n",
    "xs  = np.linspace(110,145,100)\n",
    "ys  = interpolate.splev(xs+5, tck)#apply the spline\n",
    "\n",
    "#now we cna plot it\n",
    "plt.plot(x, y, 'go',label='data')\n",
    "plt.plot(x2, y2, 'r',label='spline')\n",
    "plt.plot(xs, ys, 'b',label='spline shift')\n",
    "plt.xlabel(\"$m_{\\gamma\\gamma}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a name='section_10_8'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L11.7 Dealing with non-analytic forms</h2>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "\n",
    "At this point, we have gone a bit away from just fitting functions. Its not always the case that we have a nice function that we can fit to describe our data. It is sometimes the case that we have a simulated shape. For High energy physics, simulated shapes often come from so called \"Monte-Carlo\" simulation. In this approach, we construct distributions by randomly sampling many millions of events. The resulting simulated events can then be treated like data. The data set below represents another Higgs boson channel, the Higgs decay to 4 leptons. However, in this instance, we want to fit the peak at 90 with a simulation of the peak. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "x,y_data,y_err,weights=load(\"data/data.txt\",True)\n",
    "x,y_mc,y_mc_err,_=load(\"data/zz_narrow.txt\",True)\n",
    "\n",
    "tck = interpolate.splrep(x, y_mc)\n",
    "x2 = np.linspace(50, 160,1000)\n",
    "y2 = interpolate.splev(x2, tck)\n",
    "plt.errorbar(x,y_data,yerr=y_err,marker='.',linestyle = 'None', color = 'black')\n",
    "plt.plot(x2, y2, 'b')\n",
    "plt.plot(x,y_mc,drawstyle = 'steps-mid')\n",
    "plt.xlabel(\"$m_{4\\ell}$\")\n",
    "plt.ylabel(\"$N_{events}$\")\n",
    "plt.legend()\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "There are clearly a few things off. First of all the number of events is off. Secondly, the shapes don't look exactly the same. \n",
    "\n",
    "Oftentimes, when we actually want to do a precision fit, we will rely on our simulated samples to extract the signal. What we will do is allow the shape to be modified by a number of different approaches. One appraoch is to apply a numerical convolution of the shape with a gaussian distribution, so that we can smear it out, making it wider. This what we will do here. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#spline convolve by hand\n",
    "def splineconvolvegaus(x,mean,sigma,iMin=-15,iMax=15,iN=500):\n",
    "    step=float((iMax-iMin))/float(iN)\n",
    "    pInt=0\n",
    "    for i0 in range(iN):\n",
    "            pX   = i0*step+iMin\n",
    "            pVal = interpolate.splev(x-pX,tck)*gaussian(pX,mean,sigma)\n",
    "            pInt += pVal*step\n",
    "    return pInt\n",
    "\n",
    "def gausconv(x,mean,sigma,amp,a,b):\n",
    "    val=splineconvolvegaus(x,mean,sigma)*amp\n",
    "    val=a + b*x + val\n",
    "    return val\n",
    "\n",
    "model  = lmfit.Model(gausconv)\n",
    "p = model.make_params(mean=0,sigma=1.0,amp=3.0,a=1,b=0)\n",
    "p[\"sigma\"].value=0.1\n",
    "#p[\"sigma\"].vary=False\n",
    "result = model.fit(data=y_data,params=p,x=x,weights=weights)\n",
    "lmfit.report_fit(result)\n",
    "result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "This doesn't fully get the data correct, but its definitely a big step towards getting something much better. You will have to use these distributions in future datasets, so you will see more about how things. Just to get a better idea of what is going in this fit, lets actually look at what happens, if we don't allow the peak to move aroundn, and we also fix the resolution to be narrow. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def splineconvolvegaus(tck,f2,x,mean,sigma,iMin=-15,iMax=15,iN=500):\n",
    "    step=float((iMax-iMin))/float(iN)\n",
    "    pInt=0\n",
    "    for i0 in range(iN):\n",
    "            pX   = i0*step+iMin\n",
    "            pVal = interpolate.splev(x-pX,tck)*f2(pX,mean,sigma)\n",
    "            pInt += pVal*step\n",
    "    return pInt\n",
    "\n",
    "def gausconv(x,mean,sigma,sig,baseline,slope):\n",
    "    val=splineconvolvegaus(tck,gaussian,x,mean,sigma)\n",
    "    output = baseline+sig*val+slope*x\n",
    "    return output\n",
    "\n",
    "model  = lmfit.Model(gausconv)\n",
    "p = model.make_params(mean=0,sigma=1,sig=2,baseline=2,slope=0)\n",
    "p[\"mean\"].vary = False\n",
    "p[\"sigma\"].vary = True\n",
    "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n",
    "lmfit.report_fit(result)\n",
    "result.plot()\n",
    "\n",
    "#Now lets not smear the data\n",
    "p[\"sigma\"].value = 0.01\n",
    "p[\"sigma\"].vary = False\n",
    "result = model.fit(data=y_data, params=p, x=x, weights=weights)\n",
    "lmfit.report_fit(result)\n",
    "_=result.plot()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "##  Going beyond\n",
    "\n",
    "This is just the start of construction fit functions. The next set of tools we can explore is how to build deep learning algorithms to model the data. As we have gone through this lecture, and the previous lectures, we have been abstracting and standardizing the tools more and more to get to the point of automating the whole procedure that is the point and this is where we start for the next sections of the course. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
