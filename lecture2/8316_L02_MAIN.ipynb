{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d6d3c5b9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<hr style=\"height: 1px;\">\n",
    "<i>This notebook was authored by the 8.316 Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "<h1>Lesson 2: Binomial, Poisson, and Gaussian Distributions</h1>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f15f03f",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.0 Overview</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "63e300db",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Navigation</h3>\n",
    "\n",
    "<table style=\"width:100%\">\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_1\">L2.1 Introduction to Binomial Distribution</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_1\">L2.1 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_2\">L2.2 Applications Using the Binomial Distribution</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_2\">L2.2 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_3\">L2.3 The Poisson Distribution</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_3\">L2.3 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_4\">L2.4 Poisson Distribution Continued</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_4\">L2.4 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_5\">L2.5 The Gaussian Distribution</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_5\">L2.5 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_6\">L2.6 Uncertainties in Measurement</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_6\">L2.6 Exercises</a></td>\n",
    "    </tr>\n",
    "    <tr>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#section_2_7\">L2.7 Propagating Uncertainties</a></td>\n",
    "        <td style=\"text-align: left; vertical-align: top; font-size: 10pt;\"><a href=\"#exercises_2_7\">L2.7 Exercises</a></td>\n",
    "    </tr>\n",
    "</table>\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a864232e",
   "metadata": {
    "tags": [
     "learner",
     "catsoop_00",
     "md"
    ]
   },
   "source": [
    "<h3>Learning Objectives</h3>\n",
    "\n",
    "By the end of this Lesson, you should be able to do the following:\n",
    "\n",
    "- Understand how and when to use Binomial, Poisson, and Gaussian distributions.\n",
    "- Understand how uncertainties propagate in measurements"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d39fe23b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<h3>Importing Data (Colab Only)</h3>\n",
    "\n",
    "If you are in a Google Colab environment, run the cell below to import the data for this notebook. Otherwise, if you have downloaded the course repository, you do not have to run the cell below.\n",
    "\n",
    "See the source and attribution information below:\n",
    "\n",
    ">data: data/L02/tmpdata.txt, data/L02/tmpmc.txt<br>\n",
    ">source:  https://arxiv.org/pdf/1104.0699.pdf<br>\n",
    ">attribution: CDF Collaboration, arXiv:1104.0699v2<br>\n",
    ">license type: https://arxiv.org/licenses/nonexclusive-distrib/1.0/license.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4f25a8db",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Importing Libraries</h3>\n",
    "\n",
    "Before beginning, run the cell below to import the relevant libraries for this notebook.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec93314",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.0-runcell01\n",
    "\n",
    "# The documentation to these packages is linked beside them if you have questions\n",
    "\n",
    "import numpy as np                 #https://numpy.org/doc/stable/ \n",
    "from scipy.special import comb     #https://docs.scipy.org/doc/scipy/reference/special.html\n",
    "import scipy.stats as stats        #https://docs.scipy.org/doc/scipy/reference/stats.html\n",
    "import matplotlib.pyplot as plt    #https://matplotlib.org/stable/api/_as_gen/matplotlib.pyplot.html"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0286eb9f",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "109098ea",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.0-runcell02\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (7,4)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dfdd1e71",
   "metadata": {},
   "outputs": [],
   "source": []
  },
  {
   "cell_type": "markdown",
   "id": "1227be74",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "<a name='section_1_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L1.4 Expectation and Variance</h2>\n",
    "\n",
    "| [Top](#section_1_0) | [Previous Section](#section_1_3) | [Exercises](#exercises_1_4) | [Next Section](#section_1_5) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7e46e458",
   "metadata": {
    "tags": [
     "learner",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Definitions</h3>\n",
    "\n",
    "We can use PDFs to define the expectation $E[x]$ for a continuous variable,\n",
    "\n",
    "$$E[x]=\\int_{-\\infty}^{\\infty}xp(x)dx$$\n",
    "\n",
    "or, in other words, the value of $x$ weighted by its PDF. This is also commonly referred to as the mean or average of the distribution. \n",
    "\n",
    "Furthermore, we can also define the variance of this distribution, as: \n",
    "\n",
    "$$V[x]=\\int_{-\\infty}^{\\infty}\\left(x-E[x]\\right)^{2}p(x)dx$$\n",
    "\n",
    "or, in other words, the spread of the numbers about the mean of the distribution. The variance holds an important interpretation. It is a measure of the width of our distribution, and we often use this as a way to describe the uncertainty of our measurement. Uncertainty is simply defined to be the square root of the above, $\\sqrt{V(x)}$, which we refer to as the standard deviation $\\sigma$. We will clarify this interpretation more in a later section. \n",
    "\n",
    "$$\\sigma=\\sqrt{V(x)}$$\n",
    "\n",
    "You will often see the standard deviation called the root mean squared, or RMS.\n",
    "\n",
    "<br>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1e636e3e",
   "metadata": {
    "tags": [
     "learner",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Example of Computing Mean and Variance</h3>\n",
    "\n",
    "Let's estimate the mean and variance of a flat (i.e., uniform) distribution by using a sample of data points. \n",
    "\n",
    "The formulas given above apply to finding exact statistical properties of a known distribution. However, they can also be applied to finding these properties for a data sample. Suppose we have ${N_\\mathrm{samples}}$ data points and we pick out one arbitrary data point. All the data points have an equal probability of being selected, so there is a probability $p(x)=\\frac{1}{N_\\mathrm{samples}}$ that we pick a specific one.\n",
    "\n",
    "Note that this probability is independent of what distribution the points were sampled from. We can use this PDF with the formulas above to calculate the mean and variance. Of course, for a set of discrete values of $x$, we need to do a sum, not an integral.\n",
    "\n",
    "You can see that using this $p(x)$ and a sum for the expectation value, you are just doing the usual procedure for calculating an average of any set of numbers."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dcf3796b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.4-runcell01\n",
    "\n",
    "#Again another exercise\n",
    "\n",
    "#sample\n",
    "nsamples=100000\n",
    "bkg1 = np.random.uniform(0,10, nsamples)\n",
    "\n",
    "#mean\n",
    "mean=0\n",
    "prob=1./nsamples\n",
    "for x in bkg1:\n",
    "    mean+=x*prob\n",
    "print(\"Mean: \",mean)\n",
    "\n",
    "#Now we can do the variance\n",
    "var=0\n",
    "for x in bkg1:\n",
    "    var+=(x-mean)*(x-mean)*prob\n",
    "print(\"Var: \",var)\n",
    "\n",
    "#Now we can do it the fast way, using intrinsic numpy functions\n",
    "print(\"Mean:\",bkg1.mean(),\"Variance:\",bkg1.var())\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81adc199",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-1.4.1: Mean of Uniform Distribution</span>\n",
    "\n",
    "Compute the \"ideal\" value of the mean (i.e., the average) of a uniform distribution from 0 to 5. You may do this analytically or by augmenting the code above."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9db7345f",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "2.5\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The mean of a uniform distribution over the range $[a,b]$ is $(a+b)/2$.\n",
    "\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b225b639",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 1.4.1a (ungraded)\n",
    ">   \n",
    ">Compute the mean and variance of a uniform distribution from 0 to 5 by taking 1000 random samples. Use the starting code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "39ab3c8e",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>FOLLOW-UP\n",
    "# Use this cell for drafting your solution (if desired)\n",
    "\n",
    "def uniform_mean(start, end, num_points):\n",
    "    mean_val = 0 # your code here\n",
    "    return mean_val\n",
    "\n",
    "def uniform_variance(start, end, num_points):\n",
    "    var_val = 0 # your code here\n",
    "    return var_val"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4370f323",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "\n",
    "def uniform_mean(istart, iend, inum_points):\n",
    "    mean_val = np.random.uniform(istart, iend, inum_points).mean()\n",
    "    return mean_val\n",
    "\n",
    "def uniform_var(istart, iend, inum_points):\n",
    "    var_val = np.random.uniform(istart, iend, inum_points).var()\n",
    "    return var_val\n",
    "\n",
    "\n",
    "start = 0\n",
    "end = 5\n",
    "num_points = 1000\n",
    "\n",
    "print(uniform_mean(start, end, num_points))\n",
    "print(uniform_var(start, end, num_points))<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Create a uniform, random distribution using np.random.uniform. The mean and variance are simply `array.mean()` and `array.var()`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9a8ab023",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Create a uniform, random distribution using np.random.uniform. The mean and variance are simply `array.mean()` and `array.var()`.\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce9bd843",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-1.4.2: Mean and Variance of a Uniform Distribution</span>\n",
    "\n",
    "Calculate analytically the mean and variance for a uniform distribution in the following ranges:\n",
    "\n",
    "(a) 0 to 2;\n",
    "(b) 0 to 5;\n",
    "(c) 0 to 10;\n",
    "(d) 0 to 20\n",
    "\n",
    "What is the trend in mean (expectation) and variance, from (a) to (d)? Choose from the options below:\n",
    "\n",
    "- mean decreasing, variance decreasing\n",
    "- mean decreasing, variance increasing\n",
    "- mean increasing, variance decreasing\n",
    "- mean increasing, variance increasing\n",
    "\n",
    "You may want to get a general formula. Alternatively, use large samples (>1000 events) from these distributions to computationally estimate the mean and variances (starting code below).\n",
    "    \n",
    "Whichever method you chose, you are encouraged to look at the solution after you submit your answer to make sure you understand how this works analytically. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bb912330",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample from uniform distributions in [0,2], [0,5], [0,10], [0,20]\n",
    "samples_list = [np.random.uniform(0, upper, 1000) for upper in [2, 5, 10, 20]] # Python list compr. syntax\n",
    "\n",
    "# Get a list of tuples of (mean, variance)\n",
    "means_vars = 0 #YOUR CODE HERE\n",
    "\n",
    "# Print results\n",
    "for mean, variance in means_vars:\n",
    "    print(f\"Mean: {mean}, Variance: {variance}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "63941caa",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "import numpy as np\n",
    "\n",
    "# Sample from uniform distributions in [0,2], [0,5], [0,10], [0,20]\n",
    "samples_list = [np.random.uniform(0, upper, 1000) for upper in [2, 5, 10, 20]] # Python list compr. syntax\n",
    "\n",
    "# Get a list of tuples of (mean, variance)\n",
    "means_vars = [(np.mean(samples), np.var(samples)) for samples in samples_list]\n",
    "\n",
    "# Print results\n",
    "for mean, variance in means_vars:\n",
    "    print(f\"Mean: {mean}, Variance: {variance}\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7f3431e1",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "Mean increasing, variance increasing.\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "\n",
    "*Analytical*\n",
    "\n",
    "Suppose we have a uniform distribution on the interval $[0, A]$. This distribution is symmetric. This allows us to intuit that the mean should be $E[x] = A/2$. Explicitly,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[x] = \\int_{-\\infty}^{\\infty} x p(x)\\, dx\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "where for our flat distribution\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x) = \\frac{1}{A}\\quad\\text{for}\\quad x\\in [0, A]\\quad\\text{and}\\quad 0\\quad\\text{elsewhere}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "Substituting in,\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[x] = \\int_0^A \\frac{x}{A}\\, dx = \\frac{1}{A}\\left.\\left(\\frac{x^2}{2}\\right)\\right|_0^A = \\frac{A}{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "For the variance, we could similarly calculate the integral given in the preceding section. However, here I'll introduce another equivalent formula that's easier since we already have $E[x]$. This can be derived from viewing our integral formula for $V[x]$ as $E[(x-E[x])^2]$. The result is\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V[x] = E[x^2] - (E[x])^2\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "So, we want to calculate $E[x^2]$.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[x^2] = \\int_{-\\infty}^{\\infty} x^2 p(x) = \\int_0^A\\frac{x^2}{A}\\,dx = \\frac{1}{A}\\left.\\left(\\frac{x^3}{3}\\right)\\right|_0^A = \\frac{A^2}{3}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "and thus\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V[x] = \\frac{A^2}{3} - \\frac{A^2}{4} = \\frac{A^2}{12}\n",
    "\\end{equation}\n",
    "$$\n",
    "    \n",
    "*Computational*\n",
    "\n",
    "Perhaps you can see that this approach is less tedious if you're comfortable programming and can accept inexact answers. It also can work in cases where the distribution is not easy to work with analytically.\n",
    "    \n",
    "<pre>\n",
    "# Sample from uniform distributions in [0,2], [0,5], [0,10], [0,20]\n",
    "samples_list = [np.random.uniform(0, upper, 1000) for upper in [2, 5, 10, 20]] # Python list compr. syntax\n",
    "\n",
    "# Get a list of tuples of (mean, variance)\n",
    "means_vars = [(np.mean(samples), np.var(samples)) for samples in samples_list]\n",
    "\n",
    "# Print results\n",
    "for mean, variance in means_vars:\n",
    "    print(f\"Mean: {mean}, Variance: {variance}\")\n",
    "</pre>\n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "14f80235",
   "metadata": {
    "tags": [
     "learner"
    ]
   },
   "source": [
    "<a name='exercises_1_4'></a>\n",
    "\n",
    "| [Top](#section_1_0) | [Restart Section](#section_1_4) | [Next Section](#section_1_5) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a10f518c",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "<a name='section_1_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L1.5 Sum of Two Distributions Continued</h2>\n",
    "\n",
    "| [Top](#section_1_0) | [Previous Section](#section_1_4) | [Exercises](#exercises_1_5) | [Next Section](#section_1_6) |"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6e4b4def",
   "metadata": {
    "tags": [
     "learner",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Back to the sum distribution!</h3>\n",
    "\n",
    "Finally, we're ready to see what distribution results from summing realizations of two uniform distributions. Let's first turn a uniform distribution from 0 to 10 into a PDF. For this we know that the function is flat between 0 and 10, and zero otherwise. Consequently $f(x)=a$ when $x\\in[0,10]$, and \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "1=\\int_{0}^{10}adx=ax|_{0}^{10}=10a\\\\\n",
    "a=\\frac{1}{10}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "As a quick check, we find the expectation of this distribution is \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[x]=\\int_{0}^{10}axdx=\\frac{1}{2}ax^2|_{0}^{10}=50a\\\\\n",
    "E[x]=5\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, let's consider sampling this distribution twice. If we get a result $x^\\prime=x_{1}+x_{2}$, there are a broad range of possible values for $x_{1}$ and $x_{2}$ for a fixed $x^{\\prime}$. Let's say $x^{\\prime}=10$ then it could be that one sample $x_{1}=5$ and the other sample $x_{2}=5$ or it could be that $x_{1}=10$ and $x_{2}=0$. To get all the possible values for $x^{\\prime}=10$ we need to compute the expectation over all possibilities. This is equivalent imposing a constraint that $x^{\\prime}=x_{1}+x_{2}\\rightarrow x_{2}=x^{\\prime}-x_{1}$. \n",
    "\n",
    "The best way to think about this is as a 2D distribution. Let's make a plot of this. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8ac1d467",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.5-runcell01\n",
    "\n",
    "bkg1 = np.random.uniform(0,10, 100)\n",
    "bkg2 = np.random.uniform(0,10, 100)\n",
    "\n",
    "#now let's fix x' to be 10, this means only bkg1 is an independent variable\n",
    "val=10-bkg1\n",
    "\n",
    "#now let's plot them\n",
    "#plotting-------------------\n",
    "#plot data\n",
    "plt.scatter(bkg1,bkg2, label=\"x1, x2 independently sampled\")\n",
    "plt.scatter(bkg1,val, label=\"x' = x1 + x2 = 10\")\n",
    "\n",
    "#plot labels and style\n",
    "plt.legend(fontsize=15)\n",
    "plt.xlabel('x1', fontsize=15) #Label x\n",
    "plt.ylabel('x2', fontsize=15) #Label y\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "23718011",
   "metadata": {
    "tags": [
     "learner",
     "lect_05"
    ]
   },
   "source": [
    "So when we fix $x^{\\prime}$ and sample events, that is equivalent to just drawing a line on the 2D plot. Now, if we think of this distribution as a 2D probability distribution function, we can write. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "P(x_{1},x_{2})&=&\\int_{x_1^\\mathrm{min}}^{x_1^\\mathrm{max}} \\int_{x_2^\\mathrm{min}}^{x_2^\\mathrm{max}}p(x_{1})p(x_{2})dx_{1}dx_{2}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "with the probability $P$ now defined as a 2D integral. In this case, we treated it as 2 independent probability distributions $p(x_{1})$ and $p(x_{2})$. In reality this function can be a function of both variables $p(x_{1},x_{2})$. We can now simplify this distribution into a 1D distribution by integrating over the line where $x^{\\prime}=x_1+x_2$\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(x^\\prime=x_{1}+x_{2})&=&\\int_{-\\infty}^{\\infty}p(x_{1})p(x^\\prime-x_{1})dx_{1}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "For this, we have then \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(x^\\prime=x_{1}+x_{2})&=& \\int_{0}^{10}\\frac{1}{a}p(x^\\prime-x_{1})dx_{1}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "We have to deal with two cases separately. One is where $x^{\\prime} \\geq 10$, and so the smaller number cannot go down to zero, but only to a minimum of $x^{\\prime}-10$ (the max number would be 10 in this case). The other case is where $x^{\\prime} \\lt 10$ and so the larger number cannot exceed $x^{\\prime}$ (the min number would be 0 in this case). Expanding this out gives us:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(x^\\prime=x_{1}+x_{2}) &=&\\int_{x^\\prime-10}^{10}\\frac{1}{a^2}dx_{1}~\\forall x^\\prime \\geq 10\\\\\n",
    "&=&\\frac{20-x^\\prime}{a^2}~\\forall x^\\prime \\geq 10\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p(x^\\prime=x_{1}+x_{2}) &=&\\int_0^{x^{\\prime}}\\frac{1}{a^2}dx_{1}~\\forall x^\\prime < 10\\\\\n",
    "&=&\\frac{x^\\prime}{a^2}~\\forall x^\\prime < 10\\\\\n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ae4e0509",
   "metadata": {
    "tags": [
     "learner",
     "lect_05"
    ]
   },
   "source": [
    "Or, in other words, the probability of finding a particular value of $x^\\prime$ is a line sloping up from 0 when we are below 10, and then sloping back down to 0 when we are above 10. To verify that this is a full PDF we can check the normalization:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\int_{-\\infty}^{\\infty}p(x^\\prime)dx^{\\prime}&=&\\frac{x^2}{2a^2}|^{10}_{0}+\\frac{20x-x^2/2}{a^2}|^{20}_{10}\\\\\n",
    "&=&\\frac{100}{200}+\\frac{400-200}{100}-\\frac{200-50}{100}\\\\\n",
    "&=&\\frac{1}{2}+\\frac{200-150}{100}\\\\\n",
    "&=&1\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "The nice thing about computers is we don't need to do all these integrals to get these lines. Finally, let's actually plot all of these on the same plot! We now have a histogram and a function. \n",
    "\n",
    "The one tricky component in the above formula is that we want to compare a distribution with a histogram, so we need to ensure that the integrals are the same over each region.  \n",
    "\n",
    "To make sure they are the same, lets pick a specific bin with minimum $x_{min}$ and maximum $x_{max}$.  Ensuring the integrals per bin are the same means that for each bin the values need to be the same or in other words\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "N^{\\rm bin}_\\mathrm{samples} = \\int_{x_\\mathrm{min}}^{x_\\mathrm{max}} C p(x) dx\\approx C p(x)\\left(x_\\mathrm{max}-x_\\mathrm{min}\\right) = C p(x)\\Delta x\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, for a distribution, we can write $\\Delta x=\\frac{x_\\mathrm{max}-x_\\mathrm{min}}{N_\\mathrm{bins}}$. Additionally, if we sum all bins, we have \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "N_{\\rm samples} & = & \\sum_{i=1}^{N} C p(x_{i}) \\Delta x \\\\\n",
    "N_{\\rm samples} & = & C   \\sum_{i=1}^{N}  p(x_{i}) \\Delta x \\\\ \n",
    "N_{\\rm samples} & = & C    \\\\ \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Then, we can write our normalization term per bin as $N_{samples}\\Delta x$. Thus, per bin we write our function as\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(x)= p(x) N_{\\rm samples} \\Delta x \n",
    "\\end{equation}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6805e6ef",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.5-runcell02\n",
    "\n",
    "#Let's first add our numbers\n",
    "nsamples=1000\n",
    "bkg1 = np.random.uniform(0,10, nsamples)\n",
    "bkg2 = np.random.uniform(0,10, nsamples)\n",
    "data = bkg1+bkg2\n",
    "nbins=100\n",
    "\n",
    "#now we make a histogram\n",
    "histy, bin_edges = np.histogram(data, bins=nbins)\n",
    "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "\n",
    "#Now let's define our function. \n",
    "def function(ix):#note the norm is for n bins over 0-20\n",
    "    if ix < 10:\n",
    "        return(ix/100)\n",
    "    else: \n",
    "        return (20-ix)/100\n",
    "\n",
    "def functionnp(ix):#note the norm is for n bins over 0-20\n",
    "    return np.where(ix < 10,(ix/100),(20-ix)/100 )\n",
    "    \n",
    "#We need to evaluate the function, so we do it like this\n",
    "x = np.linspace(start=0, stop=20, num=100)\n",
    "#this list(map) is just a trick to run this function on all elements in the array\n",
    "#y =  ntot*(20/inbins)*np.array(list(map(function, x)))\n",
    "y =  nsamples*(20/nbins)*functionnp(x) #this just uses numpy\n",
    "\n",
    "\n",
    "#plotting-------------------\n",
    "#plot size\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "#plot data\n",
    "plt.plot(x, y,label='analytic PDF')\n",
    "plt.plot(bin_centers,histy,drawstyle = 'steps-mid',label='points')\n",
    "plt.legend()\n",
    "#plot labels and style\n",
    "plt.xlabel('x1+x2', fontsize=15) #Label x\n",
    "plt.ylabel('$N_\\mathrm{samples}$/bin', fontsize=15) #Label y\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "192986f0",
   "metadata": {
    "tags": [
     "learner",
     "lect_05"
    ]
   },
   "source": [
    "Now, to understand the power of sampling, let's make a much more complicated function based on sampling, and let's see what this more complicated function looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "23851d52",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.5-runcell03\n",
    "\n",
    "#Sample something crazy\n",
    "bkg1 = np.random.uniform(0,10, 10000) # a random like before\n",
    "bkg2 = np.random.normal (5,2 , 10000) # a gaussian distribution centered about 2 with width 5\n",
    "data = bkg1+bkg2\n",
    "\n",
    "\n",
    "#plotting-------------------\n",
    "#plot size\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "\n",
    "#plot data and axes limits\n",
    "histy, bin_edges = np.histogram(data, bins=20)\n",
    "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "#ax.set_ylim([0,150])\n",
    "\n",
    "#plot labels and style\n",
    "plt.plot(bin_centers,histy,drawstyle = 'steps-mid')\n",
    "plt.xlabel('x', fontsize=15) #Label x\n",
    "plt.ylabel('f(x)', fontsize=15) #Label y\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e367c62a",
   "metadata": {
    "tags": [
     "learner",
     "lect_05"
    ]
   },
   "source": [
    "The point of this is that we can put sample events from any distribution we want, and make very complicated distributions that have all sorts of features. Instead of going through the analytical calculations to figure out what these distributions look like, we can simply sample!"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a2e53fc",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-1.5.1: Generalizing the Sum Distribution</span>\n",
    "\n",
    "Let's generalize the sum distribution. Compute the sum of 3 random variables that are distributed uniformly between 0 and 1 (draw 10000 samples, as was done abobe). Plot the histogram of this sum, as was done above. Try the sum of 4 random variables.\n",
    "\n",
    "How does the sum distribution behave as we scale to N uniform random variables? Choose from the options below:\n",
    "\n",
    "- it looks like a uniform distribtion\n",
    "- it looks like a sharper triangle shape\n",
    "- it looks like a normal distribution"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "074f021b",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "#(computational)\n",
    "# Get 4000 samples from each of N uniform random variables on [0,1]\n",
    "N = 50\n",
    "samples_list = [np.random.uniform(0,1,4000) for i in range(N)] # list of N numpy arrays\n",
    "\n",
    "# add the realizations\n",
    "sum_of_realizations = np.zeros(4000) # empty array to start\n",
    "for samples in samples_list:\n",
    "   sum_of_realizations += samples\n",
    "\n",
    "# histogram with 20 bins\n",
    "plt.hist(sum_of_realizations, bins=20)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "30e6c189",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "As `N` get larger, the sum distribution starts looking smoother and begins to resemble a normal distribution.\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "\n",
    "This is curious! Eventually we will bring up the Central Limit Theorem, which accounts for this phenomenon.\n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d62e9578",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "<!--start-block-->\n",
    "<a name='section_1_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L1.6 Generalizing to Many Measurements</h2>\n",
    "    \n",
    "| [Top](#section_1_0) | [Previous Section](#section_1_5) | [Exercises](#exercises_1_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "244630b6",
   "metadata": {
    "tags": [
     "learner",
     "lect_06"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Let's consider a set of measurements. Like we had with the two variables, if we take $N$ measurements, we can treat them as $N$ separate variables. Often the measurements can be sampled from the same distribution (like we had for the above case). \n",
    "\n",
    "Typically, we treat these measurements as independent variables. This means that the result of any one of the measurement doesn't affect the results of any of the other measurements. The probability distributions of these is similar to the case above where we had two measurements $x_{1}$ and $x_{2}$.  Like the 2D probability distribution we had before, we now have $N$ variables, yielding an $N$ dimensional distribution. Yes, this is complicated! But, it won't get too scary. \n",
    "\n",
    "To deal with this very high dimensional space, we can define the joint PDF distribution of these as the multiplication of the individual PDFs. For two independent measurements it's:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x_{1},x_{2})=p(x_{1})p(x_{2}) \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "For $N$ independent measurements it is, \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "p(x_{1},...,x_{N})=\\prod_{i=1}^{i=N}p(x_{i})\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Let's visualize some measurements sampled from the same distribution. We will take two measurements, $x_{1}$ and $x_{2}$, and, because we can, we will run this experiment 1000 times. To perform this experiment, we will use a normal distribution, which is defined as \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{N}(x; \\mu,\\sigma) = \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mu$ and $\\sigma$ are the mean and standard deviation, respectively, of the distribution and the fraction before the exponential is required for the integral of the distribution to equal 1.\n",
    "\n",
    "This is a distribution that we will use quite extensively in this course.     "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "82c79ab4",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.6-runcell01\n",
    "\n",
    "#Let's first add our numbers\n",
    "nsamples=1000\n",
    "mu=0\n",
    "sigma=1\n",
    "x1 = np.random.normal(mu,sigma, nsamples)\n",
    "x2 = np.random.normal(mu,sigma, nsamples)\n",
    "\n",
    "#plotting-------------------\n",
    "#plot size\n",
    "fig, ax = plt.subplots(figsize=(6,6)) #a square plot size is more useful for this data\n",
    "\n",
    "#plot data and axes limits\n",
    "plt.scatter(x1,x2,)\n",
    "plt.xlim(-4,4)\n",
    "plt.ylim(-4,4)\n",
    "\n",
    "#plot labels and style\n",
    "plt.title(\"Joint PDF of two normal distributions\", fontsize=15)\n",
    "plt.xlabel('x1', fontsize=15) #Label x\n",
    "plt.ylabel('x2', fontsize=15) #Label y\n",
    "plt.xticks(fontsize=12)\n",
    "plt.yticks(fontsize=12)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c1bdea72",
   "metadata": {
    "tags": [
     "learner",
     "lect_06"
    ]
   },
   "source": [
    "Now, from this setup, we can define a bunch of variables that help to understand the data. We call the variables that we define \"observables\" or data summaries. Let's list the definition of these variables, and then we will go ahead and see what we can do with them. First, we define the mean $\\bar{x}$ of our *sample* (not necessarily of the distribution). \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\bar{x}=\\frac{1}{N}\\sum_{i=1}^{N} x_{i}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "To be clear, this is an observable, (i.e., a calculation), that we perform on the data that we have at hand. \n",
    "\n",
    "We can then compute the expectation of $\\bar{x}$ for our sampled distribution. This expectation gives us: \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E[\\bar{x}]&=&\\int \\left(\\frac{1}{N}\\sum_{j=1}^{N} x_{j}\\right)\\prod_{i=0}^{i=N}p(x_{i}) dx_{i}\\\\\n",
    "          &=&\\frac{1}{N}\\sum_{j=1}^{N}\\left(\\int x_{j}\\prod_{i=0}^{i=N}p(x_{i}) dx_{i}\\right)\\\\\n",
    "          &=&\\frac{1}{N}\\times N \\int x p(x) dx\\\\\n",
    "E[\\bar{x}]&=&E[x]\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Or, on average, our sample mean $\\bar{x}$ will be the mean of the distribution. \n",
    "\n",
    "Now, we can define the variance of the data in a similar way\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "V(x)=\\frac{1}{N}\\sum_{i=1}^{N} (x_{i}-\\bar{x})^{2}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Note that the form above is very similar to the variance of a distribution, defined by\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E\\left[V(x)\\right]&=&\\frac{1}{N}\\int_{-\\infty}^{\\infty}\n",
    "\\left(\\sum_{j=1}^{N}\\left(x_{j}-\\bar{x}\\right)^{2}\\right)\\prod_{i=0}^{i=N}p(x_{i}) dx_{i}\\\\\n",
    "E\\left[V(x)\\right]&=&\\frac{1}{N}\\sum_{i=1}^{N}\\int_{-\\infty}^{\\infty}\\left(x_{i}-\\bar{x}\\right)^{2} p(x_{i}) dx_{i}\\\\\n",
    "E\\left[V(x)\\right]&=&\\frac{N}{N}E[(x-\\bar{x})^2]\\\\\n",
    "E\\left[V(x)\\right]&=&V(x)\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "And now we can consider the variance of these distributions. Let's do the variance of our defined $\\bar{x}$. First let's derive it for just one variable. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V\\left[\\bar{x}\\right]&=&\\int (x_i-\\bar{x})^2 p(x_i)dx_i\\\\\n",
    "                     &=&\\int (x^2_i-2x_{i}\\bar{x}+\\bar{x}^2) p(x_i)dx_i\\\\\n",
    "                     &=&\\int x^2_i p(x_i)dx_i - 2\\bar{x}^2+\\bar{x}^2\\\\\n",
    "                     &=&\\int x^2_i p(x_i)dx_i -  \\bar{x}^2\\\\\n",
    "                     &=&\\int (x^2_i-\\bar{x}^2) p(x_i)dx_i  \n",
    "\\end{eqnarray}\n",
    "$$"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "aede6e2a",
   "metadata": {
    "tags": [
     "learner",
     "lect_06"
    ]
   },
   "source": [
    "Now, let's generalize this whole thing to $N$ measurements. I would like to warn you that this is a complicated calculation; it's not the focus of this course, but it's here for completeness.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\sum_{i=1}^{N} x_{i}-\\bar{x}\\right)^{2} \\Pi_{i=0}^{i=N}p(x_{i}) dx_{i} \\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left( \\left(\\sum_{i=1}^{N} x_{i}\\right)^2-2N\\left(\\sum_{i=1}^{N} x_{i}\\right)\\bar{x}+ N^2\\bar{x}^2\\right)\\Pi_{i=0}^{i=N}p(x_{i}) dx_{i}\\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left( \\left(\\sum_{i=1}^{N} x_{i}\\right)^2\\right) \\Pi_{i=0}^{i=N}p(x_{i}) dx_{i}-\\frac{2}{N}\\bar{x}N\\bar{x}+\\bar{x}^2 \\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left( \\left(\\sum_{i=1}^{N} x_{i}\\right)\\left(\\sum_{i=1}^{N} x_{i}\\right) \\right) \\Pi_{i=0}^{i=N}p(x_{i}) dx_{i}-\\bar{x}^2\\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left(\\sum_{i} x_{i}^2 + 2\\sum_{i}\\sum_{j\\neq i}x_{i}x_{j} \\right) \\Pi_{i=0}^{i=N}p(x_{i}) c-\\bar{x}^2\\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left(\\sum_{i} x_{i}^2\\right)\\Pi_{i=0}^{i=N}p(x_{i})dx_{i} + \\frac{(N-1)}{N}\\bar{x}^2-\\bar{x}^2\\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\int \\left(\\frac{1}{N}\\right)^2 \\left(\\sum_{i} x_{i}^2 -N\\bar{x}^2\\right)\\Pi_{i=0}^{i=N}p(x_{i})dx_{i} + \\frac{1}{N}\\bar{x}^2 + \\frac{(N-1)}{N}\\bar{x}^2-\\bar{x}^2\\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\left(\\frac{1}{N}\\right) \\int \\left(\\frac{1}{N}\\right)  \\left(\\sum_{i} x_{i}^2 -N\\bar{x}^2\\right)\\Pi_{i=0}^{i=N}p(x_{i})dx_{i} \\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\left(\\frac{1}{N}\\right)  \\left(\\frac{1}{N}\\right) N  \\left(\\int x_{i}^2 p(x_{i}) dx_{i} -\\bar{x}^2\\right) \\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\left(\\frac{1}{N}\\right) \\left(\\frac{1}{N}\\right)N V(x) \\\\\n",
    "V\\left[\\bar{x}\\right]&=&\\left(\\frac{1}{N}\\right) V(x) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "where $V(x)$ is the variance of any distribution. This is very important. What this means is that if we sample many times a distribution with a variance $\\sigma^2$, we have that the variance of the average over this distribution scales as $\\frac{1}{N}$ times the variance of the sampled distribution. This means that if you are measuring the mean, the uncertainty on the mean scales as the $\\sqrt{V[\\bar{x}]}=\\sqrt{\\frac{1}{N}V[x]}$. \n",
    "\n",
    "We will state without proof that the variance scales in a similar way. \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V\\left[V(\\bar{x})\\right]&=&\\left(\\frac{1}{2N}\\right) V(x) \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To understand how this works, let's run some toy calculations. \n",
    "\n",
    "<br>\n",
    "<!--end-block-->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ead1547",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L1.6-runcell02\n",
    "\n",
    "import math\n",
    "\n",
    "#define a function that samples a normal distribution N times and then returns mean and root mean-square rms \n",
    "#(also known as standard deviation, the square root of variance)\n",
    "def sample(iN,imean,istdev):\n",
    "    sample = np.random.normal(imean,istdev,iN)\n",
    "    return sample.mean(),sample.std()\n",
    "\n",
    "#This function runs 100 tests where we sample N times, we call these toys\n",
    "def meansample(iN,imean,istdev):\n",
    "    ntoys=100\n",
    "    allmeans=np.array([])\n",
    "    allrmses=np.array([])\n",
    "    for i0 in range(ntoys):\n",
    "        pMean,pRMS=sample(iN,imean,istdev)\n",
    "        allmeans=np.append(allmeans,pMean)\n",
    "        allrmses=np.append(allrmses,pRMS)\n",
    "    return allmeans.std(),allrmses.std()\n",
    "\n",
    "\n",
    "def get_sim_mean_rms(isamples,imean,istdev):\n",
    "  nvar=np.array([])\n",
    "  mean=np.array([])\n",
    "  rms=np.array([])\n",
    "\n",
    "  distmean=np.array([])\n",
    "  distrms=np.array([])\n",
    "\n",
    "\n",
    "  #Now we iterate from 1 to 250 in sampling and compute mean and RMS\n",
    "  for i0 in range(isamples):\n",
    "      nvar = np.append(nvar,i0)\n",
    "\n",
    "      #Sample just once \n",
    "      pMean,pRMS=sample(i0,imean,istdev)\n",
    "      distmean = np.append(distmean,pMean)\n",
    "      distrms  = np.append(distrms,pRMS)\n",
    "\n",
    "      #sample many times\n",
    "      pMean,pRMS=meansample(i0,imean,istdev)\n",
    "      mean = np.append(mean,pMean)\n",
    "      rms  = np.append(rms,pRMS)\n",
    "    \n",
    "  return nvar, distmean, mean, distrms, rms\n",
    "\n",
    "\n",
    "isamples,imean,istdev = [250,0,1]\n",
    "nvar, distmean, mean, distrms, rms = get_sim_mean_rms(isamples,imean,istdev)\n",
    "\n",
    "\n",
    "# plt.plot(nvar,funcmean,'--',label='func rms of mean')\n",
    "# plt.plot(nvar,funcrms,'--',label='func rms of rms')\n",
    "plt.plot(nvar,distmean,label='mean')\n",
    "plt.plot(nvar,distrms,label='rms')\n",
    "plt.plot(nvar,mean,label='rms of mean')\n",
    "plt.plot(nvar,rms + np.ones(len(rms)),label='rms of rms') # Add 1 to the rms to separate these two quantities in the plot\n",
    "plt.legend(loc='lower right')\n",
    "plt.xlabel(\"Number of samples\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c47582c6",
   "metadata": {
    "tags": [
     "learner",
     "lect_06"
    ]
   },
   "source": [
    "As you can see from the plot, the mean and rms of a single sample fluctuate a lot for small sample sizes, a fact that is quantified by the relatively large standard deviations (rms) of those two observables. As we increase the size of the samples, the rms of the mean and the rms of the rms decrease asymptotically  (don't forget that the rms of the rms is increased by 1 on the plot for clarity). This same effect is seen in the reduced fluctuations in the values of the mean and rms for single samples.\n",
    "\n",
    "This means that with larger samples, we can be more confident in our measurement of the distribution's mean and rms. Note that the shape of the decrease of the two variances matches quite well the prediction of $1/\\sqrt{N}$ and $1/ \\sqrt{2N}$ scaling for the rms of the mean and rms, respectively (recall that the rms or standard deviation is the square root of the variance)."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e7ab7a5f",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-1.6.1: Variance of the Mean and Variance of the Variance</span>\n",
    "\n",
    "We know that the variance and mean for a normal distribution are \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E(\\mathcal N(x; \\mu,\\sigma)) & = & \\mu \\\\\n",
    "V(\\mathcal N(x; \\mu,\\sigma)) & = & \\sigma^2 \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "Now we wish to derive a functional form for $\\sqrt{V[\\bar{x}]}$ (the rms of the mean) and $\\sqrt{V\\left[(x-\\bar{x})^2\\right]}$ (the rms of the rms), and compare with simulated data.\n",
    "\n",
    "Write a function that accepts an array and returns the rms of the mean and the rms of the rms."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b0d3505a",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>EXERCISE\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "\n",
    "def func_rms_mean(isample_array,imean,istdev):\n",
    "    rms_mean = 0. #complete the function\n",
    "    return rms_mean\n",
    "    \n",
    "\n",
    "def func_rms_rms(isample_array,imean,istdev):\n",
    "    #this is a piecewise function with rms=0 for isample[i]=1\n",
    "    rms_rms = 0. #complete the function\n",
    "    return np.where(isample_array <=1, 0, rms_rms)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "de2e44b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>SOLUTION\n",
    "\n",
    "def func_rms_mean(isample_array,imean,istdev):\n",
    "    rms_mean = istdev/np.sqrt(isample_array)\n",
    "    return rms_mean\n",
    "    \n",
    "\n",
    "def func_rms_rms(isample_array,imean,istdev):\n",
    "    #this is a piecewise function with rms=0 for isample[i]=1\n",
    "    rms_rms = istdev/np.sqrt(2*(isample_array))\n",
    "    return np.where(isample_array <=1, 0, rms_rms)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0e1ea867",
   "metadata": {
    "tags": [
     "solution"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "For this, we just need to know that \n",
    "\n",
    "$$ V[\\bar{x}] = \\frac{1}{N} V[x] = \\frac{\\sigma^2}{N} $$ \n",
    "\n",
    "and\n",
    "\n",
    "$$ V[(x-\\bar{x})^2] = \\frac{1}{2N} V[x] = \\frac{\\sigma^2}{2N}  $$ \n",
    "    \n",
    "</div>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "611b8b4e",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 1.6.1a (ungraded)\n",
    ">\n",
    ">Plot the functions that correspond to the rms of the mean and the rms of the rms, and compare these predictions with the results from simulated data."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f2982d89",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>FOLLOW-UP\n",
    "# Run this cell as follow-up to the previous exercise.\n",
    "\n",
    "#make plots\n",
    "#####################\n",
    "def get_new_func_mean_rms(isamples,imean,istdev):\n",
    "    sample_array = np.arange(1,isamples+1,1)\n",
    "    \n",
    "    funcrmsmean = func_rms_mean(sample_array,imean,istdev)\n",
    "    funcrmsrms = func_rms_rms(sample_array,imean,istdev)\n",
    "    return funcrmsmean, funcrmsrms\n",
    "\n",
    "\n",
    "isamples,imean,istdev = [250,0,1]\n",
    "nvar, mean_mean, rms_mean, mean_rms, rms_rms = get_sim_mean_rms(isamples,imean,istdev)\n",
    "funcrmsmean, funcrmsrms = get_new_func_mean_rms(isamples,imean,istdev)\n",
    "\n",
    "\n",
    "\n",
    "def plot_mean_and_rms_mean_vals():\n",
    "    plt.errorbar(nvar,mean_mean,yerr=rms_mean, ecolor='red', label='mean and rms of mean')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "def plot_rms_mean_vals():\n",
    "    plt.plot(nvar,rms_mean,label='rms of mean')\n",
    "    plt.plot(nvar,funcrmsmean,'--',label='func rms of mean')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_rms_rms_vals():\n",
    "    plt.plot(nvar,rms_rms,label='rms of rms')\n",
    "    plt.plot(nvar,funcrmsrms,'--',label='func rms of rms')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "def plot_residuals():\n",
    "    plt.plot(nvar,funcrmsmean - rms_mean,'--',label='func rms of mean residuals')\n",
    "    plt.plot(nvar,funcrmsrms - rms_rms,'--',label='func rms of rms residuals')\n",
    "    plt.legend(loc=1)\n",
    "    plt.xlabel(\"Number of samples\")\n",
    "    plt.show()\n",
    "    return\n",
    "\n",
    "\n",
    "plot_mean_and_rms_mean_vals()\n",
    "plot_rms_mean_vals()\n",
    "plot_rms_rms_vals()\n",
    "plot_residuals()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "713285fa",
   "metadata": {
    "tags": [
     "learner",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 1.6.1b (ungraded)\n",
    ">   \n",
    ">Sample a normal distribution of mean 1 and variance $\\sigma$ with an increasing number of events, N. What do you observe as the trend of this distribution as a function of N?\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbea03c9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_1'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.1 Introduction to Binomial Distribution</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_0) | [Exercises](#exercises_2_1) | [Next Section](#section_2_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "549f4d68",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this lesson, which are discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides1.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b9f82529",
   "metadata": {
    "tags": [
     "learner",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.0-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L02/slides1.html', width=975, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a874d14e",
   "metadata": {
    "tags": [
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are also discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides1.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L02/slides1.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3965767e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "<!--<img src=\"https://external-preview.redd.it/Kt_QUdsFmAI4v9hPw-JbYA2wC9blaF8iIvnVdla2aaE.jpg?auto=webp&s=da7915bb8a27e47d88adaf6f58bab193b7d1f35f\" width=\"500\"/>-->\n",
    "\n",
    "Often we perform measurements having some probability. Let's say we perform many equal-probability measurements. What will be our distribution? Since this is not a math class, we will not go into the depth of the math behind this, but let's at least walk through a basic derivation.\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "77b3064b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "Let's say you flip a coin 10 times, and the probability of heads is $p$. Let's say you get heads 3 times and tails 7 times.\n",
    "\n",
    "* What are the number of different cases where there are 3 heads?\n",
    "\n",
    "In this case, all we care about is the number of heads out of the total number of flips, so we can use the formula for a *combination*: $_{10}C_{3}=\\frac{10!}{3!(10-3)!}=120$ (<a href=\"https://en.wikipedia.org/wiki/Combination\" target=\"_blank\">details</a>). As a brief reminder of how this works, there is a total of $10!$ different ordered combinations of our distinct flips 1 through 10. Let's say we identify 3 of those 10 flips as special for whatever reason (e.g. let's take the first 3 flips). Then there are $3!$ ways to order those flips (e.g. (1,2,3),(1,3,2),....) and there are $7!$ ways to order the remaining 7 flips. To find the *distinct* number of ways a group of 3 and a group of 7 can happen, we divide the total number of different ordered combinations $10!$ by these two groups. Thus, for all 10 flips, there are $_{10}C_{3}$ different ways to order a group of 3 flips and a group of 7 flips, where the group of 3 and the group of 7 are *distinct* from one another (one example being 1,2,3 is heads and 4...10 is tails). More generally, for $n$ total flips and $m$ heads/tails, the total number of distinct combinations is written as $\\frac{n!}{m!(n-m)!}$. \n",
    "\n",
    "\n",
    "* What is the probability of the scenario where we get 3 heads out of 10 total flips?\n",
    "\n",
    " * Each flip of the coin has equal probability of landing on heads. Let's say that this probability is $p$. With one heads and one tails in two flips, the probability would be the probability to get heads ($p$) multiplied by the probability to get tails ($1-p$), multiplied by the number of distinct combinations that would give you one head and one tail. In this case, the number of combinations is $2$ (heads first and tails second, *or* tails first and heads second). This yields a total probability of $p(1-p)\\times N_\\mathrm{combo}=2p(1-p)$.\n",
    "\n",
    " * In the case of 10 flips where there are 3 heads, the probability becomes the probability of 3 heads $p^{3}$ multiplied by the probability of 7 tails $(1-p)^{7}$ multiplied by $N_\\mathrm{combo}$. In general, for $n$ flips and $m$ heads, we have $p^{m}(1-p)^{n}\\times N_\\mathrm{combo}$. \n",
    "\n",
    "\n",
    "\n",
    "* What is the distribution?\n",
    " * If we combine everything for our specific case, we have $_{10}C_{3}\\times p^3(1-p)^7$ \n",
    " * The general formula is actually called the binomial distribution! It is given by $f(m)=p^{m}(1-p)^{n}\\frac{n!}{m!(n-m)!}$\n",
    "\n",
    "Let's actually compute this for a few cases!\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "149413af",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.1-runcell01\n",
    "\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "print(\"Test comb:\",comb(2,1),\"True: 2\",comb(3,2),\"True: 3\",comb(10,3),\"True: 120\")\n",
    "\n",
    "#for p=0.5, what is the probability of 3 heads out of 10 draws?\n",
    "def prob(p=0.5,nheads=3,ntotal=10):\n",
    "    pheads=np.power(p,nheads)\n",
    "    ptails=np.power(1-p,ntotal-nheads)\n",
    "    combos=comb(ntotal,nheads)\n",
    "    return combos*ptails*pheads\n",
    "\n",
    "print(\"Probability of 3 heads in 10 draws is:\",prob(nheads=3,ntotal=10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d71379ab",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "Out of all of this math, we have derived the binomial distribution. This is the first empirical distribution we will need for this Lesson. In fact, all of the other distributions we will study are built upon the binomial distribution. Let's compute the expectation and variance of this distribution. First, we can define the distribution:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "f(p,n,m) = \\frac{n!}{m!(n-m)!}p^{m}(1-p)^{n-m}\\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, let's compute the expectation over $m$ for $p$ and $n$ fixed, defined as $E[m;p,n]$ the semicolon denotes fixed.\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[m;p,n]=\\sum_{m=0}^{m=n} p^{m}(1-p)^{n-m}\\frac{n!}{m!(n-m)!} \\mathrm{heads}(m) dm \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "where $\\mathrm{heads}(m)$ is a function that we define as the expected value given a heads or tails observation. In this case, we will define this function as $1$ for heads and $0$ for tails. This is a complicated form, but in the case of just getting one head out of one flip, we have: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[m;p,n=1]= p\\times1+(1-p)\\times0\\\\\n",
    "E[m;p,n=1]= p\n",
    "\\end{equation} \n",
    "$$\n",
    "\n",
    "Now let's introduce a new function that is defined as the sum of $n$ individual experiments, we can define the function $f(x)=\\sum_{i} \\rm{heads}(x_{i})$. The expectation for this is\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E[f(x)]&=&\\sum_{0}^{n} p\\times 1+(1-p)\\times 0 \\\\\n",
    "E[f(x)]&=&np\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "That means that the average value over $n$ tries $\\bar{x}=f(x)/n$, or rather simply the value $p$. \n",
    "\n",
    "In a similar way, we can define the variance as: \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V[f(x)]&=&\\sum_{i=0}^{n} (x-\\mu)^2 \\\\\n",
    "V[f(x)]&=&\\sum_{i=0}^{n} p\\times(1-\\mu)^2 + (1-p) \\times (0-\\mu)^2 \\\\\n",
    "V[f(x)]&=&\\sum_{i=0}^{n} p \\times (1-p)^2 + (1-p) \\times (0-p)^2 \\\\\n",
    "V[f(x)]&=&\\sum_{i=0}^{n} (1-p)\\times (p^2 +p(1-p))  \\\\\n",
    "V[f(x)]&=&\\sum_{i=0}^{n} (1-p)\\times p   \\\\\n",
    "V[f(x)]&=&np(1-p)\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "or in other words, if we consider performing $n$ independent measurements, the variance over this distribution becomes $V[f(x)/n]=V[f(x)]/n=p(1-p)$."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dde2d970",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_01"
    ]
   },
   "source": [
    "<h3>A Computational Example</h3>\n",
    "\n",
    "It's all fun to do math, but the point of this class is to do it with computers, so let's do the same derivations numerically. Note that we plot these as discrete lines rather than continuous points, since the binomial distribution is discrete and only defined for discrete numbers (integers in this case). \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9fbc58b5",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_01",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.1-runcell02\n",
    "\n",
    "#We are going to use scipy stats package\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=30\n",
    "p=0.25\n",
    "\n",
    "#let's get the integral of this guy\n",
    "def ExpectationAndVar(n,p): \n",
    "    norm=0\n",
    "    exp=0\n",
    "    var=0\n",
    "    for i0 in range(n):\n",
    "        norm+=stats.binom.pmf(i0,n,p)\n",
    "        exp+=i0*stats.binom.pmf(i0,n,p)\n",
    "    exp=exp/norm\n",
    "    \n",
    "    for i0 in range(n):\n",
    "        pVal=stats.binom.pmf(i0,n,p)\n",
    "        var+=(i0-exp)**2*pVal\n",
    "    \n",
    "    #Print it out\n",
    "    print(\"norm:\",norm,\"expectation:\",exp,\"Var:\",var/norm)\n",
    "\n",
    "    #Now let's check with the expectation\n",
    "    print(\"norm: 1.000000, expectation:\",n*p,\"Var:\",n*p*(1-p))\n",
    "\n",
    "ExpectationAndVar(n,p)\n",
    "\n",
    "#Scipy has a binomial, but since this is a discrete distribution we use pmf (probability mass function) rather than pdf\n",
    "k=np.arange(0,n)\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "\n",
    "plt.plot(k,binomial,'o')\n",
    "plt.vlines(k,0, binomial)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.xlabel(\"Number of successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "267e36b8",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_1'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_1) | [Next Section](#section_2_2) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f2695caa",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.1.1: Probability as a Function of Coin Fairness</span>\n",
    "\n",
    "In order to answer this question, plot the probability of flipping a coin 10 times and observing 3 heads, for varying \"coin fairness.\" In other words, as a function of probability $p$ (i.e., the probability $p$ of getting a heads is varying).\n",
    "\n",
    "\n",
    "<u>Hint:</u> Use the previously defined function `prob(p=0.5,nheads=3,ntotal=10)` and vary `p`, or use the built-in function `stats.binom.pmf(k,n,p)`, defined <a href=\"https://docs.scipy.org/doc/scipy/reference/generated/scipy.stats.binom.html\" target=\"_blank\">here</a>.\n",
    "\n",
    "\n",
    "As the probability $p$ varies from 0 to 1, which of the following correctly describes the behavior? The probability of observing 3 head out of 10 flips...\n",
    "\n",
    "- keeps increasing to a value of 1 when p=1\n",
    "- keeps decreasing to a value of 0 when p=1\n",
    "- increases from 0 at p=0 to a maximum value, then decreases to 0 at p=1\n",
    "- maintains a constant value for all p"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8decfd1b",
   "metadata": {
    "tags": [
     "py",
     "draft"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.1.1\n",
    "# Use this cell for drafting your solution (if desired)\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import comb\n",
    "\n",
    "#for p=0.5, what is the probability of 3 heads out of 10 draws?\n",
    "def prob(p=0.5,nheads=3,ntotal=10):\n",
    "    #YOUR CODE HERE\n",
    "    return ###\n",
    "\n",
    "\n",
    "def plot_prob(x):\n",
    "    #plotting-------------------\n",
    "    #plot data\n",
    "    ydata = prob(p=x,nheads=3,ntotal=10)\n",
    "    plt.plot(x, ydata, 'o')\n",
    "    plt.vlines(x,0, ydata)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    #plot labels and style\n",
    "    plt.title('Probability of 3 Heads in 10 Draws for Varying Coin Fairness', fontsize=15)\n",
    "    #plt.legend(loc='lower right', fontsize = 12)\n",
    "    plt.xlabel('Coin Fairness (Probability of Landing Heads)', fontsize=15) #Label x\n",
    "\n",
    "    # changing the fontsize of ticks\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # a grid\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "prob_vals = np.linspace(0,1,11)\n",
    "plot_prob(prob_vals)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "521cd001",
   "metadata": {
    "hideCode": true,
    "tags": [
     "py",
     "solution"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.1.1\n",
    "\n",
    "import numpy as np\n",
    "import matplotlib.pyplot as plt\n",
    "from scipy.special import comb\n",
    "\n",
    "#for p=0.5, what is the probability of 3 heads out of 10 draws?\n",
    "def prob(p=0.5,nheads=3,ntotal=10):\n",
    "    pheads=np.power(p,nheads)\n",
    "    ptails=np.power(1-p,ntotal-nheads)\n",
    "    combos=comb(ntotal,nheads)\n",
    "    return combos*ptails*pheads\n",
    "\n",
    "\n",
    "def plot_prob(x):\n",
    "    #plotting-------------------\n",
    "    #plot data\n",
    "    ydata = prob(p=x,nheads=3,ntotal=10)\n",
    "    plt.plot(x, ydata, 'o')\n",
    "    plt.vlines(x,0, ydata)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    #plot labels and style\n",
    "    plt.title('Probability of 3 Heads in 10 Draws for Varying Coin Fairness', fontsize=15)\n",
    "    #plt.legend(loc='lower right', fontsize = 12)\n",
    "    plt.xlabel('Coin Fairness (Probability of Landing Heads)', fontsize=15) #Label x\n",
    "\n",
    "    # changing the fontsize of ticks\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # a grid\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "prob_vals = np.linspace(0,1,11)\n",
    "plot_prob(prob_vals)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "821058c5",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "        \n",
    "**SOLUTION:**\n",
    "    \n",
    "increases from from 0 at p=0 to a maximum value, then decreases to 0 at p=1\n",
    "    \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Run code to observe this effect:\n",
    "    \n",
    "<pre>\n",
    "import numpy as np\n",
    "from scipy.special import comb\n",
    "\n",
    "#for p=0.5, what is the probabiity of 3 heads out of 10 draws?\n",
    "def prob(p=0.5,nheads=3,ntotal=10):\n",
    "    pheads=np.power(p,nheads)\n",
    "    ptails=np.power(1-p,ntotal-nheads)\n",
    "    combos=comb(ntotal,nheads)\n",
    "    return combos*ptails*pheads\n",
    "\n",
    "\n",
    "def plot_prob(x):\n",
    "    #plotting-------------------\n",
    "    #plot data\n",
    "    ydata = prob(p=x,nheads=3,ntotal=10)\n",
    "    plt.plot(x, ydata, 'o')\n",
    "    plt.vlines(x,0, ydata)\n",
    "    plt.ylim(bottom=0)\n",
    "\n",
    "    #plot labels and style\n",
    "    plt.title('Probability of 3 Heads in 10 Draws for Varying Coin Fairness', fontsize=15)\n",
    "    #plt.legend(loc='lower right', fontsize = 12)\n",
    "    plt.xlabel('Coin Fairness (Probability of Landing Heads)', fontsize=15) #Label x\n",
    "\n",
    "    # changing the fontsize of ticks\n",
    "    plt.xticks(fontsize=12)\n",
    "    plt.yticks(fontsize=12)\n",
    "\n",
    "    # a grid\n",
    "    plt.grid()\n",
    "    plt.show()\n",
    "    \n",
    "    \n",
    "prob_vals = np.linspace(0,1,11)\n",
    "plot_prob(prob_vals)\n",
    "</pre>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "188d58b6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.1.2: Rolling a Die</span>\n",
    "\n",
    "Now, instead of flipping a coin, consider rolling a die 10 times. If the die lands on 6, we consider the trial a success. If the die lands on anything other than 6, we consider the trial a failure. Using the formulae for expectation and variance that we previously defined, calculate the expectation and variance of the binomial distribution related to these criteria.\n",
    "\n",
    "Enter your answer as a list of numbers with precision 1e-2: `[expectation, variance]`"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "078189d5",
   "metadata": {
    "hideCode": false,
    "hideOutput": true,
    "hidePrompt": false,
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "        \n",
    "**SOLUTION:**\n",
    "    \n",
    "<pre>\n",
    "[1.667,1.389]\n",
    "</pre>\n",
    "\n",
    "**EXPLANATION:**\n",
    "    \n",
    "The probability of a trial being a success is p=1/6, and we consider n=10 trials. The norm is 1, the expectation is `n*p=1.67`, and the variance is `n*p*(1-p)=1.39`.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d5f9a1e7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 2.1.2a (ungraded)\n",
    ">    \n",
    ">Plot the binomial distribution of these trials, and calculate the norm (which should be 1), expectation, and variance. Use the starting code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6a003d42",
   "metadata": {
    "tags": [
     "draft",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>FOLLOW-UP: L2.1.2a\n",
    "# Use this cell for drafting your solution (if desired)\n",
    "\n",
    "#Follow example given in L2.1\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=10\n",
    "p=1/6\n",
    "\n",
    "#use pmf rather than pdf\n",
    "k=np.arange(0,n)\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "\n",
    "\n",
    "def get_binom_integral(n,p):\n",
    "    #get the integral\n",
    "    ##########\n",
    "    #YOUR CODE HERE\n",
    "    ##########\n",
    "    return norm, exp, pVal, var\n",
    "\n",
    "norm, exp, pVal, var = get_binom_integral(n,p)\n",
    "\n",
    "#print\n",
    "print(\"norm:\",norm,\"expectation:\",exp/norm,\", Var:\",var/norm)\n",
    "\n",
    "#check\n",
    "print(\"norm: 1.000000, expectation:\",n*p,\", Var:\",n*p*(1-p))\n",
    "\n",
    "    \n",
    "plt.plot(k,binomial,'o')\n",
    "plt.vlines(k,0, binomial)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.xlabel(\"Number of successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "953901ff",
   "metadata": {
    "hideCode": true,
    "hideOutput": true,
    "hidePrompt": false,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.1.2a\n",
    "\n",
    "\n",
    "#Follow example given in L2.1\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "n=10\n",
    "p=1/6\n",
    "\n",
    "#use pmf rather than pdf\n",
    "k=np.arange(0,n)\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "\n",
    "\n",
    "def get_binom_integral(n,p):\n",
    "    #get the integral\n",
    "    norm=0\n",
    "    exp=0\n",
    "    var=0\n",
    "\n",
    "    for i0 in range(n):\n",
    "        norm+=stats.binom.pmf(i0,n,p)\n",
    "        exp+=i0*stats.binom.pmf(i0,n,p)\n",
    "\n",
    "    for i0 in range(n):\n",
    "        pVal=stats.binom.pmf(i0,n,p)\n",
    "        var+=(i0-exp/norm)*(i0-exp/norm)*pVal\n",
    "        \n",
    "    return norm, exp, pVal, var\n",
    "\n",
    "norm, exp, pVal, var = get_binom_integral(n,p)\n",
    "\n",
    "\n",
    "#print\n",
    "print(\"norm:\",norm,\"expectation:\",exp/norm,\", Var:\",var/norm)\n",
    "\n",
    "#check\n",
    "print(\"norm: 1.000000, expectation:\",n*p,\", Var:\",n*p*(1-p))\n",
    "\n",
    "    \n",
    "plt.plot(k,binomial,'o')\n",
    "plt.vlines(k,0, binomial)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.xlabel(\"Number of successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3ab245b0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_2'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.2 Applications Using the Binomial Distribution</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_1) | [Exercises](#exercises_2_2) | [Next Section](#section_2_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a5784ae9",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, let's do some problems that are more difficult than flipping a coin. Also, let's think about this in a real life setting. Let's say that you are observing <a href=\"https://en.wikipedia.org/wiki/Fast_radio_burst\" target=\"_blank\">fast radio bursts</a> and, based on the Wikipedia page <a href=\"https://en.wikipedia.org/wiki/List_of_fast_radio_bursts\" target=\"_blank\">here</a>, you observe about 19 fast radio bursts per year. What is the probability that you observe 2 fast radio bursts (FRB) within a day of each other? \n",
    "\n",
    "The trick to this problem is think of each day as flipping a coin, where the probability of heads is instead the probability of finding a FRB. We can caculate the average probability by noting that over a period 365 days (i.e. 365 experiments), we see 19 FRBs, or in other words:\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "E[f(x;n=365)]&=&np \\\\\n",
    "             &=&19 \\\\\n",
    "             &=&365\\times p \\\\\n",
    "            p&=&\\frac{19}{365}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "\n",
    "Thus, the probability of $2$ in a row turns out to be 0.3% (see below). Moreover, the probability of 2 observations in 7 days is (see below) 4.3%.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "99c8c63b",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.2-runcell01\n",
    "\n",
    "def prob(ndays=2,nobs=2,p=19/(365)):\n",
    "    return stats.binom.pmf(nobs,ndays,p)\n",
    "\n",
    "print(\"2 observations in 2 days:\",prob(2))\n",
    "print(\"2 observations in 7 days:\",prob(7))\n",
    "print(\"19 observations in 365 days:\",prob(365,19))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a9f60a5a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "Now, let's ask an important physics question. Let's say you observed 2 FRBs back to back. Given the probability of this occurence is so low, is something significant happening in the universe? <a href=\"https://en.wikipedia.org/wiki/Fast_radio_burst#FRB_201124\" target=\"_blank\">Read here.</a>\n",
    "\n",
    "Secondly, why is the probability of 19 observations in 365 days so low? (only 10%). To understand this let's make a plot. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "78aa681e",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.2-runcell02\n",
    "\n",
    "p=19/365\n",
    "n=365\n",
    "m=np.arange(0,50)\n",
    "binomial=stats.binom.pmf(m,n,p)\n",
    "\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "plotBinomial(m,binomial)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c7958a63",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "Getting exactly 19 observations is unlikely because there is variation. What we really want to do is integrate the number of observations that are either greater or less than 19. This is the cumulative distribution function. \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathrm{CDF}(\\mathrm{binomial}(x)) = \\int_{-\\infty}^{x} \\mathrm{binomial}(u;p,k) du \n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "The nice thing is that this is all built into our statistics code. Let's plot it!\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8d21604c",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_02",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.2-runcell03\n",
    "\n",
    "p=19/365\n",
    "n=365\n",
    "m=np.arange(0,50)\n",
    "binomial=stats.binom.pmf(m,n,p)\n",
    "binomialcdf=stats.binom.cdf(m,n,p)\n",
    "print(\"p-value at 19 obs:\",stats.binom.cdf(19,n,p))\n",
    "print(\"p-value at 1  obs:\",stats.binom.cdf(1,n,p))\n",
    "\n",
    "plt.plot(m,binomialcdf,'o', label=\"Binomial CDF\")\n",
    "plt.vlines(m,0, binomialcdf, color=plt.gca().lines[-1].get_color())\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.plot(m,binomial,'o', label=\"Binomial PMF\")\n",
    "plt.vlines(m,0, binomial, color=plt.gca().lines[-1].get_color())\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.xlabel(\"Number of observations per year\")\n",
    "plt.ylabel(\"Probability\")\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "mean = np.average(m, weights=binomial)\n",
    "variance = np.average((m-mean)**2, weights=binomial)\n",
    "print(\"mean:\",mean,\"stddev:\",np.sqrt(variance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "671cb843",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_02"
    ]
   },
   "source": [
    "So, now we see clearly that the CDF is at approximately 50% for 19 observations. It's not exactly 50% for the simple fact that this is a discrete distribution. However, the expectation will be at exactly 19. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2af4818",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_2'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_2) | [Next Section](#section_2_3) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5febd043",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.2.1: Rate of GW Detections</span>\n",
    "\n",
    "Let's do another related problem. With the current rate of gravitational wave (GW) detections, we observe a GW once per week. What is the probability of 3 or more gravitational waves being detected in one week? Use the starting code below to compute your answer."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab40146a",
   "metadata": {
    "hideCode": false,
    "tags": [
     "solution",
     "py",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.2.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "nobs = #YOUR CODE HERE\n",
    "n = #YOUR CODE HERE\n",
    "p = #YOUR CODE HERE\n",
    "k = np.arange(0,10)\n",
    "\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "total1=0\n",
    "for i0 in range(len(k)):\n",
    "    if k[i0] > nobs:\n",
    "        total1+= binomial[i0]\n",
    "\n",
    "print(\"binomial:\",total1)\n",
    "\n",
    "plotBinomial(k,binomial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1630a530",
   "metadata": {
    "hideCode": true,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.2.1\n",
    "\n",
    "import numpy as np\n",
    "import scipy.stats as stats\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "n=7.\n",
    "p=1./7.\n",
    "k = np.arange(0,10)\n",
    "\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "total1=0\n",
    "for i0 in range(len(k)):\n",
    "    if k[i0] > 2:\n",
    "        total1+= binomial[i0]\n",
    "\n",
    "print(\"binomial:\",total1)\n",
    "\n",
    "plotBinomial(k,binomial)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1704db8e",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "0.06523\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "Use the following code to make this calculation:\n",
    "\n",
    "<pre>\n",
    "import scipy.stats as stats\n",
    "#solution\n",
    "n=7.\n",
    "p=1./7.\n",
    "k=np.arange(0,10)\n",
    "binomial=stats.binom.pmf(k,n,p)\n",
    "total1=0\n",
    "for i0 in range(len(k)):\n",
    "    if k[i0] > 2:\n",
    "        total1+= binomial[i0]\n",
    "print(\"binomial:\",total1)\n",
    "</pre>\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0417592e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 2.2.1a (ungraded)\n",
    ">  \n",
    ">Try plotting this distribution! Additionally, what is the probability distribution for the number of GW events observed over a whole year, and what is the mean and variance of this distribution? Use the starting code below."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3ad465dc",
   "metadata": {
    "tags": [
     "py",
     "draft",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>FOLLOW-UP: L2.2.1a\n",
    "# Use this cell for drafting your solution (if desired)\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "\n",
    "\n",
    "#now what about for GWs in a year\n",
    "n = #YOUR CODE HERE\n",
    "p = #YOUR CODE HERE\n",
    "m = #YOUR CODE HERE\n",
    "\n",
    "binomial=stats.binom.pmf(m,n,p)\n",
    "\n",
    "plotBinomial(k,binomial)\n",
    "plt.show()\n",
    "\n",
    "average  = np.average(m, weights=binomial)\n",
    "variance = np.average((m-average)**2, weights=binomial)\n",
    "print(\"mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fc5fb182",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.2.1a\n",
    "\n",
    "import scipy.stats as stats\n",
    "\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "\n",
    "#now what about for GWs in a year\n",
    "n=365. #we don't need to divide by day!\n",
    "p=1/7.\n",
    "m=np.arange(0,100)\n",
    "binomial=stats.binom.pmf(m,n,p)\n",
    "\n",
    "plotBinomial(m,binomial)\n",
    "plt.show()\n",
    "\n",
    "average  = np.average(m, weights=binomial)\n",
    "variance = np.average((m-average)**2, weights=binomial)\n",
    "print(\"mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d98afb62",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.2.2: Probability of Coin Flips</span>\n",
    "\n",
    "What is the probability of 2 heads in 10 coin flips, given a 50% probability for heads? What about if there is a 10% probability for heads?\n",
    "\n",
    "Enter your answer as a list of two numbers, where the numbers correspond to probabilities: `[prob with p=50%, prob with p=10%]`\n",
    "\n",
    "Use the starting code below to aid your calculation."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b1378c33",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.2.2\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "def prob(nheads=2,nflips=10,p=0.5):\n",
    "    return #your code here\n",
    "\n",
    "print(\"2 heads in 10 flips:\",prob())\n",
    "print(\"2 heads in 10 flips:\",prob(p=1/10))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3d5da34f",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.2.2\n",
    "\n",
    "def prob(nheads=2,nflips=10,p=0.5):\n",
    "    return stats.binom.pmf(nheads,nflips,p)\n",
    "\n",
    "print(\"2 heads in 10 flips:\",prob())\n",
    "print(\"2 heads in 10 flips:\",prob(p=1/10))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4d8b39b1",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[0.0439,0.1937]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "To solve the problem, we simply ran a binomial distribution for both scenerios.\n",
    "    \n",
    "Use the following code to make this calculation:\n",
    "\n",
    "<pre>\n",
    "def prob(nheads=2,nflips=10,p=0.5):\n",
    "    return stats.binom.pmf(nheads,nflips,p)\n",
    "\n",
    "print(\"2 heads in 10 flips:\",prob())\n",
    "print(\"2 heads in 10 flips:\",prob(p=1/10))\n",
    "</pre>\n",
    "\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "910a11da",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    ">#### Follow-up 2.2.2a (ungraded)\n",
    "> \n",
    ">Given the situation described in the exercise above (2 heads seen in 10 coin flips), how confidently can we conclude that our coin is biased?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d55932f2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_3'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.3 The Poisson Distribution</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_2) | [Exercises](#exercises_2_3) | [Next Section](#section_2_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a1b3c733",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "The ugly thing about the binomial distribution is that it has these damn factorials. One way to get rid of the factorials is to do an approximation of the binomial distribution. We can define this by taking a limit over the number of experiments going to infinity $n\\rightarrow\\infty$. To do this, we first define $\\lambda$ as:\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\lambda = \\lim_{n\\rightarrow\\infty} np \\rightarrow p=\\frac{\\lambda}{n} \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Now, in this limiting case, we can replace the binomial distribution with an approximate form that has fewer factorials:\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "\\lim_{n\\rightarrow\\infty}\\frac{n!}{m!(n-m)!}p^{m}(1-p)^{n} & = & \\frac{n(n-1)...(n-m+1)}{m!}\\left(\\frac{\\lambda}{n}\\right)^{m}\\left(1-\\frac{\\lambda}{n}\\right)^{n} \\\\\n",
    "&\\approx&\\frac{n^m}{m!}\\frac{\\lambda^{m}}{n^{m}}\\left(1-\\frac{\\lambda}{n}\\right)^{n} \\\\\n",
    "&\\approx&\\frac{\\lambda^{m}}{m!}e^{-\\lambda} \\\\\n",
    "f(m;\\lambda=np) & = & \\frac{\\lambda^{m}}{m!}e^{-\\lambda}\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This form is known as the Poisson distribution, and is achieved by taking the binomial distribution to the large $n$ limit. We still have a pesky factorial, but one factorial is better than three factorials. \n",
    "\n",
    "We can treat the Poisson distribution just like the binomial distribution, using something similar to the computations done previously for the mean and variance (of $n\\rightarrow\\infty$ experiments sampling a Poisson distribution, noting that $p\\rightarrow0$ in the large $n$ limit):\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[f(x)]=\\lambda \\\\\n",
    "V[f(x)]=\\lambda\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "What is most important from this observation is that the standard deviation of the distribution goes as the $\\sqrt{\\lambda}$ or the square root of the mean of the distribution. This will play a critical role going forward. \n",
    "\n",
    "<br>\n",
    "<!--end-block-->\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3a9169de",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "<h3>Comparison</h3>\n",
    "\n",
    "Now, let's see how a Poisson compares to a binomial in our previous plots. Let's use our FRB example $p=19/365$. Alternatively, let's also consider the probability of a sunny day in Boston $p=200/365$.\n",
    "\n",
    "<br>\n",
    "<!--end-block-->\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f0862a0",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_03",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.3-runcell01\n",
    "def plotBinomial(iX,iBinomial,label='Binomial',color='black'):\n",
    "    plt.plot(iX,iBinomial,'o')\n",
    "    plt.vlines(iX,0, iBinomial,label=label,color=color,alpha=0.5)\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.xlabel(\"Number of observations per year\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "\n",
    "    \n",
    "#Let's make a function for plotting\n",
    "def plotWeekYear(p, title=''):\n",
    "    #Week comparison\n",
    "    n=7\n",
    "    m=np.arange(0,n+1)\n",
    "    binomial_week=stats.binom.pmf(m,n,p)\n",
    "    poisson_week=stats.poisson.pmf(m,n*p)#note we give lambda=n*p\n",
    "    #plotting code\n",
    "    plt.title(title)\n",
    "    plotBinomial(m,binomial_week,label='Binomial',color='blue')\n",
    "    plotBinomial(m,poisson_week,label='Poisson',color='orange')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('number of observations per week')\n",
    "    plt.show()\n",
    "\n",
    "    n=365\n",
    "    m=np.arange(0,2*p*n)\n",
    "    binomial_year=stats.binom.pmf(m,n,p)\n",
    "    poisson_year=stats.poisson.pmf(m,n*p)#note we give lambda=n*p\n",
    "    #plotting code\n",
    "    plt.title(title)\n",
    "    plotBinomial(m,binomial_year,label='Binomial',color='blue')\n",
    "    plotBinomial(m,poisson_year,label='Poisson',color='orange')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    average  = np.average(m, weights=binomial_year)\n",
    "    variance = np.average((m-average)**2, weights=binomial_year)\n",
    "    print(\"Yearly Binomial mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "    \n",
    "    average  = np.average(m, weights=poisson_year)\n",
    "    variance = np.average((m-average)**2, weights=poisson_year)\n",
    "    print(\"Yearly Poisson mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "    print()\n",
    "\n",
    "    \n",
    "#First FRBs\n",
    "p=19/365\n",
    "plotWeekYear(p, title='Probability of Observing FRBs')\n",
    "\n",
    "#Now let's do sunny days\n",
    "p=200/365\n",
    "plotWeekYear(p, title='Probability of Sunny Days in Boston')\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f4059087",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_03"
    ]
   },
   "source": [
    "So, we see that the Poisson approximation is really quite good for the case where the $p\\ll1$. However, when $p$ is large (like the number of sunny days in the example above) and the number of events is small, the Poisson result can be quite far off. Just look at the number of sunny days per week. The binomial distributions gives about two percent probability of having one sunny day in a week, vs. nearly eight percent probability given by the Poisson distribution. Looking at the number of observations per year, clearly there is also a dramatic difference in the width of the distributions with $p$ is large.\n",
    "\n",
    "Which one of these distributions is correct for weather? (Answer: Neither are good because the weather from yesterday gives you some information about what will happen today, it's not a random process on a day-to-day level)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54e627c0",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_3'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_3) | [Next Section](#section_2_4) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4e87813d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.3.1: GW Detection Timescale Comparison</span>\n",
    "\n",
    "With the current rate of Gravitational wave detections, we observe a GW once per week (one way to phrase this is that the probability of a GW on a given day is 1/7). Compare the Poisson and binomial distributions for gravitational wave observations over the period of a week vs. a year. **Hint: the previously defined function `plotWeekYear` may be useful.**\n",
    "\n",
    "Can GW detections be reasonably approximated by a Poisson process?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "dc870c7a",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.3.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "pass"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e2d0e62",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.3.1\n",
    "\n",
    "#Answer\n",
    "#Now with the current rate of Gravitational wave detection we observe a GW once per week\n",
    "#One way to phrase this is that the probability of a GW on a day is 1/7, and we have 7 days in a week\n",
    "\n",
    "#Use the previously defined function\n",
    "def plotWeekYear(p, title=''):\n",
    "    #Week comparison\n",
    "    n=7\n",
    "    k=np.arange(0,n+1)\n",
    "    binomial_week=stats.binom.pmf(k,n,p)\n",
    "    poisson_week=stats.poisson.pmf(k,n*p)#note we give lambda=n*p\n",
    "    plt.title(title)\n",
    "    plotBinomial(k,binomial_week,label='Binomial',color='blue')\n",
    "    plotBinomial(k,poisson_week,label='Poisson',color='orange')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.xlabel('number of observations per week')\n",
    "    plt.show()\n",
    "\n",
    "    average  = np.average(k, weights=binomial_week)\n",
    "    variance = np.average((k-average)**2, weights=binomial_week)\n",
    "    print(\"Weekly Binomial mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "    \n",
    "    average  = np.average(k, weights=poisson_week)\n",
    "    variance = np.average((k-average)**2, weights=poisson_week)\n",
    "    print(\"Weekly Poisson mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "\n",
    "\n",
    "    n=365\n",
    "    k=np.arange(0,2*p*n)\n",
    "    binomial_year=stats.binom.pmf(k,n,p)\n",
    "    poisson_year=stats.poisson.pmf(k,n*p)#note we give lambda=n*p\n",
    "    plt.title(title)\n",
    "    plotBinomial(k,binomial_year,label='Binomial',color='blue')\n",
    "    plotBinomial(k,poisson_year,label='Poisson',color='orange')\n",
    "    plt.legend(loc='upper right')\n",
    "    plt.show()\n",
    "\n",
    "    average  = np.average(k, weights=binomial_year)\n",
    "    variance = np.average((k-average)**2, weights=binomial_year)\n",
    "    print(\"Yearly Binomial mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "    \n",
    "    average  = np.average(k, weights=poisson_year)\n",
    "    variance = np.average((k-average)**2, weights=poisson_year)\n",
    "    print(\"Yearly Poisson mean:\",average,\"stddev:\",np.sqrt(variance))\n",
    "\n",
    "    \n",
    "p=1./7.\n",
    "plotWeekYear(p, 'Probability of Observing GW Event')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c4b5995",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "        \n",
    "**EXPLANATION:**\n",
    "        \n",
    "Yes, since $p$ is relatively low. The distributions look approximately the same.\n",
    "\n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6694499b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.3.2: Fraction of GWs per Week</span>\n",
    "\n",
    "If more GWs are detected per week, on average, will a Poisson distribution be a BETTER fit for the distribution of GW observations or a WORSE fit?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7801d58d",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.4 Poisson Distribution Continued</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_3) | [Exercises](#exercises_2_4) | [Next Section](#section_2_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4c545768",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "Now, why are we spending so much time on Poisson distributions? Let's say I have a distribution that is flat and I sampled that distribution 10000 times, and then made a histogram with 100 bins. Let's make a distribution like that!"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12d02cbd",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.4-runcell01\n",
    "\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "N=10000\n",
    "nbins=100\n",
    "sample  = np.random.uniform (0,1,N)\n",
    "\n",
    "def plotHist(iSample,iNBins):\n",
    "    histy, bin_edges = np.histogram(iSample, bins=iNBins)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    ax.set_ylim([0,2*N/nbins])\n",
    "    plt.plot(bin_centers,histy,drawstyle = 'steps-mid')\n",
    "    plt.xlabel(\"x\")\n",
    "    plt.ylabel(\"Events/bin\")\n",
    "    plt.show()\n",
    "    return bin_centers, histy\n",
    "\n",
    "_,_ = plotHist(sample,nbins)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "da42eb57",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "Each bin in the sampling has a 1/100 probability of being in any one of the respecitve bins. \n",
    "\n",
    "Let's look at the mean and variance over the bins. What is the distribution of the variations over these bins? \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e0337645",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.4-runcell02\n",
    "\n",
    "#copy and past above distribution\n",
    "fig, ax = plt.subplots(figsize=(9,6))\n",
    "N=10000\n",
    "#N=1000000 #Try larger N value\n",
    "nbins=100\n",
    "sample  = np.random.uniform (0,1,N)\n",
    "histx, histy = plotHist(sample,nbins)\n",
    "\n",
    "#code that just makes a histogram and gives error bars on each bin \n",
    "def normhist(iVars,iNbins=30,iNormalize=True):\n",
    "    y0, bin_edges = np.histogram(iVars, bins=iNbins)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0 = 1 \n",
    "    if iNormalize:\n",
    "        norm0=len(iVars)*(bin_edges[-1]-bin_edges[0])/iNbins\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0, ###<===== Pay attention to the error why is it this way? \n",
    "                 drawstyle = 'steps-mid',c='red')\n",
    "    return bin_centers,y0,bin_edges\n",
    "\n",
    "#Ok first lets look at the mean and variance across the bins \n",
    "residx,residy,_=normhist(histy)\n",
    "haverage  = histy.mean()\n",
    "hvariance = histy.var()\n",
    "print(\"Actual mean:\",haverage,\"Variance:\",hvariance) \n",
    "\n",
    "#Now lets do this analytically with an Poisson approximation\n",
    "#Now since we have 100 bins with p=1/100 and we sample 10000 times we have lamb=np= N (1/nbins)\n",
    "lamb=N/nbins # = n = number experiments * p = (1/nbins) \n",
    "k=np.arange(0.55*N/nbins,1.45*N/nbins) \n",
    "#k=np.arange(0.85*N/nbins,1.15*N/nbins) #adjust range if using larger N \n",
    "poisson=stats.poisson.pmf(k,lamb)#lambda = n * p \n",
    "paverage  = np.average(k, weights=poisson)\n",
    "pvariance = np.average((k-paverage)**2, weights=poisson)\n",
    "print(\"Poisson mean:\",paverage,\"Variance:\",pvariance)\n",
    "\n",
    "plt.plot(k,poisson,'o')\n",
    "# plt.vlines(k,0, poisson)\n",
    "plt.ylim(bottom=0)\n",
    "\n",
    "plt.xlabel(\"Mean per bin\")\n",
    "plt.ylabel(\"probability\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cc403c95",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_04"
    ]
   },
   "source": [
    "It's a Poisson distribution! Now, this brings us to a very important plot. If we have a histogram with $N$ events in a particular bin, what are the fluctuations in that bin? \n",
    "\n",
    "\n",
    "If it is Poisson, then the variance is going to be $N$ and the standard deviation is going to be $\\sqrt{N}$. As a consequence, we can characterize the fluctuations per bin by the standard deviation. Thus, whenever we have a plot with data and we want to plot the expected fluctuations per bin, we plot the Poisson fluctuations. The previous distribution would thus look like:"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "342d96ea",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_04",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.4-runcell03\n",
    "\n",
    "#And so the bins are Poisson fluctuated. This is why when we plot data in a histogram we put error bars \n",
    "#Corresponding the Poisson uncertainty in a bin\n",
    "N=10000\n",
    "nbins=100\n",
    "sample  = np.random.uniform (0,1,N)\n",
    "histy, bin_edges = np.histogram(sample, bins=nbins)\n",
    "yerr=np.sqrt(histy) #### <==== Now we add the uncertainty\n",
    "bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "ax.set_ylim([0,2*N/nbins])\n",
    "\n",
    "#Here is the command\n",
    "plt.errorbar(bin_centers,histy,yerr=yerr,marker='.',c='black',linestyle = 'None',label='Data')\n",
    "print(np.mean(yerr))\n",
    "\n",
    "k=np.arange(0,1,0.01)\n",
    "vals=np.full((100),N/nbins)\n",
    "plt.plot(k,vals,'o--',label=\"Expected value\")\n",
    "plt.ylim(0,1.5*(N/nbins))\n",
    "plt.xlabel(\"x\")\n",
    "plt.ylabel(\"Events/bin\")\n",
    "plt.legend(loc='lower right')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ca231f1b",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_4'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_4) | [Next Section](#section_2_5) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a58d32cc",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.4.1: Calculating Error for a Poisson Distribution</span>\n",
    "\n",
    "For 100 bins, what is the Poisson error (standard deviation) averaged over all bins for an experiment run 100, 1000, and 10000 times?\n",
    "\n",
    "Hint: You can use the code below to help calculate the yerr for all bins and then average them yourself.\n",
    "\n",
    "Enter your answer as a list of numbers rounded to the nearest integer: `[avg(100), avg(1000), avg(10000)]`."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b4541317",
   "metadata": {
    "hideCode": true,
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.4.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "N=10000 #YOUR CODE HERE [VARY 100, 1000, 10000]\n",
    "nbins= 100 \n",
    "\n",
    "sample  = np.random.uniform (0,1,N)\n",
    "histy, bin_edges = np.histogram(sample, bins=nbins)\n",
    "yerr=np.sqrt(histy)\n",
    "\n",
    "#YOUR CODE HERE"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2cac1578",
   "metadata": {
    "hideCode": true,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.4.1\n",
    "\n",
    "N=10000\n",
    "nbins=100\n",
    "sample  = np.random.uniform (0,1,N)\n",
    "histy, bin_edges = np.histogram(sample, bins=nbins)\n",
    "yerr=np.sqrt(histy)\n",
    "print(np.mean(yerr))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c8541fb8",
   "metadata": {
    "hideCode": false,
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "[1,3,10]\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "The standard deviation of a Poisson distribution is $\\sqrt{N}$. Each bin's standard deviation thus becomes $\\sqrt{\\frac{N}{N_{bin}}}$. \n",
    "\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f0d13378",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.5 The Gaussian Distribution</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_4) | [Exercises](#exercises_2_5) | [Next Section](#section_2_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8c6cddf6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "The Poisson distribution discussed above is very powerful. However, we will often view it as a subset of the Normal or Gaussian distributions, given by the form: \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{N}(x,\\mu,\\sigma)=\\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(x-\\mu)^2}{2\\sigma^2}}\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This distribution has the very important properties that you can derive yourselves: \n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "E[N(x,\\mu,\\sigma]=\\mu \\\\\n",
    "V[N(x,\\mu,\\sigma]=\\sigma^2 \\\\\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "It is effectively a Poisson distribution where the variance is now not $\\sigma=\\sqrt{\\lambda}=\\sqrt{\\mu}$, but instead a free parameter $\\sigma$. A Gaussian is often viewed as a generalized version of the Poisson distribution. There are many names for this distribution. Mathematicians and statistician's often call this the Normal distribution. The public frequently refers to this as the bell curve. Physicists call this the Gaussian distribution. These notes will refer to it as Gaussian, since Normal can be confusing (especially with non-native English speakers). \n",
    "\n",
    "\n",
    "While the Gaussian distribution looks simple, there are several things to notice about it. The most important is that the CDF\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\mathcal{N}(x,\\mu,\\sigma)=\\int_{-\\infty}^{x} \\frac{1}{\\sqrt{2\\pi\\sigma^2}} e^{-\\frac{(u-\\mu)^2}{2\\sigma^2}} du\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "does not have a closed analytic form. In fact, we have to integrate this numerically. \n",
    "\n",
    "What makes the Gaussian distribution so powerful is that it appears all over the place. Let's understand the Gaussian distribution in the context of the most important theorem in all of statistics.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6031ed00",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Central Limit Theorem</h3>\n",
    "\n",
    "Recall that in Lesson 1 we derived the sum distribution of two objects. This gave us a triangle distribution. What happens when we consider the sum of more than just two numbers, in particular as we approach the sum of a very large set of numbers?\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d737ee3d",
   "metadata": {
    "scrolled": false,
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.5-runcell01\n",
    "\n",
    "import math\n",
    "\n",
    "\n",
    "def normhist(iVars,iNbins=30,iNormalize=True):\n",
    "    y0, bin_edges = np.histogram(iVars, bins=iNbins)\n",
    "    bin_centers = 0.5*(bin_edges[1:] + bin_edges[:-1])\n",
    "    norm0 = 1 \n",
    "    if iNormalize:\n",
    "        norm0=len(iVars)*(bin_edges[-1]-bin_edges[0])/iNbins\n",
    "    plt.errorbar(bin_centers,y0/norm0,yerr=y0**0.5/norm0,drawstyle = 'steps-mid',c='red')\n",
    "    return bin_centers,y0,bin_edges\n",
    "\n",
    "def plotSum(iN):\n",
    "    ntoys=100000\n",
    "    sums=np.array([])\n",
    "    for i0 in range(ntoys):\n",
    "        pToy = np.random.uniform(0,10,iN)\n",
    "        sums = np.append(sums,pToy.sum())\n",
    "    _,_,binrange=normhist(sums) #plots a Gaussian hist\n",
    "    k=np.linspace(binrange[0],binrange[-1], 50)\n",
    "    normal=stats.norm.pdf(k,sums.mean(),sums.std())\n",
    "    plt.plot(k,normal,'o-')\n",
    "    plt.xlabel(\"Number of successes\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    print(\"Summing:\",iN,\" numbers with mean:\",sums.mean(),\" and std-deviation\",sums.std(),sums.mean()/math.sqrt(3*iN))\n",
    "    plt.show()\n",
    "\n",
    "plotSum(1)\n",
    "plotSum(2)\n",
    "plotSum(3)\n",
    "plotSum(4)\n",
    "plotSum(50)\n",
    "plotSum(5000)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ce376be6",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "So, the sum of a large group of random numbers drawn from a uniform distribution approaches a Gaussian. This is a very important statement. Effectively, this means that any combination of random variables is a Gaussian distribution; this sounds crazy! We will not show the full proof here, but suffice it to say, doing the integrals yields the same observation. \n",
    "\n",
    "\n",
    "Another interesting thing to note is that the standard deviation of this Gaussian is incidentally given by the (range of uniform distribution)$/\\sqrt{12}$. To verify this, for the last experiment with $N = 5000$ draws, we have the following:\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5d436079",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.5-runcell02\n",
    "\n",
    "N = 5000\n",
    "unif_range = 10\n",
    "print(\"stddev:\", np.sqrt(N * unif_range ** 2 / 12))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7bf5703e",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "This matches the standard deviation we obtain numerically. To see this analytically, let's compute it using the variable substitution $2a'=b-a$.\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V[x]&=&\\int_{a}^{b}\\frac{1}{b-a}\\left(x-\\frac{b-a}{2}\\right)^2 dx\\\\\n",
    "V[x]&=&\\int_{-a^\\prime}^{a^\\prime}\\frac{1}{2a^\\prime}\\left(x\\right)^2 dx\\\\\n",
    "V[x]&=&\\frac{1}{2a^\\prime}\\frac{1}{3}\\left(x\\right)^3|_{-a^\\prime}^{a^\\prime} \\\\\n",
    "V[x]&=&\\frac{2a'^3}{2a^\\prime}\\frac{1}{3} \\\\\n",
    "V[x]&=&\\frac{\\left(\\frac{b-a}{2}\\right)^2}{3} \\\\\n",
    "V[x]&=&\\frac{\\left(b-a\\right)^2}{12} \\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "So, to get the RMS of $N$ random variables summed up, we multiply this variance by $N$. We can further note that the average of $N$ summed variables gives $\\bar{x}=\\frac{b-a}{2}$. Combining all of this, we have: \n",
    "\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "V[x_1+x_2+...+x_N]&=& N \\frac{\\left(b-a\\right)^2}{12}\\\\\n",
    "V[x_1+x_2+...+x_N]&=& N \\frac{\\bar{x}^2}{3}\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This is why we can calculate the standard deviation of our sample using the above formula. In any case, we will take this demo as a proof by demo of what we call **the central limit theorem** which states that **for any distribution composed of inputs from a large number of continuous random variables, the sum tends to a Gaussian**. For fun, outside of class, go ahead and derive it."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e0355fe1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_05"
    ]
   },
   "source": [
    "<h3>Comparison of Poisson and Gaussian</h3>\n",
    "\n",
    "Let's compare the Gaussian with a Poisson distribution, so we can connect all of our friends together. \n",
    "\n",
    "Compare the Poisson distribution to a Gaussian distribution for $\\lambda$=3, 15, 100. How do these distributions vary? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "51383fde",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_05",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.5-runcell03\n",
    "\n",
    "#solution 1\n",
    "##### Let's plot a Gaussian and Poisson with same mean and RMS\n",
    "def poisGausPlot(n):\n",
    "    lamb=n\n",
    "    k=np.arange(-2,3.0*n)\n",
    "    poisson=stats.poisson.pmf(k,lamb) ### <=====\n",
    "    normal=stats.norm.pdf(k,n,math.sqrt(n)) #### <=====\n",
    "    plt.plot(k,poisson,'o',label='Poisson')\n",
    "    plt.vlines(k,0, poisson, color=plt.gca().lines[-1].get_color())\n",
    "    plt.ylim(bottom=0)\n",
    "    plt.plot(k,normal,'-',label='Gaussian')\n",
    "    plt.xlabel(\"Number of successes\")\n",
    "    plt.ylabel(\"Probability\")\n",
    "    plt.legend(loc='lower right')\n",
    "    plt.show()\n",
    "\n",
    "poisGausPlot(3)\n",
    "poisGausPlot(15)\n",
    "poisGausPlot(100)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6c444db1",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_5'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_5) | [Next Section](#section_2_6) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f3c3a1c7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.5.1: Sum of Two Gaussians</span>\n",
    "\n",
    "Show that the sum of two Gaussian distributions is also Gaussian. To do this, plot the normalized histogram of the sum of two numbers drawn from identical Gaussian distributions. In the same figure, plot a Gaussian distribution with mean and standard deviation equal to the mean and standard deviation of the summed distribution. Write your own code, or run the code below.\n",
    "\n",
    "\n",
    "Based on the output of your code, how is the standard deviation of the summed distribution, $\\sigma_{\\mathrm{sum}}$, related to the standard deviation of the Gaussian distributions from which the samples are drawn (call this $\\sigma_0$)? Choose from the options below.\n",
    "\n",
    "- $\\sigma_{\\mathrm{sum}} = 2\\sigma_0$\n",
    "- $\\sigma_{\\mathrm{sum}} = \\sqrt{2}\\sigma_0$\n",
    "- $\\sigma_{\\mathrm{sum}} = \\sigma_0$\n",
    "- $\\sigma_{\\mathrm{sum}} = \\sigma_0/\\sqrt{2}$\n",
    "- $\\sigma_{\\mathrm{sum}} = \\sigma_0/2$\n",
    "\n",
    "\n",
    "How would this relation change if you summed more samples (here we just did 2, 100000 times). Try varying $\\sigma$ and the number of samples chosen (where are these defined in the code)?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "07e31baf",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.5.1\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "#Generate 2 Gaussian and sum \n",
    "ntoys=100000\n",
    "istdev=1\n",
    "sums=np.array([])\n",
    "for i0 in range(ntoys):\n",
    "    pToy = np.random.normal(0,istdev,2)\n",
    "    sums = np.append(sums,pToy.sum())\n",
    "_,_,binrange=normhist(sums)\n",
    "\n",
    "k=np.arange(binrange[0],binrange[-1])\n",
    "normal=stats.norm.pdf(k,sums.mean(),sums.std())\n",
    "\n",
    "plt.plot(k,normal,'o-')\n",
    "plt.xlabel(\"Number of successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "print(\"Summing: 2, numbers with mean:\",sums.mean(),\" and std-deviation\",sums.std())\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b22c6dd",
   "metadata": {
    "hideCode": true,
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.5.1\n",
    "\n",
    "#Generate 2 Gaussian and sum \n",
    "ntoys=100000\n",
    "istdev=1\n",
    "sums=np.array([])\n",
    "for i0 in range(ntoys):\n",
    "    pToy = np.random.normal(0,istdev,2)\n",
    "    sums = np.append(sums,pToy.sum())\n",
    "_,_,binrange=normhist(sums)\n",
    "\n",
    "k=np.arange(binrange[0],binrange[-1])\n",
    "normal=stats.norm.pdf(k,sums.mean(),sums.std())\n",
    "\n",
    "plt.plot(k,normal,'o-')\n",
    "plt.xlabel(\"Number of successes\")\n",
    "plt.ylabel(\"Probability\")\n",
    "print(\"Summing: 2, numbers with mean:\",sums.mean(),\" and std-deviation\",sums.std())\n",
    "print(\"ratio of sum std-deviation to sampled std-deviation\", sums.std()/istdev)\n",
    "plt.show()\n",
    "\n",
    "#we expect a standard deviation of sqrt(2), this follows from Var(x1+x2)=N Var(x)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "618ffa1a",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "See the code above, we expect  $\\sqrt{2}\\sigma_0$ for the standard devaition\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "    \n",
    "We expect a standard deviation of $\\sqrt{2}\\sigma_0$, this follows from `Var(x1+x2)=N Var(x)`, if `x1` and `x2` have the same variances.\n",
    "    \n",
    "You could include the following line to demonstrate this:\n",
    "\n",
    "<pre>\n",
    "print(\"ratio of sum std-deviation to sampled std-deviation\", sums.std()/istdev)\n",
    "</pre>\n",
    "    \n",
    "We will explore this more in the next section.\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "4b6815a7",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 2.5.1a (ungraded)\n",
    ">  \n",
    ">We've compared the Poisson and binomial distributions, and Poisson and Gaussian distribution. Now try comparing the binomial and Gaussian distributions. What similarities and differences do they have?"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9dffc6f2",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.6 Uncertainties in Measurement</h2>  \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_5) | [Exercises](#exercises_2_6) | [Next Section](#section_2_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c15f0c01",
   "metadata": {
    "tags": [
     "learner",
     "md"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "Run the code below to view the slides for this lesson, which are discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides2.html\" target=\"_blank\">HERE</a>."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8173c533",
   "metadata": {
    "tags": [
     "learner",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.6-slides\n",
    "\n",
    "from IPython.display import IFrame\n",
    "IFrame(src='https://mitx-8s50.github.io/slides/L02/slides2.html', width=975, height=550)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "55a3efad",
   "metadata": {
    "tags": [
     "md",
     "lect_06"
    ]
   },
   "source": [
    "<h3>Slides</h3>\n",
    "\n",
    "View the slides for this section below, which are also discussed in the videos. You can also open the slides in a separate window <a href=\"https://mitx-8s50.github.io/slides/L02/slides2.html\" target=\"_blank\">HERE</a>.\n",
    "\n",
    "<p align=\"center\">\n",
    "<iframe src=\"https://mitx-8s50.github.io/slides/L02/slides2.html\" width=\"900\", height=\"550\" frameBorder=\"0\"/>\n",
    "</p>"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "db8fc145",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "<h3>Overview</h3>\n",
    "\n",
    "In the previous Lesson, we explained that the expectation is the mean of a distribution and the variance is a measure of the width. When we perform a measurement, we are just sampling from an unknown distribution, or worse yet, we are sampling from an unknown distribution and then distorting that distribution with some sort of effect. \n",
    "\n",
    "Let's say that you are sampling a distribution that is fundamentally a Gaussian. Now on top of this, we then distort this distribution by a function $f(x)$. This distortion will modify the distribution of the events, making it less Gaussian, or shrinking and stretching it. A distortion function can arise from many aspects of the measurement, such as from sending a particle through a magnetic field, or having light reflect off a mirror. There are countless examples of such distortive effects in physical measurements. If we happen to know $f(x)$ as well as the distribution of $x$, how does the shape of $x$ get distorted by $f(x)$?\n",
    "\n",
    "To answer this question, consider the case where the probability of the input distribution is $p(x)$. If we define $x^{\\prime}=f(x)$,  the probability to be in a small region $dx^\\prime$ of the modified coordinates is defined by\n",
    "$x^{\\prime}=f(x)$,  the probability to be in small region $dx^\\prime$ of the modified coordinates is defined by\n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    "p^{\\prime}(x^\\prime)dx^\\prime&=&f(p(x))dx^\\prime \\\\\n",
    "                             &=&f(p(x))\\frac{dx^\\prime}{dx}dx\\\\\n",
    "                             &=&f(p(x))\\frac{df}{dx}dx\\\\\n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "This follows from the fact that the spread of a function sampled from $p(x)$ would be modified by the spread of $f(x)$ defined as  $f(x+\\Delta x)-f(x)\\approx\\frac{df}{dx}\\Delta x$. \n",
    "\n",
    "As a simple example, in the case of $f(x)=x^{2}$ or $\\frac{df}{dx}=2x$. What that means is that $\\sigma_{f(x)}\\approx2x\\sigma_{x}$. Let's actually see that empirically.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a893e204",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.6-runcell01\n",
    "\n",
    "#Now let's say we do a measurement, and the measurement takes an input variable that is varying, \n",
    "#and applies a function to it. What is the spread of the function\n",
    "ntries=1000\n",
    "sigx=1\n",
    "meas = np.full(ntries,100) #The value 100, 1k times\n",
    "unc  = np.random.normal (0,sigx, ntries) #a randomly sampled value from a Gaussian with width 1 1k times\n",
    "meas = meas+unc # the value 100 now smeared with sigma=1\n",
    "\n",
    "def function(ix):#our function\n",
    "    return ix**2\n",
    "outmeas = function(meas)\n",
    "_,_,_=normhist(outmeas)\n",
    "\n",
    "print(\"Mean:\",outmeas.mean(),\"Stddeviation:\",outmeas.std())\n",
    "print(\"Predicted Mean:\",function(100),\"Stddeviation:\",2*100*sigx) #expect it to be 2*100*1\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5f486d95",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "Now, what about if we have two sources of uncertainty? This is a little bit different in the sense that these variations are independent of each other. Let's consider the very simple function $f(x)=x$. Now, let's say that $x$ can vary by a Gaussian distributed variable $\\sigma_1$ and a second Gaussian distributed variable $\\sigma_2$. If we consider these variations, we have that $f(x)$ will be modified by\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    " f(x) = x + \\sigma_1 + \\sigma_2\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "This will give us two Gaussians. If we look to see the variance of this distribution, we can treat these two fluctuations as two independent measurements, which means we can write. \n",
    "\n",
    "$$\n",
    "\\begin{eqnarray}\n",
    " V[f(x)] &=& V(x) + V(\\sigma_1) + V(\\sigma_2)\\\\\n",
    "                &=& \\sigma_1^2 + \\sigma_2^2 \n",
    "\\end{eqnarray}\n",
    "$$\n",
    "\n",
    "To visualize what is going on, we can imagine plotting these variations in a 2D plot. \n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "03d9f016",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_06",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.6-runcell02\n",
    "\n",
    "ntoys=10000\n",
    "err1=np.array([])\n",
    "err2=np.array([])\n",
    "for i0 in range(ntoys):\n",
    "    pToy = np.random.normal(0,1,2) #==> Notice the ,2 at the rightmost point \n",
    "    err1 = np.append(pToy[0],err1)\n",
    "    err2 = np.append(pToy[1],err2)\n",
    "angle = np.linspace( 0 , 2 * np.pi , 150 ) \n",
    "\n",
    "#correct circle (radius sqrt(2))\n",
    "radius = 1*np.sqrt(2)\n",
    "x = radius * np.cos( angle ) \n",
    "y = radius * np.sin( angle ) \n",
    "#too large circle (radius 2)\n",
    "radius = 1*2.0\n",
    "x2 = radius * np.cos( angle ) \n",
    "y2 = radius * np.sin( angle ) \n",
    "\n",
    "plt.rcParams['figure.figsize'] = (6,6)\n",
    "plt.plot(err1,err2,\"p\")\n",
    "plt.plot(x,y,c='r')\n",
    "plt.plot(x2,y2,c='r',linestyle='dashed')\n",
    "plt.xlabel(\"$\\sigma_{1}$\")\n",
    "plt.ylabel(\"$\\sigma_{2}$\")\n",
    "plt.show()\n",
    "plt.rcParams['figure.figsize'] = (9,6)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b6c0903a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_06"
    ]
   },
   "source": [
    "Sampling two Gaussians gives us a circular distribution with a width given by the radius of the circle. This radius can be seen to be the standard deviation of $f(x)$ or the $\\sqrt{V[f(x)]}=\\sqrt{\\sigma_1^2+\\sigma_2^2}$. In other words, when sampling two independent variables, the variations add as their squares, as if they are two separate independent coordinates. This is often denoted as a \"Sum in Quadrature.\""
   ]
  },
  {
   "cell_type": "markdown",
   "id": "627f6f67",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_6'></a>     \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_6) | [Next Section](#section_2_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d0477604",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-2.6.1: Uncertainty in $f(x)$</span>\n",
    "\n",
    "If $f(x) = \\log(x)$, what is $\\sigma_{f(x)}$ in terms of $x$ and $\\sigma_{x}$? Express your answer in terms of `x` and `sigma_x` for $\\sigma$.\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "81b536db",
   "metadata": {
    "tags": [
     "solution",
     "md"
    ]
   },
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "\n",
    "The derivation is shown here: $\\sigma_{f(x)} = \\frac{df}{dx} \\sigma_{x} = \\frac{1}{x}\\sigma_{x}$\n",
    "\n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd412324",
   "metadata": {
    "tags": [
     "md",
     "learner"
    ]
   },
   "source": [
    ">#### Follow-up 2.6.1a (ungraded)\n",
    ">  \n",
    ">Try computing this numerically and comparing to your analytic solution.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "895aea72",
   "metadata": {
    "tags": [
     "draft",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>EXERCISE: L2.6.1a\n",
    "# Use this cell for drafting your solution (if desired),\n",
    "# then enter your solution in the interactive problem online to be graded.\n",
    "\n",
    "ntries=100000\n",
    "mean=100\n",
    "sigma=5\n",
    "meas = np.full(ntries,mean) #The value 100, 1k times\n",
    "unc  = np.random.normal (0,sigma, ntries) #a randomly sampled value from a Gaussian with width 1 1k times\n",
    "meas = meas+unc # the value 100 now smeared with sigma=1\n",
    "\n",
    "def function(ix):#our function\n",
    "    return np.log(ix)\n",
    "\n",
    "outmeas = function(meas)\n",
    "_,_,_=normhist(outmeas)\n",
    "\n",
    "analytic_stdev = #YOUR CODE HERE\n",
    "\n",
    "print(\"Mean:\",outmeas.mean(),\"Stddeviation:\",outmeas.std())\n",
    "print(\"Predicted Mean:\",function(mean),\"Stddeviation:\",analytic_stdev)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "70193cd5",
   "metadata": {
    "tags": [
     "solution",
     "py"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>SOLUTION: L2.6.1a\n",
    "\n",
    "ntries=100000\n",
    "mean=100\n",
    "sigma=5\n",
    "meas = np.full(ntries,mean) #The value 100, 1k times\n",
    "unc  = np.random.normal (0,sigma, ntries) #a randomly sampled value from a Gaussian with width 1 1k times\n",
    "meas = meas+unc # the value 100 now smeared with sigma=1\n",
    "\n",
    "def function(ix):#our function\n",
    "    return np.log(ix)\n",
    "\n",
    "outmeas = function(meas)\n",
    "_,_,_=normhist(outmeas)\n",
    "\n",
    "analytic_stdev = (1./mean)*sigma\n",
    "\n",
    "print(\"Mean:\",outmeas.mean(),\"Stddeviation:\",outmeas.std())\n",
    "print(\"Predicted Mean:\",function(mean),\"Stddeviation:\",analytic_stdev)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "40b92fd5",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='section_2_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L2.7 Propagating Uncertainties</h2>     \n",
    "\n",
    "| [Top](#section_2_0) | [Previous Section](#section_2_6) | [Exercises](#exercises_2_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "18427974",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "<h3>A realistic example</h3>\n",
    "\n",
    "Very famously, there was an excess of events found at a certain point in a distribution measured by an experiment at the Tevatron collider in Fermilab. This excess caused a lot of excitement. However many people were skeptical. <a href=\"https://www.science20.com/quantum_diaries_survivor/no_jetjet_bump_new_cdf_diboson_analysis-123327\" target=\"_blank\">HERE</a> is a full description of what was going on. In brief, the two plots shown below summarize the evidence that people thought showed a bump indicating the existence of a new particle:\n",
    "\n",
    "<img alt=\"Fig 2.7.1: excess of events at the Tevatron collider\" src=\"http://www.pd.infn.it/~dorigo/wjjcdf73fb.jpg\" width=\"700\"/>\n",
    "\n",
    ">source:  https://arxiv.org/pdf/1104.0699.pdf<br>\n",
    ">attribution: CDF Collaboration, arXiv:1104.0699v2\n",
    "\n",
    "The plots show the number of events as a function of the so-called \"invariant mass\" of a pairs of jets. The term \"jets\" refers to a cluster of particles all emitted in very close to the same direction. When two such jets are observed, one can calculate the mass of a hypothetical very short lifetime particle which could have decayed to produce the two observed jets. This mass needs to be corrected for relativistic effects to determine its value when that particle was at rest, hence the term \"invariant mass\".\n",
    "\n",
    "In both plots, the black points represent a histogram of the data. The filled in areas in the left plot are a histogram summing up the simulations of all the other physics processes that we know are occurring. The different fill colors represent each individual prediction. Finally, on the right, we subtract the solid distribution from the data. However, notice that the \"WW+WZ\" process shown in red in the left plot has not been subtracted. That contribution is shown as the red histogram in the right plot. The blue histogram on the right plot is a Gaussian fit to what appears to be a deviation in the data compared to the sum of all expected physics processes. \n",
    "\n",
    "The fact that this deviation corresponds to a bump makes us think this is a new particle. The problem with this bump is that it's a bump on top of a steeply falling distribution. You need to look very closely at the left plot to see this same blue \"bump\" contribution. So, what would happen to this comparison if our predicted distribution was shifted to the right by a little bit. What would the effect be on the appearance of the bump?\n",
    "\n",
    "To see this, let's open a file with this data and try to shift it ourselves. Note that the following code subtracts *all* of the expected physics processes so that the second plot below does not show the red bump seen in the right plot above."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6628f11e",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.7-runcell01\n",
    "\n",
    "import csv\n",
    "import matplotlib.pyplot as plt\n",
    "import numpy as np\n",
    "import urllib.request\n",
    "\n",
    "#load the file\n",
    "def load(iName):\n",
    "    label=iName\n",
    "    datax=np.array([])\n",
    "    datay=np.array([])\n",
    "    datayerr=np.array([])\n",
    "    with open(label,'r') as csvfile:\n",
    "        plots = csv.reader(csvfile, delimiter=',')\n",
    "        for row in plots:\n",
    "            datax    = np.append(datax,float(row[0]))\n",
    "            datay    = np.append(datay,float(row[1]))\n",
    "            datayerr = np.append(datayerr,np.sqrt(float(row[1])))\n",
    "    return datax,datay,datayerr\n",
    "\n",
    "#compute the ratio between data and simulation\n",
    "def histratio(iydata,iyderr,iysim):\n",
    "    newydata=np.array([])\n",
    "    newyderr=np.array([])\n",
    "    for i0 in range(len(iysim)):\n",
    "        ynew=iydata[i0]/iysim[i0]\n",
    "        yner=iyderr[i0]/iysim[i0]\n",
    "        newydata=np.append(newydata,ynew)\n",
    "        newyderr=np.append(newyderr,yner)\n",
    "    return newydata,newyderr\n",
    "\n",
    "fig = plt.figure(figsize=(10.5, 9.5))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "datax,datay,datayerr=load(\"data/tmpdata.txt\")\n",
    "simx,simy,simyerr=load(\"data/tmpmc.txt\")\n",
    "plt.errorbar(datax,datay,yerr=datayerr,marker='.',c='black',linestyle = 'None')\n",
    "plt.plot    (datax,simy,drawstyle = 'steps-mid')\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "yrdata,yrderr=histratio(datay,datayerr,simy)\n",
    "ax.errorbar(datax,yrdata,yerr=yrderr,marker='.',c='black',linestyle = 'None')\n",
    "ax.axhline(1, c='red')\n",
    "ax.set_ylim(0.5,1.5)\n",
    "plt.xlabel(\"Mjj [GeV]\")\n",
    "plt.ylabel(\"Data/Simulation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "191c9137",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "We can define a shift in a histogram by just shuffling events in bins. This we do by \n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{align}\n",
    "f(x^{\\prime}) & =f(x-\\sigma)\\approx f(x)-\\frac{df}{dx}\\sigma\\\\\n",
    " & =f(x)-\\frac{f(x)-f(x-\\Delta x)}{\\Delta x}\\sigma\\\\\n",
    " & =f(x)\\left(1-\\frac{\\sigma}{\\Delta x}\\right)+f(x-\\Delta x)\\frac{\\sigma}{\\Delta x}\n",
    "\\end{align}\n",
    "$$\n",
    "\n",
    "which we can rewrite in terms of bin shifts with a fractional uncertainty of $\\textrm{s}=\\frac{\\sigma}{\\Delta x}$. This gives us\n",
    "\n",
    "\n",
    "$$\n",
    "\\begin{equation}\n",
    "\\rm{bin_{i}} = (1-s)\\rm{bin}_{i} + (s) \\rm{bin}_{i-1} \\\\\n",
    "f(x_{i}) = f(x_{i})(1-s)+f(x-\\Delta x)s\n",
    "\\end{equation}\n",
    "$$\n",
    "\n",
    "Such a shift could occur if there was an additional uncertainty in the invariant mass which had not been accounted for. Let's add this modification and see if a fractional shift can explain our deviation. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "589ad4b2",
   "metadata": {
    "tags": [
     "learner",
     "py",
     "lect_07",
     "learner_chopped"
    ]
   },
   "outputs": [],
   "source": [
    "#>>>RUN: L2.7-runcell02\n",
    "\n",
    "#Now let's shift the bins of the simulation by a fraction\n",
    "def shifthist(isunc,isimy):\n",
    "    newsimy=np.array([])\n",
    "    for i0 in range(len(isimy)):\n",
    "        ynew = isimy[i0]*(1-isunc)\n",
    "        if i0 > 1:\n",
    "            ynew = isimy[i0-1]*isunc + isimy[i0]*(1-isunc)\n",
    "        newsimy=np.append(newsimy,ynew)\n",
    "    return newsimy\n",
    "\n",
    "            \n",
    "fig = plt.figure(figsize=(10.5, 9.5))\n",
    "ax = fig.add_subplot(2,1,1)\n",
    "newsimy=shifthist(0.1,simy)\n",
    "plt.errorbar(datax,datay,yerr=datayerr,marker='.',c='black',linestyle = 'None')\n",
    "plt.plot    (datax,simy,drawstyle = 'steps-mid')\n",
    "plt.plot    (datax,newsimy,drawstyle = 'steps-mid')\n",
    "\n",
    "ax = fig.add_subplot(2,1,2)\n",
    "yrdata,yrderr=histratio(datay,datayerr,newsimy)\n",
    "ax.errorbar(datax,yrdata,yerr=yrderr,marker='.',c='black',linestyle = 'None')\n",
    "ax.axhline(1, c='red')\n",
    "ax.set_ylim(0.5,1.5)\n",
    "plt.xlabel(\"Mjj [GeV]\")\n",
    "plt.ylabel(\"Data/Simulation\")\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "32d1a831",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "lect_07"
    ]
   },
   "source": [
    "We can see that a fractional shift of merely half the bin size is sufficient to explain this effect. Do you believe that the bump is real? \n",
    "\n",
    "As an aside, think about how unintuitive it is that simply shifting the prediction slightly in the horizontal axis results in what looks like a very clear peak when comparing to the data. This is a good example of how subtle effects can often produce what looks otherwise like a very obvious discovery.\n",
    "\n",
    "If you are a big proponent of the existence of a peak, you could still see some evidence for a small excess in the data around 150. However, given that the vertical bars represent the independent statistical uncertainties, think about the probability that a few points will together deviate upward simply by chance."
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4f4dd2a",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    "<a name='exercises_2_7'></a>   \n",
    "\n",
    "| [Top](#section_2_0) | [Restart Section](#section_2_7) |\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b7fa6ba3",
   "metadata": {
    "tags": [
     "learner",
     "md",
     "learner_chopped"
    ]
   },
   "source": [
    ">#### Follow-up 2.7.1a (ungraded)\n",
    ">    \n",
    ">Examine the CDF data further. Try other fractional shifts. Does the resulting fit look better or worse (look at the residuals)?\n"
   ]
  }
 ],
 "metadata": {
  "celltoolbar": "Hide code",
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  },
  "latex_envs": {
   "LaTeX_envs_menu_present": true,
   "autoclose": false,
   "autocomplete": true,
   "bibliofile": "biblio.bib",
   "cite_by": "apalike",
   "current_citInitial": 1,
   "eqLabelWithNumbers": true,
   "eqNumInitial": 1,
   "hotkeys": {
    "equation": "Ctrl-E",
    "itemize": "Ctrl-I"
   },
   "labels_anchors": false,
   "latex_user_defs": false,
   "report_style_numbering": false,
   "user_envs_cfg": false
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
