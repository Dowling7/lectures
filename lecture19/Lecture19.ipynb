{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "b4997a0a",
   "metadata": {},
   "source": [
    "<!--start-block-->\n",
    "<hr style=\"height: 1px;\">\n",
    "<i>This code was authored by the 8.316 Course Team, Copyright 2023 MIT All Rights Reserved.</i>\n",
    "<hr style=\"height: 1px;\">\n",
    "<br>\n",
    "\n",
    "# LECTURE 19: Deep Learning Monte Carlo\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "100dd6f6",
   "metadata": {},
   "source": [
    "<a name='section_19_0'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.0 Overview of Learning Objectives</h2>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7cb9ddf5",
   "metadata": {},
   "source": [
    "While, we have been randomly sampling and building toys throughout this class. We have yet to devote a whole lecture to Monte Carlo Simulation. This is the lecture that you have been missing. In this lecture, we are going to build a Monte Carlo simulation critical for proton therapy. Then we are going to explore Machine Learning Monte Carlo strategies, and show how some of the biggest advances in Artificial Intelligence are changing the way we simulation scientific data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1b423792",
   "metadata": {},
   "outputs": [],
   "source": [
    "import imageio\n",
    "from PIL import Image\n",
    "\n",
    "import numpy as np\n",
    "import torch\n",
    "import torch.nn as nn\n",
    "import matplotlib.pyplot as plt\n",
    "import csv\n",
    "import math\n",
    "from scipy import optimize as opt \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1ac6f212",
   "metadata": {},
   "source": [
    "<h3>Setting Default Figure Parameters</h3>\n",
    "\n",
    "The following code cell sets default values for figure parameters.\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "6e40a219",
   "metadata": {},
   "outputs": [],
   "source": [
    "#>>>RUN: L10.0-runcell02\n",
    "\n",
    "#set plot resolution\n",
    "%config InlineBackend.figure_format = 'retina'\n",
    "\n",
    "#set default figure parameters\n",
    "plt.rcParams['figure.figsize'] = (9,6)\n",
    "\n",
    "medium_size = 12\n",
    "large_size = 15\n",
    "\n",
    "plt.rc('font', size=medium_size)          # default text sizes\n",
    "plt.rc('xtick', labelsize=medium_size)    # xtick labels\n",
    "plt.rc('ytick', labelsize=medium_size)    # ytick labels\n",
    "plt.rc('legend', fontsize=medium_size)    # legend\n",
    "plt.rc('axes', titlesize=large_size)      # axes title\n",
    "plt.rc('axes', labelsize=large_size)      # x and y labels\n",
    "plt.rc('figure', titlesize=large_size)    # figure title"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea30879b",
   "metadata": {},
   "source": [
    "<a name='section_18_4'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.1 Modelling Physics Observables : Bragg Scattering for Proton Therapy </h2>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "70caebc8",
   "metadata": {},
   "source": [
    "There are a broad range of physics processes that we can use Monte Carlo to simulate. They typically involve scenarios where we don't have a good understanding of the analytic for of the whole system, but we understand how it works. \n",
    "\n",
    "We can embed all of our knowledge into understanding how the system works into a simulation. A simulation that relies on sampling of events, and modelling the effects in an event by event way is deemed a Monte-Carlo simulation, there are a huge number of Monte Carlo simulations. \n",
    "\n",
    "Lets start by simulating a particle passing through matter. This is one of the most common simulations, because we understand all the different sub processes well, but when we put them all together they are rather complicated.\n",
    "\n",
    "We will do this in the context of proton therapy data. Proton therapy is a way to treat cancerous cells through a non invasive approach whereby we fire energetic proton beams to various body parts. These beams are precisely controled to deliver a dose of radiation at just the right spot. Let's go ahead and do a simple simulation. \n",
    "\n",
    "What we are going to do first, is model the energy loss in matter, given by \n",
    "\n",
    "$$\n",
    "-\\frac{dE}{dx} = \\frac{4\\pi nz^2}{m_{e}c^{2}\\beta^{2}}\\left(\\frac{e^{2}}{4\\pi\\epsilon_{0}}\\right)^{2}\\left(\\log\\left(\\frac{2\\gamma^{2}m_{e}c^{2}\\beta^{2}T_{\\rm max}}{I(1-\\beta)^{2}}\\right)-\\beta^{2}-\\frac{\\delta(\\beta\\gamma)}{2}\\right)\n",
    "$$\n",
    "\n",
    "Now to get this to work completely we need to compute the Ionization energy. The ionization energy is stored in a bunch of places. In particular, you can find it in [figure 34.5](https://pdg.lbl.gov/2022/reviews/rpp2022-rev-passage-particles-matter.pdf), this depends on the nuclear stucture, but is roughly flat as a function of the nuclear charge as you get to higher elements. For our purposes, we will will just parameterize this as as table. \n",
    "\n",
    "In addition, we also need the atomic masses $A$ of all the objects, we can again take this from an online source. For people familiar, we are taking this from the particle simulation code Geant in Fortran. Let's plot them below. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2ac201a3",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "#values\n",
    "def I(iZ,iPlot=False):\n",
    "    #https://github.com/nrc-cnrc/EGSnrc/blob/master/HEN_HOUSE/pegs4/pegs4.mortran#L1354-L1391\n",
    "    lI=[19.2,41.8,40.,63.7,76.0,78.0,82.0,95.0,115.,137.,\n",
    "     149.,156.,166.,173.,173.,180.,174.,188.,190.,191.,216.,233.,245.,\n",
    "     257.,272.,286.,297.,311.,322.,330.,334.,350.,347.,348.,357.,352.,\n",
    "     363.,366.,379.,393.,417.,424.,428.,441.,449.,470.,470.,469.,488.,\n",
    "     488.,487.,485.,491.,482.,488.,491.,501.,523.,535.,546.,560.,574.,\n",
    "     580.,591.,614.,628.,650.,658.,674.,684.,694.,705.,718.,727.,736.,\n",
    "     746.,757.,790.,790.,800.,810.,823.,823.,830.,825.,794.,827.,826.,\n",
    "     841.,847.,878.,890.,902.,921.,934.,939.,952.,966.,980.,994.]\n",
    "    lZ=np.arange(1,len(lI)+1)\n",
    "    if iPlot:\n",
    "        plt.plot(lZ,lI/lZ)\n",
    "        plt.xlabel('Z')\n",
    "        plt.ylabel('I$_{adj}$/Z (eV/Z)')\n",
    "        plt.show()\n",
    "    return lI[iZ]*1e-6 #MeV not eV\n",
    "def A(iZ,iPlot=False):\n",
    "    #https://github.com/nrc-cnrc/EGSnrc/blob/master/HEN_HOUSE/pegs4/pegs4.mortran#L1354-L1391\n",
    "    lA=[1.00797,4.0026,6.939,9.0122,10.811,12.01115,14.0067,\n",
    "     15.9994,18.9984,20.183,22.9898,24.312,26.9815,28.088,30.9738,\n",
    "     32.064,35.453,39.948,39.102,40.08,44.956,47.90,50.942,51.998,\n",
    "     54.9380,55.847,58.9332,58.71,63.54,65.37,69.72,72.59,74.9216,\n",
    "     78.96,79.808,83.80,85.47,87.62,88.905,91.22,92.906,95.94,99.0,\n",
    "     101.07,102.905,106.4,107.87,112.4,114.82,118.69,121.75,127.60,\n",
    "     126.9044,131.30,132.905,137.34,138.91,\n",
    "     140.12,140.907,144.24,147.,150.35,151.98,157.25,158.924,162.50,\n",
    "     164.930,167.26,168.934,173.04,174.97,178.49,180.948,183.85,\n",
    "     186.2,190.2,192.2,195.08,196.987,200.59,204.37,207.19,208.980,\n",
    "     210.,210.,222.,223.,226.,227.,232.036,231.,238.03,237.,242.,\n",
    "     243.,247.,247.,248.,254.,253.   \n",
    "    ]\n",
    "    lZ=np.arange(1,len(lA)+1)\n",
    "    if iPlot:\n",
    "        plt.plot(lZ,lA/lZ)\n",
    "        plt.xlabel('Z')\n",
    "        plt.ylabel('A/Z (Atomic mass/Z)')\n",
    "        plt.show()\n",
    "    return lA[iZ-1]\n",
    "lItmp=I(1,True)\n",
    "lItmp=A(1,True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7d5195e9",
   "metadata": {},
   "source": [
    "Now we have all the elements to compute the Bethe-Bloch formula, and look at the charged particle energy loss over distance. For this part, we are going to focus on protons. However, this generally applies to a lot of different phenomena.  \n",
    "\n",
    "Additionally, we will plot this for two atomic elements. Since we care about the human body, we will plot this for water (Oxygen) and just to show something heavy thats in a human (calcium). See this [page](https://en.wikipedia.org/wiki/Composition_of_the_human_body)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fb55e1f5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://indico.cern.ch/event/753612/contributions/3121551/attachments/1974578/3285956/MC_2019.pdf\n",
    "#https://www.nature.com/articles/s41598-017-10554-0\n",
    "\n",
    "m_e = 0.511 # Mass of electron in MeV\n",
    "\n",
    "def gamma(ip,im): #E^2=gamma^2m^2=p^2+m^2\n",
    "    return np.sqrt(1+(ip/im)**2)\n",
    "\n",
    "def beta(ip,im): #gamma=1/sqrt(1-b^2)\n",
    "    g=gamma(ip,im)\n",
    "    return np.sqrt(1-1./g**2)\n",
    "\n",
    "def betagamma(ip,im):#p=bgm\n",
    "    return ip/im\n",
    "\n",
    "def Tmax(ip,im): # Maximum energy transfer in one collision in MeV\n",
    "    return 2*m_e*(ip/im)**2/(1+2*gamma(ip,im)*m_e/im+(m_e/im)**2)\n",
    "\n",
    "def TKinheavy(ip,im): #(T+M)^2=sqrt(p)+sqrt(m)\n",
    "    return np.sqrt(np.sqrt(ip)+np.sqrt(um))-im\n",
    "\n",
    "def delta(ip,im):\n",
    "    C = 4.44\n",
    "    a = 0.1492\n",
    "    m = 3.25\n",
    "    X1 = 2.87\n",
    "    X0 = 0.2014\n",
    "    delta0 = 0.14\n",
    "    x = np.log10(ip/im)\n",
    "    #f1 = lambda x: delta0 * 10**(2*(x-X0)) # conductors pdg\n",
    "    f2 = 2 * x * np.log(10) - C + (a * ((X1 - x)**m))\n",
    "    f3 = 2 * x * np.log(10) - C\n",
    "    delta_full = np.where(x < X0, 0, f2)\n",
    "    delta_full = np.where(x < X1 ,delta_full, f3)\n",
    "    return delta_full\n",
    "        \n",
    "def dEdxF(ip,im,iZ,zpart=1,rho=1.0,nodelta=False): #Bethe-Bloch equation\n",
    "    K = 0.307075 # constant K in MeV cm mol^-1\n",
    "    #rho = 2.336 # Density of material in g cm^-3 (here: silicon density)\n",
    "    const   = zpart**2 * (K * rho * iZ ) / (2 * A(iZ)) * (1./beta(ip,im)**2)\n",
    "    logterm = 2 * m_e * Tmax(ip,im) * ((ip/im)**2)/(I(iZ)**2) \n",
    "    dEdxV   =  const * (np.log(logterm)  - 2*(beta(ip,im))**2 - delta(ip,im))              \n",
    "    if nodelta:\n",
    "        print(\"delta:\",delta(ip,im),dEdxV)\n",
    "        dEdxV    =  const * (np.log(logterm) - 2*(beta(ip,im))**2)\n",
    "    return dEdxV\n",
    "    \n",
    "mproton=938\n",
    "mpion=135.4\n",
    "mmuon=105.4\n",
    "print(gamma(100,mproton),beta(100,mproton))\n",
    "p=np.arange(15,1000000,10)\n",
    "dEdxOut1p = dEdxF(p,mproton,8,1)\n",
    "dEdxOut2p = dEdxF(p,mproton,18,1)\n",
    "dEdxOut1pi = dEdxF(p,mpion,8,1)\n",
    "dEdxOut2pi = dEdxF(p,mpion,18,1)\n",
    "dEdxOut1mu = dEdxF(p,mmuon,8,1)\n",
    "dEdxOut2mu = dEdxF(p,mmuon,18,1)\n",
    "\n",
    "plt.plot(p,dEdxOut1p,label=\"Proton\")\n",
    "plt.plot(p,dEdxOut2p,label=\"Proton(Calcium)\")\n",
    "\n",
    "plt.plot(p,dEdxOut1pi,label=\"Pion\")\n",
    "plt.plot(p,dEdxOut2pi,label=\"Pion(Calcium)\")\n",
    "\n",
    "plt.plot(p,dEdxOut1mu,label=\"Muon\")\n",
    "plt.plot(p,dEdxOut2mu,label=\"Muon(Calcium)\")\n",
    "\n",
    "plt.xlabel('Momentum (MeV)')\n",
    "plt.ylabel('dE/dx (MeV/cm) (rho=1 g/cm$^{3}$)')\n",
    "plt.xscale('log')\n",
    "plt.yscale('log')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "beta(935,mproton)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "515332b2",
   "metadata": {},
   "source": [
    "Now, we can run a basic simulation of this already by considering a particle of a specific energy and then stepping it down by the energy loss. We could in fact do this analytically, but using the above formula makes it particulary nice and elegant. Let's go ahead and run our simulation for our proton energy beam. \n",
    "\n",
    "An important point to remember is that energy and momentum differ. In otherwords a 350 MeV proton will give a differnt momentum. Note that when we say a 350 MeV proton. We actually mean the kinetic energy is 350 MeV. This means our momentum is given by\n",
    "\n",
    "$$\n",
    "p^2 + m^2 = E_{\\rm kin}^2 + m^2 \\\\\n",
    "p^2 = (E_{kin} + m)^2 - m^2\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "37c169b4",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def dP(dE,ip,im): #solving \n",
    "    #dp = ip - np.sqrt(dE**2+ip**2-2*dE*np.sqrt(ip**2+im**2))\n",
    "    #E=p^2/2m=> p=\\sqrt(2mE)=>dp=sqrt(2m)/sqrt(E) dE\n",
    "    #return dE*(np.sqrt(ip**2+im**2)/ip)*gamma(ip,im)\n",
    "    return dE#*(ip/im)\n",
    "\n",
    "def eToP(iE,im):\n",
    "    return np.sqrt((iE+im)**2-im**2)\n",
    "\n",
    "def sim(ie=500,im=935,idt=1e-11,iZ=8):\n",
    "    xstep  = np.array([])\n",
    "    estep  = np.array([])\n",
    "    pstep  = np.array([])\n",
    "    c=3e10\n",
    "    dist=0\n",
    "    e=ie\n",
    "    while e > 5:\n",
    "        p = eToP(e,im)\n",
    "        dEdxS  = dEdxF(p,im,iZ=iZ,rho=1.06)\n",
    "        #print(dEdxS)\n",
    "        dx     = beta(p,im)*c*idt#speed of light\n",
    "        #print(dEdxS,dP(dEdxS*dx,p,im))\n",
    "        e      -= dEdxS*dx\n",
    "        dist   += dx\n",
    "        xstep  = np.append(xstep,dist)\n",
    "        estep  = np.append(estep,dEdxS*dx)\n",
    "        pstep  = np.append(pstep,e)\n",
    "    return xstep,pstep,estep\n",
    "    \n",
    "print(\"350 MeV Proton Momemtum:\",eToP(350,mproton))\n",
    "xstep150,pstep150,estep150 = sim(ie=150,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep200,pstep200,estep200 = sim(ie=200,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep250,pstep250,estep250 = sim(ie=250,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep300,pstep300,estep300 = sim(ie=300,im=mproton,idt=1e-11,iZ=8)\n",
    "\n",
    "plt.plot(xstep150,pstep150,label='150 MeV')\n",
    "plt.plot(xstep200,pstep200,label='200 MeV')\n",
    "plt.plot(xstep250,pstep250,label='250 MeV')\n",
    "plt.plot(xstep300,pstep300,label='300 MeV')\n",
    "plt.xlabel('Distance(cm)')\n",
    "plt.ylabel('Momentum(MeV)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(xstep150,estep150,label='150 MeV')\n",
    "plt.plot(xstep200,estep200,label='200 MeV')\n",
    "plt.plot(xstep250,estep250,label='250 MeV')\n",
    "plt.plot(xstep300,estep300,label='300 MeV')\n",
    "plt.xlabel('Distance(cm)')\n",
    "plt.ylabel('E-deposit(MeV/mm)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "#sim(60,dt=1e-12)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43f8337b",
   "metadata": {},
   "source": [
    "Now what you can see above is that we get a very sharp energy deposit at a very specific region. As a result, we can use proton beams to deposit a huge amount of energy in the body for cancer thereapy. This peak is known as the Bragg peak. You can see that various energy beams get us various ranges in the body. Let's go one step further and make our simulation even more realistic.  \n",
    "\n",
    "There are three ways to do this: \n",
    "1. Sample the full energy profile interaction as we move along the decay \n",
    "2. Smear the initial beam energy by the input resolution\n",
    "3. Add other effects, namely multiple scattering. \n",
    "\n",
    "Let's go ahead and start with the first since this is often the most illustrative.\n",
    "\n",
    "Energy loss is actually governed by a Landau distribution. What that means is that the energy depositied is not a fixed amount per step, but is distribution. The full distribution follows the Landau form given by \n",
    "\n",
    "$$\n",
    "p(x) = \\frac{1}{\\pi}\\int_{0}^{\\infty} e^{-t \\log(t)-xt} \\sin(\\pi t) dt\n",
    "$$\n",
    "\n",
    "To model energy loss exactly, what we need to do is sample a landau distribution at every step and apply the sampled value to our calculation of $\\frac{dE}{dx}$. Let's go ahead and plot the Landau and come up with a scheme to sample it. \n",
    "\n",
    "We will take a resolution of 1.5 percent following [here](https://indico.cern.ch/event/654712/contributions/2666034/attachments/1531773/2397743/SJ_STFCDetectors_ProCal_25-09-17.pdf). Additionally, we will stop our proton beam once we have < 15 MeV. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bd4f90b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import pylandau\n",
    "from landaupy import landau\n",
    "\n",
    "def landauMPV(ip,im,iZ,irho=1,zpart=1):\n",
    "    K = 0.307075 # constant K in MeV cm mol^-1\n",
    "    const   = zpart**2 * (K * irho * iZ ) / (2 * A(iZ)) * (1./beta(ip,im)**2)\n",
    "    #logterm  = 2 * m_e * Tmax(ip,im) * ((ip/im)**2)/(I(iZ)**2) \n",
    "    logterm1 = 2 * m_e *               ((ip/im)**2)/(I(iZ)) \n",
    "    logterm2 = const/I(iZ)\n",
    "    dEdxV    =  const * (np.log(logterm1) + np.log(logterm2) + 0.2     - (beta(ip,im))**2 - delta(ip,im))       # \n",
    "    return dEdxV,const\n",
    "\n",
    "def plotLandau(ip,im,idx,iZ=14,irho=1,zpart=1):\n",
    "    lP=eToP(ip,im)\n",
    "    lMPV,lWMPV = landauMPV(lP,im,iZ,irho,zpart)\n",
    "    lMPV*=idx; lWMPV*=idx\n",
    "    x=np.arange(0,15.5,0.01)\n",
    "    landpy=landau.pdf(x,lMPV,lWMPV)\n",
    "    return x,landpy\n",
    "\n",
    "x,landZ150=plotLandau(150,mproton,1.0,iZ=8,irho=1.0)\n",
    "x,landZ200=plotLandau(200,mproton,1.0,iZ=8,irho=1.0)\n",
    "x,landZ250=plotLandau(250,mproton,1.0,iZ=8,irho=1.0)\n",
    "x,landZ300=plotLandau(300,mproton,1.0,iZ=8,irho=1.0)\n",
    "\n",
    "plt.plot(x,landZ150,label='E=150 MeV')\n",
    "plt.plot(x,landZ200,label='E=200 MeV')\n",
    "plt.plot(x,landZ250,label='E=250 MeV')\n",
    "plt.plot(x,landZ300,label='E=300 MeV')\n",
    "plt.xlabel(\"dE/dx (MeV/cm)/rho\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1339ae05",
   "metadata": {},
   "source": [
    "Now, what we can do to add some level of realisim as we move along our path of the proton going thorugh matter. Every time we step, we can sample the landau distribution pdf and step our energy loss. Given that we are sampling, it now makes sense to run this experiment a number of times. This now starts to give us some real Monte Carlo description.  \n",
    "\n",
    "Sampling complicated distributions like this can often be slow. This code was implemented [here](https://github.com/SengerM/landaupy/blob/main/landaupy/samplers.py#L42). The strategy is to randomly sample a number from 0 to 1, this a p-value, then you can get the x-value by inverting the CDF since we know that \n",
    "\n",
    "$$\n",
    "\\rm{cdf}(x)=\\int_{-\\infty}^{x} p(x^{\\prime}) dx^{\\prime} \n",
    "$$\n",
    "\n",
    "so we treat the cdf as a 1 to 1 map the y-axis is probabilty and teh x-axis is the pdf value integrated up to $x$. By inverting the cdf, treating this as a lookup table we can ivert the function so that ${\\rm cdf}^{-1}(y)=x$.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8fda3b27",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simSample(ie=500,im=935,idt=1e-11,iZ=8):\n",
    "    xstep  = np.array([])\n",
    "    estep  = np.array([])\n",
    "    pstep  = np.array([])\n",
    "    c=3e10\n",
    "    dist=0\n",
    "    e=ie\n",
    "    while e > 5:\n",
    "        p = eToP(e,im)\n",
    "        lMPV,lWMPV  = landauMPV(p,im,iZ=iZ,irho=1.06)\n",
    "        dE     = landau.sample(lMPV, lWMPV, 1)\n",
    "        dx     = beta(p,im)*c*idt#speed of light\n",
    "        e      -= dE*dx\n",
    "        dist   += dx\n",
    "        xstep  = np.append(xstep,dist)\n",
    "        estep  = np.append(estep,dE*dx)\n",
    "        pstep  = np.append(pstep,e)\n",
    "    return xstep,pstep,estep\n",
    "\n",
    "xstep150,pstep150,estep150 = simSample(ie=150,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep200,pstep200,estep200 = simSample(ie=200,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep250,pstep250,estep250 = simSample(ie=250,im=mproton,idt=1e-11,iZ=8)\n",
    "xstep300,pstep300,estep300 = simSample(ie=300,im=mproton,idt=1e-11,iZ=8)\n",
    "\n",
    "plt.plot(xstep150,pstep150,label='150 MeV')\n",
    "plt.plot(xstep200,pstep200,label='200 MeV')\n",
    "plt.plot(xstep250,pstep250,label='250 MeV')\n",
    "plt.plot(xstep300,pstep300,label='300 MeV')\n",
    "plt.xlabel('Distance(cm)')\n",
    "plt.ylabel('Momentum(MeV)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "plt.plot(xstep150,estep150,label='150 MeV')\n",
    "plt.plot(xstep200,estep200,label='200 MeV')\n",
    "plt.plot(xstep250,estep250,label='250 MeV')\n",
    "plt.plot(xstep300,estep300,label='300 MeV')\n",
    "plt.xlabel('Distance(cm)')\n",
    "plt.ylabel('E-deposit(MeV/mm)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e1a7c352",
   "metadata": {},
   "source": [
    "Ok so now lets step through this guy and look at the distribution of energy deposited in our system. What we will do is our simulation above 100 times and see how much the energy deposit varies, and how much the length of propagation varies.\n",
    "\n",
    "For this, we will define a few observables, lets look at the energy deposit in the last 3cm. Also, lets look at the energy deposit in the first 3 cm, for comparison. Also, we can look at the after distance the system went. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8b5fb56d",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def observables(iXArr,iPArr,iEArr):\n",
    "    lX=iXArr[-1]\n",
    "    dEEnd=np.sum(iEArr[(iXArr > iXArr[-1]-3)])\n",
    "    dEFrt=np.sum(iEArr[(iXArr < 3)])\n",
    "    return lX,dEEnd,dEFrt  \n",
    "    \n",
    "def simNSamples(ie=150,im=mproton,iN=100,idt=1e-11,iZ=8):\n",
    "    pXArr = np.array([])\n",
    "    pdEBArr = np.array([])\n",
    "    pdEFArr = np.array([])\n",
    "    for i0 in range(iN):\n",
    "        if i0 % 25 == 0:\n",
    "            print(i0)\n",
    "        pXstep,pPstep,pEstep = simSample(ie=ie,im=im,idt=idt,iZ=iZ)\n",
    "        pX,pdEEnd,pdEFrt = observables(pXstep,pPstep,pEstep)\n",
    "        pXArr   = np.append(pXArr,  pXstep)\n",
    "        pdEBArr = np.append(pdEBArr,pdEEnd)\n",
    "        pdEFArr = np.append(pdEFArr,pdEFrt)\n",
    "\n",
    "    plt.hist(pX,label='Distance')\n",
    "    plt.xlabel('Distance')\n",
    "    plt.ylabel('N')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(pdEBArr,label='dE/dx Back')\n",
    "    plt.xlabel('E-Deposit Back')\n",
    "    plt.ylabel('N)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    plt.hist(pdEFArr,label='dE/dx Front')\n",
    "    plt.xlabel('E-Deposit Front')\n",
    "    plt.ylabel('N)')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "#plt.plot(xstep150,estep150,label='150 MeV')\n",
    "#plt.plot(xstep200,estep200,label='200 MeV')\n",
    "#plt.plot(xstep250,estep250,label='250 MeV')\n",
    "#plt.plot(xstep300,estep300,label='300 MeV')\n",
    "#plt.xlabel('Distance(cm)')\n",
    "#plt.ylabel('E-deposit(MeV/mm)')\n",
    "#plt.legend()\n",
    "#plt.show()\n",
    "\n",
    "simNSamples()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d07c353b",
   "metadata": {},
   "source": [
    "Now that was kind of slow can we speed up this process? \n",
    "\n",
    "The way we will do this is we will step through 500 simulations all the same time in parallel. The one thing that is tricky is that parallel support for sampling the Landau is not supported, so we will have to add a for loop to do that bit. Anyway, otherwise, we can run everything else in parallel. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d85fa1c0",
   "metadata": {},
   "outputs": [],
   "source": [
    "def simNParallelSample(iN, ie=500,im=935,idt=1e-10,iZ=8):\n",
    "    xstep  = np.empty((0,iN))\n",
    "    estep  = np.empty((0,iN))\n",
    "    pstep  = np.empty((0,iN))\n",
    "    c=3e10\n",
    "    dist=np.zeros(iN)\n",
    "    e=np.ones(iN)*ie\n",
    "    print(\"Scanning:\",ie)\n",
    "    while np.any(e > 5):\n",
    "        p = eToP(e,im)\n",
    "        lMPV,lWMPV  = landauMPV(p,im,iZ=iZ,irho=1.06)\n",
    "        dE = np.zeros(lMPV.shape)\n",
    "        ##Here we have to parallelize by hand, this is not good\n",
    "        for i0, (pMPV,pWMPV) in enumerate(zip(lMPV,lWMPV)):\n",
    "            dE[i0]     = landau.sample(pMPV, pWMPV,1)\n",
    "        dx     = beta(p,im)*c*idt#speed of light\n",
    "        pdEdX  = np.minimum(dE*dx,e-0.1)\n",
    "        e      -= pdEdX\n",
    "        dist   += dx\n",
    "        xstep  = np.vstack((xstep,dist))\n",
    "        estep  = np.vstack((estep,pdEdX))\n",
    "        pstep  = np.vstack((pstep,e))\n",
    "    xstep = xstep.T\n",
    "    estep = estep.T\n",
    "    pstep = pstep.T\n",
    "    return xstep,pstep,estep\n",
    "\n",
    "def simNSamples(ie=100,im=mproton,iN=500,idt=5e-11,iZ=8):\n",
    "    xstep,pstep,estep=simNParallelSample(iN,ie=ie,im=im,idt=idt,iZ=iZ)\n",
    "    plt.hist(xstep[:,-1],alpha=0.5)\n",
    "    plt.show()\n",
    "    efront=np.zeros(xstep.shape[0])\n",
    "    eback =np.zeros(xstep.shape[0])\n",
    "    for i0 in range(xstep.shape[0]):\n",
    "        efront[i0] = np.sum(estep[i0,xstep[i0] < 3])/3.\n",
    "        eback[i0]  = np.sum(estep[i0,xstep[i0] > xstep[i0]-3])/3.\n",
    "    xrange=np.arange(0,150,2.5)\n",
    "    _,bins,_=plt.hist(efront,bins=xrange,alpha=0.5,label='dE/dx Front')\n",
    "    plt.hist(eback, bins=bins,alpha=0.5,label='dE/dx Back')\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "def sumEstep(estep,xstep):\n",
    "    efront=np.zeros(xstep.shape[0])\n",
    "    eback =np.zeros(xstep.shape[0])\n",
    "    for i0 in range(xstep.shape[0]):\n",
    "        efront[i0] = np.sum(estep[i0,xstep[i0] < 3])/3.\n",
    "        #print(xstep[i0] < 3,xstep[i0] > xstep[i0,-1]-3,xstep[i0,-1]-3,xstep[i0],estep[i0])\n",
    "        eback[i0]  = np.sum(estep[i0,xstep[i0] > xstep[i0,-1]-3])/3.\n",
    "    return efront,eback\n",
    "\n",
    "xstep150,pstep150,estep150=simNParallelSample(ie=150,im=mproton,iN=500,idt=1e-10,iZ=8)\n",
    "xstep200,pstep200,estep200=simNParallelSample(ie=200,im=mproton,iN=500,idt=1e-10,iZ=8)\n",
    "xstep250,pstep250,estep250=simNParallelSample(ie=250,im=mproton,iN=500,idt=1e-10,iZ=8)\n",
    "xstep300,pstep300,estep300=simNParallelSample(ie=300,im=mproton,iN=500,idt=1e-10,iZ=8)\n",
    "\n",
    "plt.hist(xstep150[:,-1],alpha=0.5,label='E=150 MeV')\n",
    "plt.hist(xstep200[:,-1],alpha=0.5,label='E=200 MeV')\n",
    "plt.hist(xstep250[:,-1],alpha=0.5,label='E=250 MeV')\n",
    "plt.hist(xstep300[:,-1],alpha=0.5,label='E=300 MeV')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ef150,eb150=sumEstep(estep150,xstep150)\n",
    "ef200,eb200=sumEstep(estep200,xstep200)\n",
    "ef250,eb250=sumEstep(estep250,xstep250)\n",
    "ef300,eb300=sumEstep(estep300,xstep300)\n",
    "\n",
    "xrange=np.arange(0,10,0.25)\n",
    "plt.hist(ef150,bins=xrange,alpha=0.5,label='E=150 MeV (dE/dx Front)')\n",
    "plt.hist(ef200,bins=xrange,alpha=0.5,label='E=200 MeV (dE/dx Front)')\n",
    "plt.hist(ef250,bins=xrange,alpha=0.5,label='E=250 MeV (dE/dx Front)')\n",
    "plt.hist(ef300,bins=xrange,alpha=0.5,label='E=300 MeV (dE/dx Front)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "xrange=np.arange(0,60,2)\n",
    "plt.hist(eb150,bins=xrange,alpha=0.5,label='E=150 MeV (dE/dx Back)')\n",
    "plt.hist(eb200,bins=xrange,alpha=0.5,label='E=200 MeV (dE/dx Back)')\n",
    "plt.hist(eb250,bins=xrange,alpha=0.5,label='E=250 MeV (dE/dx Back)')\n",
    "plt.hist(eb300,bins=xrange,alpha=0.5,label='E=300 MeV (dE/dx Back)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "17dd5b4d",
   "metadata": {},
   "source": [
    "From these distributions, we can start to make interesting conclusions. The higher energy beams tend to go longer, deposit less in the front, but the same as everybody else in the back. Notice also the 150 MeV feature. This Monte Carlo, we have developed is pretty sophisticated, but we can do even better if we take in mind that our beam is a full 3D distribution.  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "43e1bdf9",
   "metadata": {},
   "source": [
    "\n",
    "<a name='section_18_5'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.2 Modelling Physics Observables : Bragg Scattering for Proton Therapy in multiple dimensions </h2>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813667ee",
   "metadata": {},
   "source": [
    "\n",
    "Additionally, as the proton beam moves through the body. In addition to losing energy, it will scatter in different directions. The scatter occurs from the fact that the proton is charged, and the nuclei are charged and so we have coulomb scattering (aka Rutherford scattering) of the proton against the nuclei. This is an elastic process that will inevitably spread the beam further out. The scattering angle is often written as: \n",
    "\n",
    "$$\n",
    "\\sigma_{\\theta} = \\frac{13.6 \\rm MeV}{\\beta cp} z \\sqrt{\\frac{x}{X_{0}}}\\left(1+0.038\\log\\left(\\frac{xz^{2}}{X_{0}\\beta^{2}}\\right)\\right)\n",
    "$$\n",
    "\n",
    "The actual scatter has been studied to give a parameteriztion split in two terms\n",
    "\n",
    "$$\n",
    "\\sigma_{\\Delta y} = z_{1} \\Delta x \\frac{\\sigma_{\\theta}}{\\sqrt{12}} + \\frac{z_{2} \\Delta x\\sigma_{\\theta}}{2}\\\\\n",
    "\\sigma_{\\Delta \\theta}     = z_{2} \\sigma_{\\theta}\n",
    "$$\n",
    "Where we have $z_{1}$ and $z_{2}$ are two normal random sampled distributions that are idependent of one another."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0e5830b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def X0(iZ):\n",
    "    const=(716.408**-1)/A(iZ)\n",
    "    a = iZ/137.\n",
    "    Lrad =np.log(184.15*iZ**(-1./3.))\n",
    "    Lradp=np.log(1194*iZ**(-2./3.))\n",
    "    fZ = a**2*((1+a**2)**(-1)+0.20206-0.0369*a**2+0.0083*a**4-0.002*a**6)\n",
    "    val=const*(iZ**2*(Lrad-fZ)+iZ*Lradp)\n",
    "    return 1./val\n",
    "    \n",
    "def sigmaTheta(ip,im,iX0,idx=1.0,zpart=1):\n",
    "    C=13.6\n",
    "    X0=iX0\n",
    "    dx=idx/iX0\n",
    "    const=C/(beta(ip,im)*ip)*zpart*np.sqrt(dx)\n",
    "    logterm=1+0.038*np.log(dx*zpart**2/beta(ip,im)**2)\n",
    "    return const*logterm\n",
    "\n",
    "def thetaScatter(ip,im,iX0,idx,zpart=1):\n",
    "    z1=np.random.normal(0,1,ip.shape[0])\n",
    "    z2=np.random.normal(0,1,ip.shape[0])\n",
    "    stheta=sigmaTheta(ip,im,iX0,zpart)\n",
    "    dy    =z1*idx*stheta/np.sqrt(12.) + z2*idx*stheta/2 \n",
    "    dtheta=z2*stheta\n",
    "    return dtheta,dy\n",
    "\n",
    "lZ=np.arange(1,100,1)\n",
    "lX0=np.zeros(len(lZ))\n",
    "for pZ in lZ:\n",
    "    lX0[pZ-1] = X0(pZ)\n",
    "print(X0(82))\n",
    "plt.plot(lZ,lX0)\n",
    "plt.xlabel(\"Z\")\n",
    "plt.ylabel(\"$X_{0}$\")\n",
    "plt.ylim(0,100)\n",
    "plt.show()\n",
    "\n",
    "lP=np.arange(10,500,1)\n",
    "lST = sigmaTheta(lP,mproton,X0(8))\n",
    "plt.plot(lP,lST)\n",
    "plt.yscale('log')\n",
    "plt.xlabel('P (MeV)')\n",
    "plt.ylabel('$\\sigma_{\\Theta}$')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "479c2b6f",
   "metadata": {},
   "source": [
    "What we see above is that for very low energy we have very large angles when we get to the low momenta. Basically, the proton can go anywhere, this can blur out our beam, and if we are using this for cancer therapy can cause more damage. \n",
    "\n",
    "One important thing to consider is that we need to make sure our angular scatter is physical, that means we should make sure that our angles are ok. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0f777120",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def simNYParallelSample(iN, ie=500,im=935,idt=1e-10,iZ=8):\n",
    "    xstep  = np.empty((0,iN))\n",
    "    ystep  = np.empty((0,iN))\n",
    "    estep  = np.empty((0,iN))\n",
    "    pstep  = np.empty((0,iN))\n",
    "    theta=0\n",
    "    y=0\n",
    "    c=3e10\n",
    "    dist=np.zeros(iN)\n",
    "    e=np.ones(iN)*ie\n",
    "    lX0 = X0(iZ)\n",
    "    print(\"Scanning:\",ie)\n",
    "    while np.any(e > 5):\n",
    "        p = eToP(e,im)\n",
    "        lMPV,lWMPV  = landauMPV(p,im,iZ=iZ,irho=1.06)\n",
    "        dE = np.zeros(lMPV.shape)\n",
    "        ##Here we have to parallelize by hand, this is not good\n",
    "        for i0, (pMPV,pWMPV) in enumerate(zip(lMPV,lWMPV)):\n",
    "            dE[i0]     = landau.sample(pMPV, pWMPV,1)\n",
    "        dx     = beta(p,im)*c*idt#speed of light\n",
    "        dTheta,dy = thetaScatter(p,im,lX0,idx=dx,zpart=1)\n",
    "        pdEdX  = np.minimum(dE*dx,e-0.1)\n",
    "        e      -= pdEdX\n",
    "        dist   += dx*np.cos(theta)\n",
    "        y      += dy + np.sin(theta)*dx\n",
    "        theta  += dTheta\n",
    "        xstep  = np.vstack((xstep,dist))\n",
    "        ystep  = np.vstack((ystep,y))\n",
    "        estep  = np.vstack((estep,pdEdX))\n",
    "        pstep  = np.vstack((pstep,e))        \n",
    "    xstep = xstep.T\n",
    "    estep = estep.T\n",
    "    pstep = pstep.T\n",
    "    ystep = ystep.T\n",
    "    return xstep,pstep,estep,ystep\n",
    "\n",
    "xstep150,pstep150,estep150,ystep150=simNYParallelSample(ie=150,im=mproton,iN=1000,idt=1e-10,iZ=8)\n",
    "xstep200,pstep200,estep200,ystep200=simNYParallelSample(ie=200,im=mproton,iN=1000,idt=1e-10,iZ=8)\n",
    "xstep250,pstep250,estep250,ystep250=simNYParallelSample(ie=250,im=mproton,iN=1000,idt=1e-10,iZ=8)\n",
    "xstep300,pstep300,estep300,ystep300=simNYParallelSample(ie=300,im=mproton,iN=1000,idt=1e-10,iZ=8)\n",
    "\n",
    "xrange=np.arange(0,60,2)\n",
    "plt.hist(xstep150[:,-1],bins=xrange,alpha=0.5,label='E=150 MeV')\n",
    "plt.hist(xstep200[:,-1],bins=xrange,alpha=0.5,label='E=200 MeV')\n",
    "plt.hist(xstep250[:,-1],bins=xrange,alpha=0.5,label='E=250 MeV')\n",
    "plt.hist(xstep300[:,-1],bins=xrange,alpha=0.5,label='E=300 MeV')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "xrange=np.arange(-10,10,0.25)\n",
    "plt.hist(ystep150[:,-1],bins=xrange,alpha=0.5,label='E=150 MeV')\n",
    "plt.hist(ystep200[:,-1],bins=xrange,alpha=0.5,label='E=200 MeV')\n",
    "plt.hist(ystep250[:,-1],bins=xrange,alpha=0.5,label='E=250 MeV')\n",
    "plt.hist(ystep300[:,-1],bins=xrange,alpha=0.5,label='E=300 MeV')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "ef150,eb150=sumEstep(estep150,xstep150)\n",
    "ef200,eb200=sumEstep(estep200,xstep200)\n",
    "ef250,eb250=sumEstep(estep250,xstep250)\n",
    "ef300,eb300=sumEstep(estep300,xstep300)\n",
    "\n",
    "xrange=np.arange(0,15,0.5)\n",
    "plt.hist(ef150,bins=xrange,alpha=0.5,label='E=150 MeV (dE/dx Front)')\n",
    "plt.hist(ef200,bins=xrange,alpha=0.5,label='E=200 MeV (dE/dx Front)')\n",
    "plt.hist(ef250,bins=xrange,alpha=0.5,label='E=250 MeV (dE/dx Front)')\n",
    "plt.hist(ef300,bins=xrange,alpha=0.5,label='E=300 MeV (dE/dx Front)')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "xrange=np.arange(0,60,2)\n",
    "plt.hist(eb150,bins=xrange,alpha=0.5,label='E=150 MeV (dE/dx Back)')\n",
    "plt.hist(eb200,bins=xrange,alpha=0.5,label='E=200 MeV (dE/dx Back)')\n",
    "plt.hist(eb250,bins=xrange,alpha=0.5,label='E=250 MeV (dE/dx Back)')\n",
    "plt.hist(eb300,bins=xrange,alpha=0.5,label='E=300 MeV (dE/dx Back)')\n",
    "plt.legend()\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8d87aa09",
   "metadata": {},
   "source": [
    "## Constructing a likelihood"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5e690074",
   "metadata": {},
   "source": [
    "Now this is where things become really interesting. The simulation we made above models a very specific process, and the more and more elements of realism. However, this gets more complicated. That being said, we see in the end the distributions we model are fairly reasonable. \n",
    "\n",
    "What we can do is build a parameatric model for the behavior of what is going, this is often how we try to characterize things. This model is effectively how we built up our simulation. However, we injected realism now by sampling distributions as we moved along the way. \n",
    "\n",
    "The reality though is that a perfect likelihood is never perfect, and learning every feature becomes progressively harder as things become more complciated. In the following few sections, we are going to construct a likelihood using Deep Learning. Let's see how well it can do?\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "75155c5f",
   "metadata": {},
   "source": [
    "\n",
    "<a name='section_18_6'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.3 Interlude : Variational Autoencoders for Monte Carlo based Event Generation </h2>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9f51af1b",
   "metadata": {},
   "source": [
    "Before we go down the path of using variational autencoders for simulation. Let's take a little bit of time to run one of the famous illustrative examples of variational autoencoders. I have to say, this got me hooked on their usefulness, and made me understand the advantage of using them. \n",
    "\n",
    "For this we are going to use the MNIST character dataset. The MNIST character dataset was one of the first ML datasets. This dataset has become the basis for testing and validating deep learning algorithms. It is so deeply embedded into pytorch that we can just load it from pytorch."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b8228e0d",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install torchvision\n",
    "\n",
    "import pandas as pd \n",
    "import torch\n",
    "import torchvision\n",
    "from torchvision import transforms\n",
    "from torch.utils.data import DataLoader,random_split\n",
    "from torch import nn\n",
    "import torch.nn.functional as F\n",
    "import torch.optim as optim\n",
    "\n",
    "data_dir = 'dataset'\n",
    "\n",
    "train_dataset = torchvision.datasets.MNIST(data_dir, train=True, download=True)\n",
    "test_dataset  = torchvision.datasets.MNIST(data_dir, train=False, download=True)\n",
    "\n",
    "train_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "test_transform = transforms.Compose([transforms.ToTensor(),])\n",
    "\n",
    "train_dataset.transform = train_transform\n",
    "test_dataset.transform = test_transform\n",
    "\n",
    "m=len(train_dataset)\n",
    "\n",
    "train_data, val_data = random_split(train_dataset, [int(m-m*0.2), int(m*0.2)])\n",
    "batch_size=256\n",
    "\n",
    "train_loader = torch.utils.data.DataLoader(train_data, batch_size=batch_size)\n",
    "valid_loader = torch.utils.data.DataLoader(val_data, batch_size=batch_size)\n",
    "test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "9e48ff3b",
   "metadata": {},
   "source": [
    "Now, we need to define the neural network architecture. While this is not the focus of this lecture, lets take a few minutes to look at how we encode this. \n",
    "\n",
    "For this architecture we are going to take the MNIST input dataset, and encoder it into a large vector. The past 10 years of research on deep learning has shown that Convolutional Neural Networks (CNNs) are one of the most robust wasy to encode images. Our enconder will thus be 3 convolutional NN layers stringed together. Each one produces a smaller image from the previous image. Finally, once the image is small enough (3x3, but with 32 features), we take the image and linearize it to a single vector, which we then feed into normal Dense(Linear) layers. \n",
    "\n",
    "Finally because this is a variational autencoder, we need to output a mean and a sigma for each dimension in the latent space, once we have the mean dthe sigma, we can randomly sample a normal distribution with these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "02692699",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n",
    "        self.linear1 = nn.Linear(3*3*32, 128)\n",
    "        self.linear2 = nn.Linear(128, latent_dims)\n",
    "        self.linear3 = nn.Linear(128, latent_dims)\n",
    "\n",
    "        #Now we need sample in phase space \n",
    "        self.N       = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc   = self.N.loc \n",
    "        self.N.scale = self.N.scale\n",
    "        self.kl = 0\n",
    "\n",
    "    def forward(self, x):\n",
    "        #print(x.shape)\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x) #Mean in the gaussian space\n",
    "        sigma = torch.exp(self.linear3(x)) #sigma in the space\n",
    "        z = mu + sigma*self.N.sample(mu.shape) #smear \n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() #Now compute the KL divergence\n",
    "        return z      "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7daa4352",
   "metadata": {},
   "source": [
    "Now that we have encoded everything in a space, we can go ahead and decode it so that we can build a network that tries to generate the original image from the lower dimensional intermediate space. This decoder is the opposite of the encoer and starts from the vector in teh latent space and then expands out the image using the invers of CNNs, ConvTranspose2ds, this generates an image from a vector. Below, is our decder, which finally yields an image at the end. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b2374b20",
   "metadata": {},
   "outputs": [],
   "source": [
    "class Decoder(nn.Module):\n",
    "    \n",
    "    def __init__(self, latent_dims):\n",
    "        super().__init__()\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "        \n",
    "    def forward(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        x = torch.sigmoid(x)\n",
    "        return x"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1b67d234",
   "metadata": {},
   "source": [
    "Finally, we can go ahead and combine everything together to make our variational autoencoder. One last element that we need to understand for the variational autoencoder is how we can train this NN. The way we do this is we train for the inputs to be equal to the outputs. In otherwords, we try to minimize our loss given inputs $x_{i}$ and outputs $y_{i}$ each of size $N$, we can write\n",
    "\n",
    "$$\n",
    "\\mathcal{L} = \\sum^{N}_{i} (x_{i}-y_{i})^{2}\n",
    "$$\n",
    "\n",
    "Now from studies of the variational autoencoder with the above loss, we have come to add another nerm. This term, we refer to as the Kullback-Liebler(KL) divergence term for the latent space to approximate a Gaussian. \n",
    "\n",
    "What does this mean? \n",
    "\n",
    "KL divergence is a measure of how similar to probability distributions are from each other. It is defined as\n",
    "\n",
    "$$\n",
    "\\mathcal{D}_{\\rm KL}\\left(P||Q\\right)  = \\int_{-\\infty}^{\\infty} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) dx\n",
    "$$\n",
    "\n",
    "For two probability distributions $p(x)$ and $q(x)$. It is effectively the difference in log probabilities of two distrbiutions or some notion of the entropy. In the case of the VAE, we apply a KL diverence for the probability distribution of the latent space to approximate a Gaussian dsitriboution with width 1. This we can write as \n",
    "\n",
    "$$\n",
    "\\log\\left(\\mathcal{N}(\\mu,\\sigma)\\right) = -\\frac{1}{2}\\log\\left(2\\pi\\sigma^{2}\\right)-\\left(\\frac{x-\\mu}{2\\sigma}\\right)^{2} \\\\\n",
    "\\log\\left(\\mathcal{N}(\\mu,\\sigma)\\right) - \\log\\left(\\mathcal{N}(\\mu=0,\\sigma=1)\\right) = \n",
    "-\\frac{1}{2}\\log\\left(2\\pi\\sigma^{2}\\right)-\\left(\\frac{x-\\mu}{2\\sigma}\\right)^{2}+\\frac{1}{2}\\log\\left(2\\pi\\right)+\\frac{x^2}{2}\\\\\n",
    "\\int_{-\\infty}^{\\infty} p(x) \\log\\left(\\frac{p(x)}{q(x)}\\right) dx = \n",
    "-\\frac{1}{2}\\log\\left(\\sigma^{2}\\right) - \\frac{1}{4}  + \\frac{\\sigma^2}{2} + \\frac{\\mu^2}{2}\n",
    "\\\\\n",
    "\\mathcal{D}_{\\rm KL}\\left(\\mathcal{N}(\\mu,\\sigma)||\\mathcal{N}(\\mu=0,\\sigma=1)\\right) = \n",
    "-\\frac{1}{2}\\log\\left(\\sigma^{2}\\right) - \\frac{1}{4}  + \\frac{\\sigma^2}{2} + \\frac{\\mu^2}{2}\n",
    "$$"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f0845a7e",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoencoder(nn.Module):\n",
    "    def __init__(self, latent_dims):\n",
    "        super(VariationalAutoencoder, self).__init__()\n",
    "        self.encoder = VariationalEncoder(latent_dims)\n",
    "        self.decoder = Decoder(latent_dims)\n",
    "        self.kl      = 0\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        kl = self.encoder.kl\n",
    "        return self.decoder(z)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "39b0a9e2",
   "metadata": {},
   "source": [
    "Now let's train this and embed it into 2 latent dimensions! With that we can see what is going on. Below we are going to write the code to do the detailed training. You will see that we are now adding the Kullback Leibler loss together with the mean squred error loss. Our mean squared error loss is the typcial autoencoder loss where we try to reproduce the outputs from the inputs. \n",
    "\n",
    "In addition to that, we will write a few more functions. In particular, we are going to write : \n",
    "\n",
    "  * Test function : This function runs the neural network on the test dataset (no backprop)\n",
    "  * Plot Outputs : This is just a fun function to see how the training is going"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9587c9f9",
   "metadata": {},
   "outputs": [],
   "source": [
    "### Set the random seed for reproducible results\n",
    "torch.manual_seed(0)\n",
    "d = 2\n",
    "vae = VariationalAutoencoder(latent_dims=d)\n",
    "lr = 1e-2\n",
    "optim = torch.optim.Adam(vae.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "### Training function\n",
    "def train_epoch(vae, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for x, _ in dataloader: \n",
    "        # Move tensor to the proper device\n",
    "        x_hat = vae(x)\n",
    "        # Evaluate loss\n",
    "        loss = ((x - x_hat)**2).sum() + vae.kl\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        #print('\\t partial train loss (single batch): %f' % (loss.item()))\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "### Testing function\n",
    "def test_epoch(vae, dataloader):\n",
    "    # Set evaluation mode for encoder and decoder\n",
    "    vae.eval()\n",
    "    val_loss = 0.0\n",
    "    with torch.no_grad(): # No need to track the gradients\n",
    "        for x, _ in dataloader:\n",
    "            # Encode data\n",
    "            encoded_data = vae.encoder(x)\n",
    "            # Decode data\n",
    "            x_hat = vae(x)\n",
    "            loss = ((x - x_hat)**2).sum() + vae.kl\n",
    "            val_loss += loss.item()\n",
    "\n",
    "    return val_loss / len(dataloader.dataset)\n",
    "\n",
    "def plot_ae_outputs(encoder,decoder,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    targets = test_dataset.targets.numpy()\n",
    "    t_idx = {i:np.where(targets==i)[0][0] for i in range(n)}\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = test_dataset[t_idx[i]][0].unsqueeze(0)\n",
    "      encoder.eval()\n",
    "      decoder.eval()\n",
    "      with torch.no_grad():\n",
    "         rec_img  = decoder(encoder(img))\n",
    "      plt.imshow(img.cpu().squeeze().numpy(), cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy(), cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()  "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cd7149be",
   "metadata": {},
   "source": [
    "Alright, with all of that up and running, lets run a training. We are going to run over the data 20 epochs worth. This should be more than enough to get some pretty good results. For this loop, we will run testing and plotting so that we can observe the overall performance of the system. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0b5de372",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 20\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(vae,train_loader,optim)\n",
    "    val_loss = test_epoch(vae,valid_loader)\n",
    "    print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "    plot_ae_outputs(vae.encoder,vae.decoder,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "756bc435",
   "metadata": {},
   "source": [
    "Now, lets visualize what this looks like in the latent space. This tells us a lot of what we are learning. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "5fcd63b7",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_reconstructed(autoencoder, r0=(-8, 8), r1=(-8, 8), n=12):\n",
    "    w = 28\n",
    "    img = np.zeros((n*w, n*w))\n",
    "    for i, y in enumerate(np.linspace(*r1, n)):\n",
    "        for j, x in enumerate(np.linspace(*r0, n)):\n",
    "            z = torch.Tensor([[x, y]])\n",
    "            x_hat = autoencoder.decoder(z)\n",
    "            x_hat = x_hat.reshape(28, 28).to('cpu').detach().numpy()\n",
    "            img[(n-1-i)*w:(n-1-i+1)*w, j*w:(j+1)*w] = x_hat\n",
    "    plt.imshow(img, extent=[*r0, *r1])\n",
    "    \n",
    "plot_reconstructed(vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "28684695",
   "metadata": {},
   "source": [
    "What you see above is the important realization. We have taken a neural network, and conducted a continuous space that makes all the digits into a two dimensional space. That means just a single point in 2 dimensions, with the appropriate transforms can be mapped into a full digit. Imagine what this means. \n",
    "\n",
    "What if we took everybody's face and we mapped it to a 2D space. What if we force this space to correspond to the position on the earth's surface, thats just weird. The point being is the neural network is using gradient descent and some loss prescription to generate a series of complex functions that can take a point in 2D space and map it to a number. One improtant thing to remember is that the above is a variational autoencdoer. Repeating the above with just a regular autencoder does not guarantee a nice continuous space. The variational component is essnetial to effectively blur all the features into a fully interpretable space. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "fdabae3b",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-18.6.1</span>\n",
    "\n",
    "Turn off the variational part of the VAE and retrain. What is the space that you get from this? Is it continuous? \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "78a24ac1",
   "metadata": {},
   "source": [
    "<div style=\"border:1.5px; border-style:solid; padding: 0.5em; border-color: #90409C; color: #90409C;\">\n",
    "\n",
    "**SOLUTION:**\n",
    "\n",
    "<pre>\n",
    "The latent space is not as continuous\n",
    "</pre>\n",
    "        \n",
    "**EXPLANATION:**\n",
    "Unlike the VAE, which has random generation built into it the AE does not. As a result you there are discontinuities for similar numbers.  \n",
    "    \n",
    "</div>\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "949f706a",
   "metadata": {},
   "source": [
    "<a name='section_18_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.4 Generating Bragg Scattering with Variational Autoencoders </h2>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "16458b57",
   "metadata": {},
   "source": [
    "Ok, now that we have built a variational Autoencoder that can generate random numbers. We can go ahead and build a generative neural network that can generate random proton events. The simplest way to build a neural network for this is to build a variational autoencoder, then we can just sample the latent space to construct our \n",
    "\n",
    "Let's go ahead and do that. For this, we will first focus on a single beam energy Additionally, we will try to predict the a few specific parameters of our simulation. We will not try to predict everything just yet. As a result, lets focus on: \n",
    "\n",
    " * Beam Width\n",
    " * Beam Length\n",
    " * Energy deposited in last centimeter\n",
    " * Energy depsoited in the first centimeter\n",
    " \n",
    "Let's go ahead and construct this guy. The first step is to construct a dataset. We will make a separate dataset for each of our energy bins. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ff9f8954",
   "metadata": {},
   "outputs": [],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import Dataset\n",
    "from torch.autograd import Variable\n",
    "\n",
    "def sumEstep(estep,xstep):\n",
    "    efront=np.zeros(xstep.shape[0])\n",
    "    eback =np.zeros(xstep.shape[0])\n",
    "    for i0 in range(xstep.shape[0]):\n",
    "        efront[i0] = np.sum(estep[i0,xstep[i0] < 3])/3.\n",
    "        #print(xstep[i0] < 3,xstep[i0] > xstep[i0,-1]-3,xstep[i0,-1]-3,xstep[i0],estep[i0])\n",
    "        eback[i0]  = np.sum(estep[i0,xstep[i0] > xstep[i0,-1]-3])/3.\n",
    "    return efront,eback\n",
    "\n",
    "def createData(ixstep,ipstep,iestep,iystep):\n",
    "    length=ixstep[:,-1]\n",
    "    width =iystep[:,-1]\n",
    "    efront=np.zeros(ixstep.shape[0])\n",
    "    eback =np.zeros(ixstep.shape[0])\n",
    "    for i0 in range(ixstep.shape[0]):\n",
    "        efront[i0] = np.sum(iestep[i0,ixstep[i0] < 3])/3.\n",
    "        eback[i0]  = np.sum(iestep[i0,ixstep[i0] > ixstep[i0,-1]-1])\n",
    "    processed_data = np.vstack((length,width,efront,eback))\n",
    "    trainset       = torch.tensor(processed_data).float()\n",
    "    return trainset\n",
    "\n",
    "indataset150=createData(xstep150,pstep150,estep150,ystep150)\n",
    "indataset150=indataset150.T\n",
    "\n",
    "indataset200=createData(xstep200,pstep200,estep200,ystep200)\n",
    "indataset200=indataset200.T\n",
    "\n",
    "indataset250=createData(xstep250,pstep250,estep250,ystep250)\n",
    "indataset250=indataset250.T\n",
    "\n",
    "indataset300=createData(xstep300,pstep300,estep300,ystep300)\n",
    "indataset300=indataset300.T\n",
    "\n",
    "indataset   = np.vstack((indataset150,indataset200,indataset250,indataset300))\n",
    "print(indataset.shape)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8f87f550",
   "metadata": {},
   "source": [
    "Now we are going to make a torch dataset. That way we can use all the quality features of pytorch to really improve on what we are doing. The classic way is to extend the torch class. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "73f90ee8",
   "metadata": {},
   "outputs": [],
   "source": [
    "class DataSet(Dataset):\n",
    "    def __init__(self, samples, labels):\n",
    "        self.labels  = labels\n",
    "        self.samples = samples\n",
    "        if len(samples) != len(labels):\n",
    "            raise ValueError(\n",
    "                f\"should have the same number of samples({len(samples)}) as there are labels({len(labels)})\")\n",
    "\n",
    "    def __len__(self):\n",
    "        return len(self.labels)\n",
    "\n",
    "    def nfeatures(self):\n",
    "        return self.samples.shape[1]\n",
    "    \n",
    "    def __getitem__(self, index):\n",
    "        y = self.labels[index]\n",
    "        x = self.samples[index]\n",
    "        return x, y\n",
    "    \n",
    "dataset150=DataSet(samples=indataset150,labels=np.ones(indataset150.shape[0])*150)\n",
    "dataset=DataSet(samples=indataset,labels=np.ones(indataset.shape[0])*150)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3b2e3d77",
   "metadata": {},
   "source": [
    "Now that we have constructed a simple dataset, we can go ahead and make a variational autoencoder to describe all this information. Since we are not taking images for the moment, we can do this with dense layers as oppose to a convolutional neural network.  Don't get too sad, we are going to use a CNN again later on. \n",
    "\n",
    "On a relatied note, you see that the encoder and decoder have different sizes. This is perhaps not the best thing to do, but this came after some hand tuning. We could definitely do better. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "916d461f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, input_dims,latent_dims):  \n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.linear1 = nn.Linear(input_dims, 10*input_dims)\n",
    "        self.batch = nn.BatchNorm1d(10*input_dims)\n",
    "        self.linear2 = nn.Linear(10*input_dims, 10*input_dims)\n",
    "        self.linear3 = nn.Linear(10*input_dims, latent_dims)\n",
    "        self.linear4 = nn.Linear(10*input_dims, latent_dims)\n",
    "\n",
    "        #Now we need sample in phase space \n",
    "        self.N       = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc   = self.N.loc \n",
    "        self.N.scale = self.N.scale\n",
    "        self.kl = 0\n",
    "        \n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 25*input_dims),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25*input_dims, 25*input_dims),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(25*input_dims, input_dims),\n",
    "        )\n",
    "\n",
    "        \n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.batch(self.linear2(x)))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        mu =  self.linear3(x) #Mean in the gaussian space\n",
    "        sigma = torch.exp(self.linear4(x)) #sigma in the space\n",
    "        z = mu + sigma*self.N.sample(mu.shape) #smear \n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() #Now compute the KL divergence\n",
    "        return z\n",
    "            \n",
    "    def decoder(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        return x\n",
    "    \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "d = 2\n",
    "proton_feat_vae = VariationalAutoEncoder(input_dims=dataset.nfeatures(),latent_dims=d)\n",
    "lr = 1e-3 \n",
    "optim = torch.optim.Adam(proton_feat_vae.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_loader = torch.utils.data.DataLoader(dataset150, batch_size=500)\n",
    "#valid_loader = torch.utils.data.DataLoader(dataset150, batch_size=500)\n",
    "#test_loader = torch.utils.data.DataLoader(test_dataset, batch_size=batch_size,shuffle=True)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7287dcf6",
   "metadata": {},
   "source": [
    "Now, I know tis badd form, but we are going to skip the validation step in the interest of getting to the answer. Lets go ahead and train up this guy, and see how well we can generate these features. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "e6db8bc8",
   "metadata": {},
   "outputs": [],
   "source": [
    "num_epochs = 5001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(proton_feat_vae,train_loader,optim)\n",
    "    #al_loss = test_epoch(vae,valid_loader)\n",
    "    if epoch % 500 == 0:\n",
    "        print('EPOCH {}/{} \\t train loss {:.3f} \\t'.format(epoch + 1, num_epochs,train_loss))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "107de0c1",
   "metadata": {},
   "source": [
    "An important check is to look at he space of events within the VAE. Since we are encoding everything into two dimensions. We can go ahead and visualize where our events fall in the space. Ideally, these events should build a 2D gaussian. However, its not clear this thing will work perfectly. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d34f9f1b",
   "metadata": {},
   "outputs": [],
   "source": [
    "values = np.empty((0,2))\n",
    "with torch.no_grad(): # No need to track the gradients\n",
    "        for x, _ in train_loader:\n",
    "            encoded_data = proton_feat_vae.encoder(x)\n",
    "            values = np.vstack((values,encoded_data.detach().numpy()))\n",
    "plt.plot(values[:,0],values[:,1],'.')\n",
    "plt.xlabel(\"latent-x\")\n",
    "plt.ylabel(\"latent-y\")\n",
    "plt.show()\n",
    "\n",
    "_,bins,_=plt.hist(values[:,0],bins=20,alpha=0.5,label='latent-x')\n",
    "_,bins,_=plt.hist(values[:,1],bins=bins,alpha=0.5,label='latent-y')\n",
    "plt.legend()\n",
    "plt.show()\n",
    "\n",
    "print(np.sqrt(values[:,0]**2+values[:,1]**2).mean())\n",
    "print(\"Mean x:\",values[:,0].mean(),\"RMS y:\",values[:,1].mean())\n",
    "print(\"RMS x:\",values[:,0].std(),\"RMS y:\",values[:,1].std())"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1aa75740",
   "metadata": {},
   "source": [
    "Now, we can generate our own events from the VAE just by sampling  the latent space. Ideallly our training has converged to a gaussian with mean 0 and sigma=1. However, you can see above its not perfect. Let's try with that to start with and see how well things are generated. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7439bd9d",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plotVAEOutputs(iLatentDim,iDataSet,iVAE):\n",
    "    testlatent=torch.randn(iDataSet.shape[0],iLatentDim)\n",
    "    testlatent=testlatent.reshape(iDataSet.shape[0],iLatentDim)\n",
    "    outputvars=iVAE.decoder(testlatent)\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(iDataSet[:,0],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,0].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(-10,10,0.5)\n",
    "    plt.hist(iDataSet[:,1],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,1].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,10,0.05)\n",
    "    plt.hist(iDataSet[:,2],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,2].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(iDataSet[:,3],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,3].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "plotVAEOutputs(d,indataset150,proton_feat_vae)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "249fd92f",
   "metadata": {},
   "source": [
    "Now that captured most of the high level features, we can naturally play around with the system and try to get get it better.  Here are some func exercises that we can quickly explore to see if we can improve things overall. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3502549c",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-18.7.1</span>\n",
    "\n",
    "We saw the data had a smaller RMS than what we sampled. What happens if we shrink/expand the above RMS. How does it change the prediction?"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "10e101b1",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotVAEOutputsScale(iLatentDim,iDataSet,iVAE,iScale):\n",
    "    testlatent=torch.randn(iDataSet.shape[0],iLatentDim)*iScale\n",
    "    testlatent=testlatent.reshape(iDataSet.shape[0],iLatentDim)\n",
    "    outputvars=iVAE.decoder(testlatent)\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(iDataSet[:,0],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,0].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(-10,10,0.5)\n",
    "    plt.hist(iDataSet[:,1],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,1].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,10,0.05)\n",
    "    plt.hist(iDataSet[:,2],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,2].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(iDataSet[:,3],label='truth',alpha=0.5,bins=xrange)\n",
    "    plt.hist(outputvars[:,3].detach().numpy(),label='prediction',alpha=0.5,bins=xrange)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "\n",
    "plotVAEOutputsScale(d,indataset150,proton_feat_vae,0.7)\n",
    "plotVAEOutputsScale(d,indataset150,proton_feat_vae,2.7)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b1db6962",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-18.7.2</span>\n",
    "\n",
    "Train the above with 4 latent dimensions using just the 150 MeV dataset how good is the agreement? Why are you not surprised that 4 dimensions is so good? "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "09e18b11",
   "metadata": {},
   "outputs": [],
   "source": [
    "torch.manual_seed(0)\n",
    "d = 4\n",
    "proton_feat_vae_4d = VariationalAutoEncoder(input_dims=dataset.nfeatures(),latent_dims=d)\n",
    "lr = 1e-2 \n",
    "optim = torch.optim.Adam(proton_feat_vae_4d.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_loader = torch.utils.data.DataLoader(dataset150, batch_size=500)\n",
    "num_epochs = 5001\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(proton_feat_vae_4d,train_loader,optim)\n",
    "    #val_loss = test_epoch(vae,valid_loader)\n",
    "    if epoch % 500 == 0:\n",
    "        print('EPOCH {}/{} \\t train loss {:.3f} \\t'.format(epoch + 1, num_epochs,train_loss))\n",
    "\n",
    "plotVAEOutputs(d,indataset150,proton_feat_vae_4d)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "c199f252",
   "metadata": {},
   "source": [
    "<a name='section_18_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.5 Generating Full Bragg Scattering details </h2>  \n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "8b742f55",
   "metadata": {},
   "source": [
    "Given the success of above, lets look at our events and see if we can try to generate a fully detailed event. Recall that we actually save the energy loss, x and y positions of each proton event. We can translate this to directly to an image by considering the Z-axis as the energy deposited and suming all of our energies. Lets go ahead and try to translate this to an image, and see what it looks like. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "fec0b41f",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotImage(iId,ixstep,iestep,iystep):\n",
    "    #plt.plot(ixstep[iId],iystep[iId])#,iestep[iId])\n",
    "    #plt.show()\n",
    "    #Now lets make a regular image \n",
    "    xbin = np.arange(-1,55, 2)\n",
    "    ybin = np.arange(-3.75, 3.75, 0.25)\n",
    "    #xbin = np.arange(-0.5,60.5, 1)\n",
    "    #ybin = np.arange(-5.125, 5.125, 0.25)\n",
    "    H, xedges, yedges = np.histogram2d(ixstep.flatten(), iystep.flatten(), bins=(xbin, ybin),weights=iestep.flatten())  \n",
    "    plt.imshow(H.T,extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])  \n",
    "    plt.show()\n",
    "    #X, Y = np.meshgrid(xedges, yedges)\n",
    "    #plt.pcolormesh(X,Y,H.T)  \n",
    "    #plt.show()\n",
    "\n",
    "plotImage(-1,xstep150,estep150,ystep150)\n",
    "plotImage(-1,xstep200,estep200,ystep200)\n",
    "plotImage(-1,xstep250,estep250,ystep250)\n",
    "plotImage(-1,xstep300,estep300,ystep300)\n",
    "\n",
    "\n",
    "def makeImageDataSet(iE,ixstep,iestep,iystep):\n",
    "    dataset=np.empty((0,1,28,28))\n",
    "    for pX,pE,pY in zip(ixstep,iestep,iystep):\n",
    "        xbin = np.arange(-1,57, 2)\n",
    "        ybin = np.arange(-3.625, 3.625, 0.25)\n",
    "        H, xedges, yedges = np.histogram2d(pX.flatten(), pY.flatten(), bins=(xbin, ybin),weights=pE.flatten())  \n",
    "        #H, xedges, yedges = np.histogram2d(ixstep.flatten(), iystep.flatten(), bins=(xbin, ybin),weights=iestep.flatten())  \n",
    "        #plt.imshow(H.T,extent=[xedges[0], xedges[-1], yedges[0], yedges[-1]])  \n",
    "        #plt.show()\n",
    "        H = np.reshape(H.T,(1,1,28,28))\n",
    "        dataset = np.vstack((dataset,H))\n",
    "    print(dataset.shape)\n",
    "    Tdataset = torch.tensor(dataset).float()\n",
    "    datasetout=DataSet(samples=Tdataset,labels=np.ones(dataset.shape[0])*iE)\n",
    "    return datasetout,dataset\n",
    "image150,dimage150=makeImageDataSet(1.50,xstep150,estep150,ystep150)\n",
    "image200,dimage200=makeImageDataSet(2.00,xstep200,estep200,ystep200)\n",
    "image250,dimage250=makeImageDataSet(2.50,xstep250,estep250,ystep250)\n",
    "image300,dimage300=makeImageDataSet(3.00,xstep300,estep300,ystep300)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d4de9fda",
   "metadata": {},
   "source": [
    "Now from this lets go ahead and make a VAE that describes our dataset. We can exploit the VAE architecture used above for MNIST, just slightly tweaked to capture the pixels that we have for the above shower descriptions.  Note that I actually made the images exactly the same size as the MNIST :)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "12f3671f",
   "metadata": {},
   "outputs": [],
   "source": [
    "class VariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(VariationalAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n",
    "        self.linear1 = nn.Linear(3*3*32, 128)\n",
    "        self.linear2 = nn.Linear(128, latent_dims)\n",
    "        self.linear3 = nn.Linear(128, latent_dims)\n",
    "\n",
    "        #Now we need sample in phase space \n",
    "        self.N       = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc   = self.N.loc \n",
    "        self.N.scale = self.N.scale\n",
    "        self.kl = 0\n",
    "        \n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims, 128),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(128, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def encoder(self, x):\n",
    "        x = F.relu(self.conv1(x))\n",
    "        x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        mu =  self.linear2(x) #Mean in the gaussian space\n",
    "        sigma = torch.exp(self.linear3(x)) #sigma in the space\n",
    "        z = mu + sigma*self.N.sample(mu.shape) #smear \n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() #Now compute the KL divergence\n",
    "        return z\n",
    "\n",
    "            \n",
    "    def decoder(self, x):\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def forward(self, x):\n",
    "        z = self.encoder(x)\n",
    "        return self.decoder(z)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "d = 4\n",
    "proton_vae_image = VariationalAutoEncoder(latent_dims=d)\n",
    "lr = 1e-2 \n",
    "optim = torch.optim.Adam(proton_vae_image.parameters(), lr=lr, weight_decay=1e-5)\n",
    "train_loader = torch.utils.data.DataLoader(image150, batch_size=500)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "915af400",
   "metadata": {},
   "source": [
    "This is basically the VAE for MNIST above. Given the heavy development for deep learning, it sometimes seems that you can copy and paste anything. Anyway, lets make our plot function as well like we had above"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "43d539af",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_outputs(idataset,ivae,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    t_idx = np.random.randint(10,size=10)\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = idataset[t_idx[i]][0].unsqueeze(0)\n",
    "      vae.eval()\n",
    "      #encoder.eval()\n",
    "      #decoder.eval()\n",
    "      with torch.no_grad():\n",
    "         rec_img  = ivae.decoder(ivae.encoder(img))\n",
    "      plt.imshow(img.cpu().squeeze().numpy())#, cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy())#, cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()  \n",
    "    "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7763453a",
   "metadata": {
    "scrolled": true
   },
   "outputs": [],
   "source": [
    "num_epochs = 2500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(proton_vae_image,train_loader,optim)\n",
    "    val_loss=0\n",
    "    if epoch % 500 == 0:\n",
    "        print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "        plot_ae_outputs(image150,proton_vae_image,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "050017cb",
   "metadata": {},
   "source": [
    "Now, we can take a look at the image performance by constructing some useful metrics to see how it compares with the truth. From the above it looks like we are starting to capture things. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b835fcbf",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotVAEImageOutputs(iLatentDim,iDataSet,iVAE):\n",
    "    testlatent=torch.randn(iDataSet.shape[0],iLatentDim)\n",
    "    testlatent=testlatent.reshape(iDataSet.shape[0],iLatentDim)\n",
    "    rec_img  = iVAE.decoder(testlatent)\n",
    "    rec_img  = rec_img.detach().numpy()\n",
    "\n",
    "    xbin = np.arange(-0.5,27.5, 1)\n",
    "    ybin = np.arange(-3.5, 3.5, 0.25)\n",
    "    plt.plot(xbin,np.mean(np.sum(iDataSet,axis=2),axis=0).flatten(),drawstyle='steps-mid',label='MC')\n",
    "    plt.plot(xbin,np.mean(np.sum(rec_img,axis=2),axis=0).flatten(),drawstyle='steps-mid',label='NN')\n",
    "    plt.xlabel('x-distance(cm)')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.plot(ybin,np.mean(np.sum(iDataSet,axis=3),axis=0).flatten(),drawstyle='steps-mid',label='MC')\n",
    "    plt.plot(ybin,np.mean(np.sum(rec_img,axis=3),axis=0).flatten(),drawstyle='steps-mid',label='NN')\n",
    "    plt.xlabel('y-distance(cm)')\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,10,1)\n",
    "    plt.hist(np.sum(iDataSet,axis=2)[0:10].flatten(),label='input',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.hist(np.sum(rec_img ,axis=2)[0:10].flatten(),label='output',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(np.sum(iDataSet,axis=2)[12:20].flatten(),label='input',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.hist(np.sum(rec_img ,axis=2)[12:20].flatten(),label='output',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    dimage150_avg = np.mean(iDataSet,axis=0)\n",
    "    plt.imshow(dimage150_avg[0])\n",
    "    plt.show()\n",
    "\n",
    "    rec_img_avg = np.mean(rec_img,axis=0)\n",
    "    plt.imshow(rec_img_avg[0])\n",
    "    plt.show()\n",
    "\n",
    "#dimage150_avg = np.mean(dimage150,axis=0)\n",
    "#plt.imshow(dimage150_avg[0])\n",
    "#plt.show()\n",
    "#dimage150_tmp = np.mean(dimage150,axis=0)\n",
    "#plt.imshow(dimage150[0][0])\n",
    "#plt.show()\n",
    "#test=np.mean(np.sum(dimage150,axis=3),axis=0)\n",
    "plotVAEImageOutputs(d,dimage150,proton_vae_image)\n",
    "torch.save(proton_vae_image.state_dict(), 'vae_150.pt')"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f21e39f1",
   "metadata": {},
   "source": [
    "### <span style=\"border:3px; border-style:solid; padding: 0.15em; border-color: #90409C; color: #90409C;\">Ex-18.8.1</span>\n",
    "\n",
    "Now that we have a generator that starts to look reasonable. Look at the latency. Lets see how fast it is to generate 100 events. How does this compare to basic generation. \n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "2711cbd7",
   "metadata": {},
   "outputs": [],
   "source": [
    "import time\n",
    "\n",
    "def generate_base(iLatentDim,iN,iVAE):\n",
    "    testlatent=torch.randn(iN,iLatentDim)\n",
    "    testlatent=testlatent.reshape(iN,iLatentDim)\n",
    "    rec_img  = iVAE.decoder(testlatent)\n",
    "    rec_img  = rec_img.detach().numpy()\n",
    "    return rec_img\n",
    "\n",
    "N=250\n",
    "start=time.time()\n",
    "generate_base(4,N,proton_vae_image)\n",
    "stop=time.time()\n",
    "print(\"Time to generate 100 events:\",(stop-start))\n",
    "timeNN=(stop-start)\n",
    "\n",
    "start=time.time()\n",
    "simNYParallelSample(ie=150,im=mproton,iN=N,idt=1e-10,iZ=8)\n",
    "stop=time.time()\n",
    "print(\"Time to generate 100 events:\",(stop-start))\n",
    "timeGen=(stop-start)\n",
    "\n",
    "print(\"===> speed up\",timeGen/timeNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5ff41edd",
   "metadata": {},
   "source": [
    "<a name='section_18_7'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.5 Conditional VAEs allowing for energy based generation </h2>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41b30c80",
   "metadata": {},
   "source": [
    "Now that we have created a basica VAE that captures most of the features that our Monte Carlo Simulation does, lets go ahead and now train for all of our beam energies. \n",
    "\n",
    "Training with all of the beam energies is a bit trickier, because we would like to embed our knowledge of the energy into the VAE so that we can decode our setup with the knowledge of the beam energy. We can think of this as\n",
    "our decorder $d(x)$ now needs to be conditional on the beam energy. We can write this as: \n",
    "\n",
    "$$\n",
    "d\\left(x|E\\right) = f_{NN}\\left(\\mathcal{N}(\\vec{x}|\\vec{\\sigma}),E\\right)\n",
    "$$\n",
    "\n",
    "Where $f_{NN}$ is our neural network, which now takes in the sampled distribution, and the energy. The scheme here is that  we are adding Energy to our latent space as an additional input that we do not sample. This construction is known as a conditional VAE, since now the latent space construction changes conditioned on the beam energy. Let's take a look at the architecture. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "c3961b18",
   "metadata": {},
   "outputs": [],
   "source": [
    "class CondVariationalAutoEncoder(nn.Module):\n",
    "    def __init__(self, latent_dims):  \n",
    "        super(CondVariationalAutoEncoder, self).__init__()\n",
    "        self.conv1 = nn.Conv2d(1, 8, 3, stride=2, padding=1)\n",
    "        self.conv2 = nn.Conv2d(8, 16, 3, stride=2, padding=1)\n",
    "        self.batch2 = nn.BatchNorm2d(16)\n",
    "        self.conv3 = nn.Conv2d(16, 32, 3, stride=2, padding=0)  \n",
    "        self.linear1  = nn.Linear(3*3*32, 64)\n",
    "        self.linear1a = nn.Linear(64, 8)\n",
    "        self.linear1b = nn.Linear(9,  8)\n",
    "        self.linear2 = nn.Linear(8, latent_dims)\n",
    "        self.linear3 = nn.Linear(8, latent_dims)\n",
    "\n",
    "        #Now we need sample in phase space \n",
    "        self.N       = torch.distributions.Normal(0, 1)\n",
    "        self.N.loc   = self.N.loc \n",
    "        self.N.scale = self.N.scale\n",
    "        self.kl = 0\n",
    "        \n",
    "        self.unflatten = nn.Unflatten(dim=1, unflattened_size=(32, 3, 3))\n",
    "\n",
    "        self.decoder_conv = nn.Sequential(\n",
    "            nn.ConvTranspose2d(32, 16, 3, stride=2, output_padding=0),\n",
    "            #nn.BatchNorm2d(16),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(16, 8, 3, stride=2, padding=1, output_padding=1),\n",
    "            nn.BatchNorm2d(8),\n",
    "            nn.ReLU(True),\n",
    "            nn.ConvTranspose2d(8, 1, 3, stride=2, padding=1, output_padding=1)\n",
    "        )\n",
    "\n",
    "        self.decoder_lin = nn.Sequential(\n",
    "            nn.Linear(latent_dims+1, 64),\n",
    "            nn.ReLU(True),\n",
    "            nn.Linear(64, 3 * 3 * 32),\n",
    "            nn.ReLU(True)\n",
    "        )\n",
    "\n",
    "\n",
    "    def encoder(self, x, c): #c is our condition\n",
    "        x = F.relu(self.conv1(x))\n",
    "        #x = F.relu(self.batch2(self.conv2(x)))\n",
    "        x = F.relu(self.conv2(x))\n",
    "        x = F.relu(self.conv3(x))\n",
    "        x = torch.flatten(x, start_dim=1)\n",
    "        x = F.relu(self.linear1(x))\n",
    "        x = F.relu(self.linear1a(x))\n",
    "        x = torch.hstack((x,c))\n",
    "        x = F.relu(self.linear1b(x))\n",
    "        mu =  self.linear2(x) #Mean in the gaussian space with the condition\n",
    "        sigma = torch.exp(self.linear3(x)) #sigma in the space with the condition\n",
    "        z = mu + sigma*self.N.sample(mu.shape) #smear \n",
    "        self.kl = (sigma**2 + mu**2 - torch.log(sigma) - 1/2).sum() #Now compute the KL divergence\n",
    "        return z\n",
    "\n",
    "            \n",
    "    def decoder(self, x, c):\n",
    "        x = torch.hstack((x,c))\n",
    "        x = self.decoder_lin(x)\n",
    "        x = self.unflatten(x)\n",
    "        x = self.decoder_conv(x)\n",
    "        return x\n",
    "\n",
    "        \n",
    "    def forward(self, x, c):\n",
    "        z = self.encoder(x, c)\n",
    "        return self.decoder(z, c)\n",
    "    \n",
    "torch.manual_seed(0)\n",
    "d = 6\n",
    "cvae_proton_image = CondVariationalAutoEncoder(latent_dims=d)\n",
    "lr = 1e-2 \n",
    "optim = torch.optim.Adam(cvae_proton_image.parameters(), lr=lr, weight_decay=1e-5)\n",
    "from torch.utils.data import ConcatDataset\n",
    "megeimage=ConcatDataset([image150, image200,image250,image300])\n",
    "train_loader = torch.utils.data.DataLoader(megeimage, batch_size=500)\n",
    "#from torchsummary import summary\n",
    "#summary(cvae_proton_image, ((1, 28, 28),1))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "7351254d",
   "metadata": {},
   "source": [
    "Note the subtlelties here. For this setup, we are feeding the condition in at a few different points. The condition comes in before the latent space is constructed. The condition also comes into the decoder. The point being here is that we know the condition and so we have the liberty to put this in whereever we want. \n",
    "\n",
    "Let's go ahead and train it. We will have to rewrite the functions for training from above, so we put them down here: "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d94e0f5b",
   "metadata": {},
   "outputs": [],
   "source": [
    "def plot_ae_outputs(idataset,ivae,n=10):\n",
    "    plt.figure(figsize=(16,4.5))\n",
    "    t_idx = np.random.randint(10,size=10)\n",
    "    for i in range(n):\n",
    "      ax = plt.subplot(2,n,i+1)\n",
    "      img = idataset[t_idx[i]][0].unsqueeze(0)\n",
    "      ivae.eval()\n",
    "      #encoder.eval()\n",
    "      #decoder.eval()\n",
    "      npones=np.ones((1,1))*1.50\n",
    "      beam=torch.tensor(npones).float()\n",
    "      with torch.no_grad():\n",
    "         rec_img  = ivae.decoder(ivae.encoder(img,beam),beam)\n",
    "      plt.imshow(img.cpu().squeeze().numpy())#, cmap='gist_gray')\n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "        ax.set_title('Original images')\n",
    "      ax = plt.subplot(2, n, i + 1 + n)\n",
    "      plt.imshow(rec_img.cpu().squeeze().numpy())#, cmap='gist_gray')  \n",
    "      ax.get_xaxis().set_visible(False)\n",
    "      ax.get_yaxis().set_visible(False)  \n",
    "      if i == n//2:\n",
    "         ax.set_title('Reconstructed images')\n",
    "    plt.show()  \n",
    "\n",
    "### Training function\n",
    "def train_epoch(vae, dataloader, optimizer):\n",
    "    # Set train mode for both the encoder and the decoder\n",
    "    vae.train()\n",
    "    train_loss = 0.0\n",
    "    # Iterate the dataloader (we do not need the label values, this is unsupervised learning)\n",
    "    for x, c in dataloader: \n",
    "        # Move tensor to the proper device\n",
    "        c=c.reshape(len(c),1).float()\n",
    "        x_hat = vae(x,c)\n",
    "        # Evaluate loss\n",
    "        loss = ((x - x_hat)**2).sum() + vae.kl\n",
    "        # Backward pass\n",
    "        optimizer.zero_grad()\n",
    "        loss.backward()\n",
    "        optimizer.step()\n",
    "        # Print batch loss\n",
    "        #print('\\t partial train loss (single batch): %f' % (loss.item()))\n",
    "        train_loss+=loss.item()\n",
    "\n",
    "    return train_loss / len(dataloader.dataset)\n",
    "\n",
    "lr = 1e-3\n",
    "optim = torch.optim.Adam(cvae_proton_image.parameters(), lr=lr, weight_decay=1e-5)\n",
    "\n",
    "num_epochs = 500\n",
    "\n",
    "for epoch in range(num_epochs):\n",
    "    train_loss = train_epoch(cvae_proton_image,train_loader,optim)\n",
    "    val_loss=0\n",
    "    if epoch % 500 == 0:\n",
    "        print('\\n EPOCH {}/{} \\t train loss {:.3f} \\t val loss {:.3f}'.format(epoch + 1, num_epochs,train_loss,val_loss))\n",
    "        plot_ae_outputs(image150,cvae_proton_image,n=10)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "872037a7",
   "metadata": {},
   "source": [
    "Now, lets go ahead and see how well our conditional VAE models the shower shapes, as we vary the shower. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b86574ca",
   "metadata": {
    "scrolled": false
   },
   "outputs": [],
   "source": [
    "def plotCondVAEImageOutputs(iE,iLatentDim,iDataSet,iCVAE):\n",
    "    testlatent=torch.randn(iDataSet.shape[0],iLatentDim)\n",
    "    testlatent=testlatent.reshape(iDataSet.shape[0],iLatentDim)\n",
    "    beamenergy = torch.ones(iDataSet.shape[0],1)*iE\n",
    "    rec_img  = iCVAE.decoder(testlatent,beamenergy)\n",
    "    rec_img  = rec_img.detach().numpy()\n",
    "\n",
    "    xbin = np.arange(-0.5,27.5, 1)\n",
    "    ybin = np.arange(-3.5, 3.5, 0.25)\n",
    "    plt.plot(xbin,np.mean(np.sum(iDataSet,axis=2),axis=0).flatten(),drawstyle='steps-mid',label='MC')\n",
    "    plt.plot(xbin,np.mean(np.sum(rec_img,axis=2),axis=0).flatten(),drawstyle='steps-mid',label='NN')\n",
    "    plt.xlabel('x-distance(cm)')\n",
    "    plt.show()\n",
    "    \n",
    "\n",
    "    plt.plot(ybin,np.mean(np.sum(iDataSet,axis=3),axis=0).flatten(),drawstyle='steps-mid',label='MC')\n",
    "    plt.plot(ybin,np.mean(np.sum(rec_img,axis=3),axis=0).flatten(),drawstyle='steps-mid',label='NN')\n",
    "    plt.xlabel('y-distance(cm)')\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,20,1)\n",
    "    plt.hist(np.sum(iDataSet,axis=2)[0:10].flatten(),label='input',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.hist(np.sum(rec_img ,axis=2)[0:10].flatten(),label='output',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "\n",
    "    xrange=np.arange(0,60,2)\n",
    "    plt.hist(np.sum(iDataSet,axis=2)[12:20].flatten(),label='input',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.hist(np.sum(rec_img ,axis=2)[12:20].flatten(),label='output',alpha=0.5,bins=xrange,density=True)\n",
    "    plt.legend()\n",
    "    plt.show()\n",
    "    \n",
    "    dimage150_avg = np.mean(iDataSet,axis=0)\n",
    "    plt.imshow(dimage150_avg[0])\n",
    "    plt.show()\n",
    "\n",
    "    rec_img_avg = np.mean(rec_img,axis=0)\n",
    "    plt.imshow(rec_img_avg[0])\n",
    "    plt.show()\n",
    "\n",
    "#plotCondVAEImageOutputs(1.50,d,dimage150,cvae_proton_image) \n",
    "\n",
    "#plotCondVAEImageOutputs(2.00,d,dimage200,cvae_proton_image) \n",
    "\n",
    "plotCondVAEImageOutputs(2.50,d,dimage250,cvae_proton_image) \n",
    "torch.save(cvae_proton_image.state_dict(), 'cvae_test.pt')"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "69563505",
   "metadata": {},
   "outputs": [],
   "source": [
    "def generate(iLatentDim,iN,iCVAE,iE):\n",
    "    testlatent=torch.randn(iN,iLatentDim)\n",
    "    testlatent=testlatent.reshape(iN,iLatentDim)\n",
    "    beamenergy = torch.ones(testlatent.shape[0],1)*iE\n",
    "    rec_img  = iCVAE.decoder(testlatent,beamenergy)\n",
    "    rec_img  = rec_img.detach().numpy()\n",
    "    return rec_img\n",
    "\n",
    "N=250\n",
    "start=time.time()\n",
    "generate(d,N,cvae_proton_image,1.5)\n",
    "stop=time.time()\n",
    "print(\"Time to generate 100 events:\",(stop-start))\n",
    "timeNN=(stop-start)\n",
    "\n",
    "start=time.time()\n",
    "simNYParallelSample(ie=150,im=mproton,iN=N,idt=1e-10,iZ=8)\n",
    "stop=time.time()\n",
    "print(\"Time to generate 100 events:\",(stop-start))\n",
    "timeGen=(stop-start)\n",
    "\n",
    "print(\"===> speed up\",timeGen/timeNN)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2e7438cc",
   "metadata": {},
   "source": [
    "## Normalizing Flows :  more Expressive model "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "37072d39",
   "metadata": {},
   "source": [
    "Now, if we really want to make our model extra expressive, we can consider adding normalizing flow layers. The flow would replace the existing Gaussian sample with a Gaussian sample in a transformed space. The iterative transforming in a normalizing flow can make the latent space progressively more and more expressive. \n",
    "\n",
    "We will skip this for now, mostly because its nto clear we can train this fast enough for this lecture, and it could use some tweaking before its ready to be in the notes. "
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e06eac65",
   "metadata": {},
   "source": [
    "<a name='section_18_10'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.6 Bootstrapping </h2>  \n",
    "\n",
    "\n",
    "\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a4d40b04",
   "metadata": {},
   "source": [
    "When we look at the above generation, the full image will a full energy profile can yield a shape. We can run a lot of generation, and treat this shape as a simulation. However, when we quote uncertainties on this shape, we can treat this as a Poisson uncertainty on each bin, since there bins are correlated. \n",
    "\n",
    "To get the full uncertainty of this setup, we really need to go a bit further and deploy techniques that can account for the fact that the whole shape is correlated. There are really two ways to do this: \n",
    "\n",
    " * Write down all the uncertainties going into how you generate your simulation and vary them\n",
    "   * For above, this means many to name a few\n",
    "     * uncertainty in the beam profile, \n",
    "     * uncertainty in the enrgy distribution, \n",
    "     * uncertainty in the detector shape\n",
    "     * uncertainty in the training\n",
    "     * ...\n",
    "   * The uncertainty in the trainingcan be very hard to estimate\n",
    "     * Sometimes this means multiple traings\n",
    "   * We can often mis uncertainties\n",
    "     * Or we just ignore them (eg Laudau unc.)\n",
    " * Reverse engineer the uncertainty from the ultimate variations in the data\n",
    "   * Sometimes its just too complicated to get the uncertainty from inputs\n",
    "   * We can analyze the uncertainty by removing events and looking at the prediction variations\n",
    "     * This involves some assumptions about uncertainty\n",
    "   * Bootstrapping\n",
    "\n",
    "Now before we go ahead and try to solve the problems of our simulation, lets go ahead and do some very basic boostrap examples. Let's sample a uniform distribution from 100 to 300. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d686c921",
   "metadata": {},
   "outputs": [],
   "source": [
    "samples = np.random.uniform(100,300,500)\n",
    "plt.hist(samples)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean:\",np.mean(samples))\n",
    "print(\"RMS:\",np.std(samples))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "a8b1402b",
   "metadata": {},
   "source": [
    "Ok, so my first question for you is, what is the analytic mean and standard deviation of this sample? \n",
    "\n",
    "\n",
    "This is a just a flat distribution, so we can immediately write the mean and standard deviation of a [flat distribution](https://en.wikipedia.org/wiki/Continuous_uniform_distribution) raging from $a$ to $b$ as\n",
    "\n",
    "$$\n",
    "\\bar{x} = \\frac{1}{(b-a)}\\\\\n",
    "\\sigma_{x} = \\frac{b-a}{\\sqrt{12}}\n",
    "$$\n",
    "\n",
    "Secondly, we can ask ourselves what the uncertainty on the mean and standard variance is for $N$ measurements. If you recall early on in this, class this is computed by computing the variance in the mean over a sample and variance of a standard deviation over the sample for $N$ samples. Analytically, we can write this as: \n",
    "\n",
    "$$\n",
    "\\sigma_{\\bar{x}} = \\frac{\\sigma_{x}}{\\sqrt{N}} \\\\\n",
    "\\sigma_{\\sigma_{x}} = \\frac{\\sigma_{x}}{\\sqrt{2N}}\n",
    "$$\n",
    "\n",
    "Now, what if we want to compute the uncertainty in the kurtosis or perhaps the ratio of the kurotsis to the mean distribution, or something else, how would we compute that? \n",
    "\n",
    "Well....we can do it analytically, but lets try another approach\n",
    "\n",
    "Lets resample our existing distribution with a random poisson sampler, and make a toy experiment. When can then proceed to make many toys and then look at the distribution of our means and standard deviations, and see if they agree with our analytical forms. \n",
    "\n",
    "To do that, we are going ot use the random choice, which is our Poisson sampler. Let's take a look"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "9f1a5970",
   "metadata": {},
   "outputs": [],
   "source": [
    "arr=np.arange(10)\n",
    "test=np.random.choice(arr,size=5)\n",
    "print(test)\n",
    "test=np.random.choice(arr,size=10)\n",
    "print(test)\n",
    "plt.hist(test)\n",
    "test=np.random.choice(arr,size=20)\n",
    "print(test)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "cbeda103",
   "metadata": {},
   "source": [
    "Notice, we will have dupblicates. That happens with our poisson sampler. This is really kind of the core idea. Is that if we bias our dataset by poisson sampling it, we get a notion for how much our dataset will vary if we just choose differently from our base dataset. \n",
    "\n",
    "Let's go ahead and compute the mean and RMS of our data. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0fe2b8e7",
   "metadata": {},
   "outputs": [],
   "source": [
    "sample=np.random.choice(samples, size = 500)\n",
    "print(\"Mean:\",np.mean(sample))\n",
    "print(\"RMS:\",np.std(sample))\n",
    "\n",
    "boot_means = []\n",
    "boot_stds  = []\n",
    "for _ in range(10000):\n",
    "    boot_sample = np.random.choice(samples,replace = True, size = 500) # take a random sample each iteration\n",
    "    boot_means.append(np.mean(boot_sample)) # append the mean to boot_means\n",
    "    boot_stds.append(np.std(boot_sample)) # append the mean to boot_means\n",
    "boot_means_np = np.array(boot_means) # transform it into a numpy array for calculation\n",
    "boot_stds_np = np.array(boot_stds)\n",
    "\n",
    "plt.hist(boot_means_np)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of samples:\",np.mean(boot_means_np))\n",
    "print(\"Std of mean of samples:\",np.std(boot_means_np))\n",
    "\n",
    "plt.hist(boot_stds_np)\n",
    "plt.show()\n",
    "\n",
    "print(\"Mean of samples:\",np.mean(boot_stds_np))\n",
    "print(\"Std of mean of samples:\",np.std(boot_stds_np))"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "eafbef61",
   "metadata": {},
   "source": [
    "Now lets compare this with the true values from analytic calculations, and see how well we do"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "ab824c13",
   "metadata": {},
   "outputs": [],
   "source": [
    "analytic         = (300.+100.)/2.\n",
    "analytic_std     = (300.-100.)/np.sqrt(12.) \n",
    "analytic_err     = analytic_std/np.sqrt(500.)\n",
    "analytic_std_err = analytic_std/np.sqrt(2.*500.) \n",
    "print(\"Mean of samples:\",np.mean(boot_means_np),\"+/-\",np.std(boot_means_np)\n",
    "      ,\"\\nAnalytic:\",analytic,\"+/-\",analytic_err)\n",
    "\n",
    "print(\"Std of samples:\",np.mean(boot_stds_np),\"+/-\",np.std(boot_stds_np)\n",
    "      ,\"\\nAnalytic:\",analytic_std,\"+/-\",analytic_std_err)\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "2b04c4d8",
   "metadata": {},
   "source": [
    "Now, interestingly, what if we did the same but just randomly sampled every time. This would really be the correct way to get the variance. However, we sometimes don't have the opportunity or the computing power to do this. Let's go ahead and take a look. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "1fe3defa",
   "metadata": {},
   "outputs": [],
   "source": [
    "samp_means = []\n",
    "samp_stds  = []\n",
    "for _ in range(10000):\n",
    "    samp_sample = np.random.uniform(100,300,500)\n",
    "    samp_means.append(np.mean(samp_sample)) # append the mean to boot_means\n",
    "    samp_stds.append(np.std(samp_sample)) # append the mean to boot_means\n",
    "samp_means_np = np.array(samp_means) # transform it into a numpy array for calculation\n",
    "samp_stds_np = np.array(samp_stds)\n",
    "\n",
    "print(\"Mean of samples:\",np.mean(samp_means_np),\"+/-\",np.std(samp_means_np)\n",
    "      ,\"\\nBoot:\",np.mean(boot_means_np),\"+/-\",np.std(boot_means_np))\n",
    "\n",
    "print(\"Std of samples:\",np.mean(samp_stds_np),\"+/-\",np.std(samp_stds_np)\n",
    "      ,\"\\nBoot:\",np.mean(boot_stds_np),\"+/-\",np.std(boot_stds_np))\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ea59d3d1",
   "metadata": {},
   "source": [
    "While its not perfect, it does give us the notion of variability that we have. Importantly, for the bottom one we sampled a uniform distribution millions of times, whereas for the boostrap we only sampled the uniform distribution once. \n",
    "\n",
    "This will be critical once we go and try to assess the uncertainty on derived quantities. Ok, now that we have done this by hand, lets use the scip stats tool to make our boostrap life much easier."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "0c00c28f",
   "metadata": {},
   "outputs": [],
   "source": [
    "from scipy.stats import bootstrap\n",
    "import numpy as np\n",
    "\n",
    "test_samples = (samples,)\n",
    "bootstrap_ci = bootstrap(test_samples, np.std, confidence_level=0.68,random_state=1, method='percentile')\n",
    "print(bootstrap_ci.confidence_interval,(bootstrap_ci.confidence_interval.high-bootstrap_ci.confidence_interval.low)/2.)\n",
    "print(bootstrap_ci.standard_error)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "bbd16edb",
   "metadata": {},
   "source": [
    "Now the big gains from this approach are that we can start to compute really complicated things that we would not be able to do without sampling events. Let's consider a function that is not necessarily differentiable, and see if we can compute the uncertainty on the mean, and corrleation between teh outputs"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "f3383277",
   "metadata": {},
   "outputs": [],
   "source": [
    "def func(x):\n",
    "    x_sort = np.sort(x,axis=1)\n",
    "    print(x_sort.shape)\n",
    "    return (np.vstack((x_sort[:,0] + x_sort[:,-1],x_sort[:,1] + x_sort[:,-1] ))).T\n",
    "\n",
    "rand_data = np.random.uniform(0,100,5000).reshape(500,10)\n",
    "out_data = func(rand_data)\n",
    "plt.plot(out_data[:,0],out_data[:,1],\".\")\n",
    "plt.show()\n",
    "print(\"correlation:\",np.corrcoef(out_data[:,0],out_data[:,1]))\n",
    "\n",
    "plt.hist(out_data[:,0]+out_data[:,1])"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "689d040e",
   "metadata": {},
   "source": [
    "Ok, so the above is some complicated mess. Let's bootstrap this guy, and see how much things vary. Since, we "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b96a9619",
   "metadata": {},
   "outputs": [],
   "source": [
    "boot_sum   = []\n",
    "boot_corr  = []\n",
    "for _ in range(10000):\n",
    "    #boot_sample = np.random.choice(out_data,replace = True, size = 500) # take a random sample each iteration\n",
    "    boot_sample = out_data[np.random.choice(out_data.shape[0], 500, replace=True)]\n",
    "    boot_sum.append(np.mean(boot_sample[:,0]+boot_sample[:,1]))\n",
    "    boot_corr.append(np.corrcoef(boot_sample[:,0],boot_sample[:,1])[1,0])\n",
    "boot_sum_np = np.array(boot_sum) # transform it into a numpy array for calculation\n",
    "boot_corr_np = np.array(boot_corr)\n",
    "print(\"Sum of Boot:\",np.mean(boot_sum_np),\"+/-\",np.std(boot_sum_np),\"---\",np.sqrt(np.mean(boot_sum_np)/500))\n",
    "print(\"Corr of samples:\",np.mean(boot_corr_np),\"+/-\",np.std(boot_corr_np))\n",
    "print(boot_sum_np.shape)\n",
    "\n",
    "plt.hist(boot_sum_np,density=True)\n",
    "plt.xlabel(\"$x_{1}+x_{2}$\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.show()\n",
    "\n",
    "plt.hist(boot_corr_np,density=True)\n",
    "plt.xlabel(\"Corr($x_{1},x_{2}$)\")\n",
    "plt.ylabel(\"pdf\")\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "93f17e82",
   "metadata": {},
   "source": [
    "Now there is one other approach to obtaining uncertainty from a fixed dataset. This approach is known as the delete-d jackknikfe. In the delete-d jackknife our strategy is to remove a subset of events and recompute the observables. We can then repeat this strategy many times by removing a few events and recomputing everything. The \"Delete-d\" refers to d as the removal of an event. This is sometimes even done as the delete-1 jackknife. \n",
    "\n",
    "Unlike the bootstrap where we resample many times and we find that standard deviation of the bootstrap ($\\sigma_{\\rm bootstrap}$) approximates the standard error: \n",
    "\n",
    "$\\sigma_{\\rm bootstrap} \\approx \\sigma_{\\rm error}$ \n",
    "\n",
    "With the delete-d jackknife we have \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "813af5e1",
   "metadata": {},
   "source": [
    "<a name='section_18_11'></a>\n",
    "<hr style=\"height: 1px;\">\n",
    "\n",
    "## <h2 style=\"border:1px; border-style:solid; padding: 0.25em; color: #FFFFFF; background-color: #90409C\">L19.6 Bootstrapping For Neural Networks</h2>  \n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "e2ded8a9",
   "metadata": {},
   "source": [
    "Now that we have gone through the prospects of what it is like to use the bootstrap method, lets go ahead and see if we can use it to get the uncertainties of our generated sample of proton events. This is really where methods like boostreap can really help us.  In this section, we will take our simulated proton events and we wil extract uncertainties in the energy shape and profile of the simulation, once we have done that we can go ahead and see if we can do the same thing with our neural network output.\n",
    "\n",
    "To do this, lets look at the profile along distance of our bins. To make it clear this is an issue, we can make a plot where we use the standard devation, and one where we use the standard deviation/$\\sqrt{N}$. Neither of these seem very appropriate. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "b39f95a6",
   "metadata": {},
   "outputs": [],
   "source": [
    "def makeDataet(iId,ixstep,iestep,iystep):\n",
    "    print(ixstep.shape,iestep.shape,iystep.shape)\n",
    "    outdata = np.hstack((ixstep,iystep,iestep))\n",
    "    outdata = outdata.reshape(1000,3,ixstep.shape[1])\n",
    "    return outdata\n",
    "\n",
    "#bootdatastep150=makeDataet(-1,xstep150,estep150,ystep150)\n",
    "#bootdatastep200=makeDataet(-1,xstep200,estep200,ystep200)\n",
    "#bootdatastep250=makeDataet(-1,xstep250,estep250,ystep250)\n",
    "#bootdatastep300=makeDataet(-1,xstep300,estep300,ystep300)\n",
    "\n",
    "def profile(iInput):\n",
    "    profile_out=np.sum(iInput,axis=3)\n",
    "    return profile_out\n",
    "\n",
    "out_profile150=profile(dimage150)\n",
    "out_profile200=profile(dimage200)\n",
    "out_profile250=profile(dimage250)\n",
    "out_profile300=profile(dimage300)\n",
    "\n",
    "test_150 = np.mean(out_profile150,axis=0)\n",
    "test_200 = np.mean(out_profile200,axis=0)\n",
    "test_250 = np.mean(out_profile250,axis=0)\n",
    "test_300 = np.mean(out_profile300,axis=0)\n",
    "\n",
    "test_150_std = np.std(out_profile150,axis=0)\n",
    "test_200_std = np.std(out_profile200,axis=0)\n",
    "test_250_std = np.std(out_profile250,axis=0)\n",
    "test_300_std = np.std(out_profile300,axis=0)\n",
    "\n",
    "test_150_stderr = np.std(out_profile150,axis=0)/np.sqrt(dimage150.shape[0])\n",
    "test_200_stderr = np.std(out_profile200,axis=0)/np.sqrt(dimage200.shape[0])\n",
    "test_250_stderr = np.std(out_profile250,axis=0)/np.sqrt(dimage250.shape[0])\n",
    "test_300_stderr = np.std(out_profile300,axis=0)/np.sqrt(dimage300.shape[0])\n",
    "\n",
    "plt.errorbar(np.arange(28),test_150.flatten(),yerr=test_150_std,fmt='o',label='150')\n",
    "plt.errorbar(np.arange(28),test_200.flatten(),yerr=test_200_std,fmt='o',label='200')\n",
    "plt.errorbar(np.arange(28),test_250.flatten(),yerr=test_250_std,fmt='o',label='250')\n",
    "plt.errorbar(np.arange(28),test_300.flatten(),yerr=test_300_std,fmt='o',label='300')\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()\n",
    "\n",
    "plt.errorbar(np.arange(28),test_150.flatten(),yerr=test_150_stderr,fmt='o',label='150')\n",
    "plt.errorbar(np.arange(28),test_200.flatten(),yerr=test_200_stderr,fmt='o',label='200')\n",
    "plt.errorbar(np.arange(28),test_250.flatten(),yerr=test_250_stderr,fmt='o',label='250')\n",
    "plt.errorbar(np.arange(28),test_300.flatten(),yerr=test_300_stderr,fmt='o',label='300')\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "13e541ff",
   "metadata": {},
   "source": [
    "Ok now that we have profile, lets go ahead and run the profile with a bootstrap. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "98038f21",
   "metadata": {},
   "outputs": [],
   "source": [
    "#def profile(iInput):\n",
    "#    profile_out=np.sum(iInput,axis=3)\n",
    "#    return profile_out\n",
    "\n",
    "def profileVar(iData,iNSample):\n",
    "    boot_profile = []\n",
    "    for _ in range(iNSample):  \n",
    "        boot_sample = iData[np.random.choice(out_data.shape[0], 1000, replace=True)]\n",
    "        boot_prof   = np.mean(profile(boot_sample),axis=0)\n",
    "        boot_profile.append(boot_prof)\n",
    "    boot_profile = np.array(boot_profile) # transform it into a numpy array for calculation\n",
    "    return np.mean(boot_profile,axis=0),np.std(boot_profile,axis=0)\n",
    "\n",
    "Nsamps=1000\n",
    "dimage150_boot,dimage150_boot_err = profileVar(dimage150,Nsamps)\n",
    "dimage200_boot,dimage200_boot_err = profileVar(dimage200,Nsamps)\n",
    "dimage250_boot,dimage250_boot_err = profileVar(dimage250,Nsamps)\n",
    "dimage300_boot,dimage300_boot_err = profileVar(dimage300,Nsamps)\n",
    "\n",
    "plt.errorbar(np.arange(28),dimage150_boot.flatten(),yerr=dimage150_boot_err,fmt='o',label='150')\n",
    "plt.errorbar(np.arange(28),dimage200_boot.flatten(),yerr=dimage200_boot_err,fmt='o',label='200')\n",
    "plt.errorbar(np.arange(28),dimage250_boot.flatten(),yerr=dimage250_boot_err,fmt='o',label='250')\n",
    "plt.errorbar(np.arange(28),dimage300_boot.flatten(),yerr=dimage300_boot_err,fmt='o',label='300')\n",
    "\n",
    "plt.errorbar(np.arange(28),test_150.flatten(),yerr=test_150_stderr,fmt='o',label='150',alpha=0.5)\n",
    "plt.errorbar(np.arange(28),test_200.flatten(),yerr=test_200_stderr,fmt='o',label='200',alpha=0.5)\n",
    "plt.errorbar(np.arange(28),test_250.flatten(),yerr=test_250_stderr,fmt='o',label='250',alpha=0.5)\n",
    "plt.errorbar(np.arange(28),test_300.flatten(),yerr=test_300_stderr,fmt='o',label='300',alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6128beb8",
   "metadata": {},
   "source": [
    "Now, lets consider a more complicated variable that we can't necessarily compute.  "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "bbcc99b3",
   "metadata": {},
   "outputs": [],
   "source": [
    "def xmaxprofile(iInput):\n",
    "    profile_out=np.sum(iInput,axis=2)\n",
    "    maxbin=np.argmax(profile_out,axis=2)\n",
    "    return np.mean(maxbin,axis=0),np.std(maxbin,axis=0)\n",
    "\n",
    "def xmaxprofile(iInput):\n",
    "    profile_out=np.sum(iInput,axis=2)\n",
    "    profile_out=np.reshape(profile_out,(iInput.shape[0],28))\n",
    "    #maxbin=np.argmax(profile_out,axis=1)\n",
    "    maxbin=np.unravel_index(np.argmax(profile_out, axis=1), profile_out.shape)\n",
    "    Ein=np.sum(profile_out[:,0:2],axis=1)\n",
    "    Eout=profile_out[maxbin]#+profile_out[:,maxbin+1]+profile_out[:,maxbin-1]\n",
    "    ratio=Eout/Ein\n",
    "    #print(Ein[0],Eout[0],Eout.shape)\n",
    "    return np.mean(ratio),np.std(ratio)\n",
    "\n",
    "def xmaxprofilecheck(iInput):\n",
    "    profile_out=np.sum(iInput,axis=2)\n",
    "    profile_out=np.reshape(profile_out,(iInput.shape[0],28))\n",
    "    #maxbin=np.argmax(profile_out,axis=1)\n",
    "    maxbin=np.unravel_index(np.argmax(profile_out, axis=1), profile_out.shape)\n",
    "    Ein=np.sum(profile_out[:,0:2],axis=1)\n",
    "    Eout=profile_out[maxbin]#+profile_out[:,maxbin+1]+profile_out[:,maxbin-1]\n",
    "    ratio=Eout/Ein\n",
    "    #print(Ein[0],Eout[0],Eout.shape)\n",
    "    return ratio\n",
    "\n",
    "def xmaxprofileVar(iData,iNSample):\n",
    "    boot_profile = []\n",
    "    for _ in range(iNSample):  \n",
    "        boot_sample          = iData[np.random.choice(out_data.shape[0], 1000, replace=True)]\n",
    "        boot_prof,boot_std   = xmaxprofile(boot_sample)\n",
    "        boot_profile.append(boot_prof)\n",
    "    boot_profile = np.array(boot_profile) # transform it into a numpy array for calculation\n",
    "    #return np.mean(boot_profile,axis=0),np.std(boot_profile,axis=0)\n",
    "    return boot_profile\n",
    "\n",
    "xtest_150,xtest_150_std=xmaxprofile(dimage150)\n",
    "xtest_200,xtest_200_std=xmaxprofile(dimage200)\n",
    "xtest_250,xtest_250_std=xmaxprofile(dimage250)\n",
    "xtest_300,xtest_300_std=xmaxprofile(dimage300)\n",
    "print(xtest_150,xtest_150_std)\n",
    "\n",
    "xtest_150_stderr = xtest_150_std/np.sqrt(dimage150.shape[0])\n",
    "xtest_200_stderr = xtest_200_std/np.sqrt(dimage200.shape[0])\n",
    "xtest_250_stderr = xtest_250_std/np.sqrt(dimage250.shape[0])\n",
    "xtest_300_stderr = xtest_300_std/np.sqrt(dimage300.shape[0])\n",
    "\n",
    "Nsamps=1000\n",
    "dimage150_boot = xmaxprofileVar(dimage150,Nsamps)\n",
    "dimage200_boot = xmaxprofileVar(dimage200,Nsamps)\n",
    "dimage250_boot = xmaxprofileVar(dimage250,Nsamps)\n",
    "dimage300_boot = xmaxprofileVar(dimage300,Nsamps)\n",
    "\n",
    "plt.hist(dimage150_boot.flatten(),density=True,label='150',alpha=0.5)\n",
    "#plt.hist(dimage200_boot.flatten(),density=True,label='200',alpha=0.5)\n",
    "#plt.hist(dimage250_boot.flatten(),density=True,label='250',alpha=0.5)\n",
    "#plt.hist(dimage300_boot.flatten(),density=True,label='300',alpha=0.5)\n",
    "\n",
    "plt.hist(xtest_150.flatten(),label='150',alpha=0.5)\n",
    "#plt.hist(xtest_200.flatten(),label='200',alpha=0.5)\n",
    "#plt.hist(xtest_250.flatten(),label='250',alpha=0.5)\n",
    "#plt.hist(xtest_300.flatten(),label='300',alpha=0.5)\n",
    "\n",
    "\n",
    "xtest_150_check=xmaxprofilecheck(dimage150)\n",
    "plt.hist(xtest_150_check.flatten(),density=True,label='150 check',alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()\n",
    "\n",
    "def printStats(ival,ierr,iboot):\n",
    "    print(\"Val:\",ival,\"+/-\",ierr,\"boot\",np.mean(iboot),\"+/-\",np.std(iboot))\n",
    "\n",
    "printStats(xtest_150,xtest_150_stderr,dimage150_boot)\n",
    "printStats(xtest_200,xtest_200_stderr,dimage200_boot)\n",
    "printStats(xtest_250,xtest_250_stderr,dimage250_boot)\n",
    "printStats(xtest_300,xtest_300_stderr,dimage300_boot)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "0c34a6e5",
   "metadata": {},
   "source": [
    "Now how is this variabililty, if we generate 1000 images and try to compute it. "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "3b057226",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "dlimage150 = generate(d,N,cvae_proton_image,1.5)\n",
    "dlimage200 = generate(d,N,cvae_proton_image,2.0)\n",
    "dlimage250 = generate(d,N,cvae_proton_image,2.5)\n",
    "dlimage300 = generate(d,N,cvae_proton_image,3.0)\n",
    "\n",
    "Nsamps=1000\n",
    "dlimage150_boot,dlimage150_boot_err = xmaxprofileVar(dlimage150,Nsamps)\n",
    "dlimage200_boot,dlimage200_boot_err = xmaxprofileVar(dlimage200,Nsamps)\n",
    "dlimage250_boot,dlimage250_boot_err = xmaxprofileVar(dlimage250,Nsamps)\n",
    "dlimage300_boot,dlimage300_boot_err = xmaxprofileVar(dlimage300,Nsamps)\n",
    "\n",
    "print(\"err:\",dlimage150_boot_err)\n",
    "\n",
    "#plt.errorbar(np.arange(28),dlimage150_boot.flatten(),yerr=dlimage150_boot_err,fmt='o',label='150')\n",
    "#plt.errorbar(np.arange(28),dlimage200_boot.flatten(),yerr=dlimage200_boot_err,fmt='o',label='200')\n",
    "#plt.errorbar(np.arange(28),dlimage250_boot.flatten(),yerr=dlimage250_boot_err,fmt='o',label='250')\n",
    "#plt.errorbar(np.arange(28),dlimage300_boot.flatten(),yerr=dlimage300_boot_err,fmt='o',label='300')\n",
    "\n",
    "#plt.errorbar(np.arange(28),test_150.flatten(),yerr=test_150_stderr,fmt='o',label='150',alpha=0.5)\n",
    "#plt.errorbar(np.arange(28),test_200.flatten(),yerr=test_200_stderr,fmt='o',label='200',alpha=0.5)\n",
    "#plt.errorbar(np.arange(28),test_250.flatten(),yerr=test_250_stderr,fmt='o',label='250',alpha=0.5)\n",
    "#plt.errorbar(np.arange(28),test_300.flatten(),yerr=test_300_stderr,fmt='o',label='300',alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "7be2146f",
   "metadata": {},
   "outputs": [],
   "source": [
    "N=1000\n",
    "dlimage150 = generate_base(4,N,proton_vae_image)\n",
    "dlimage200 = generate(d,N,cvae_proton_image,2.0)\n",
    "#dlimage250 = generate(d,N,cvae_proton_image,2.5)\n",
    "#dlimage300 = generate(d,N,cvae_proton_image,3.0)\n",
    "\n",
    "Nsamps=1000\n",
    "xdlimage150_boot = xmaxprofileVar(dlimage150,Nsamps)\n",
    "xdlimage200_boot = xmaxprofileVar(dlimage200,Nsamps)\n",
    "#xdlimage250_boot = xmaxprofileVar(dlimage250,Nsamps)\n",
    "#xdlimage300_boot = xmaxprofileVar(dlimage300,Nsamps)\n",
    "\n",
    "print(\"err:\",dlimage150_boot_err)\n",
    "\n",
    "#plt.hist(xdlimage150_boot.flatten(),density=True,label='150 NN',alpha=0.5)\n",
    "#plt.hist(dimage150_boot.flatten(),density=True,label='150 MC',alpha=0.5)\n",
    "\n",
    "plt.hist(xdlimage200_boot.flatten(),density=True,label='200 NN',alpha=0.5)\n",
    "plt.hist(dimage200_boot.flatten(),density=True,label='200 MC',alpha=0.5)\n",
    "\n",
    "#plt.hist(xdlimage200_boot.flatten(),density=True,label='200',alpha=0.5)\n",
    "#plt.hist(xdlimage250_boot.flatten(),density=True,label='250',alpha=0.5)\n",
    "#plt.hist(xdlimage300_boot.flatten(),density=True,label='300',alpha=0.5)\n",
    "\n",
    "plt.hist(xtest_150.flatten(),label='150-avg',alpha=0.5)\n",
    "plt.hist(xtest_200.flatten(),label='200',alpha=0.5)\n",
    "plt.hist(xtest_250.flatten(),label='250',alpha=0.5)\n",
    "plt.hist(xtest_300.flatten(),label='300',alpha=0.5)\n",
    "\n",
    "plt.legend()\n",
    "plt.xlabel('distance(pixel)')\n",
    "plt.ylabel('Energy(MeV)')\n",
    "plt.show()\n"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "d95588e9",
   "metadata": {},
   "source": [
    "## Auxiliary info"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "55a8c434",
   "metadata": {},
   "outputs": [],
   "source": [
    "import scipy\n",
    "print(scipy.stats.norm.ppf(0.95))\n",
    "\n",
    "sample=np.random.normal(0,1,10000)\n",
    "pval=scipy.stats.norm.cdf(sample)\n",
    "plt.hist(pval)\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "8693b634",
   "metadata": {},
   "outputs": [],
   "source": [
    "\"\"\"Script which calculates the required weighting of individual Bragg peaks a to generate spread-out Bragg peak (SOBP).\n",
    "A numerical solution is compared to an exact one from Bortfeld & Schlegel, \"An analytical approximation of depth-\n",
    "dose distributions for therapeutic proton beams\", Phys. Med. Biol. 41 (1996), 1331-1339. \"\"\"\n",
    "\n",
    "import numpy as np\n",
    "from scipy import signal, integrate\n",
    "import matplotlib.pyplot as plt\n",
    "\n",
    "alpha = 1.9e-3      # Empirical constant (Bortfeld & Schlegel (1996), Eq. (1))\n",
    "p = 1.5             # Empirical exponent\n",
    "D0 = 1.0            # Desired dose in the spread-out Bragg peak (SOBP)\n",
    "\n",
    "da = 10.0           # Shallowest part of dose interval (in cm)\n",
    "db = 15.0           # Deepest part of dose interval (in cm)\n",
    "\n",
    "# Shorthand for oft-used expressions\n",
    "a = alpha**(1.0/p)\n",
    "q = 1.0 - 1.0/p\n",
    "\n",
    "# Stopping power S = -dE/dd, mirrored about the y-axis and shifted by db (Bortfeld & Schlegel (1996), Eq. (A5))\n",
    "def g(d):\n",
    "    return np.piecewise(d,[d<0,d>0],[0,lambda d: 1.0/(p*a*d**q)])\n",
    "\n",
    "def Bragg_peak(R,d):\n",
    "    return g(R-d)\n",
    "\n",
    "# Numerically computes the average of a function \"func\" on the interval [d0,d1]\n",
    "def average(func,d0,d1):\n",
    "    integral,_ = integrate.quad(func,d0,d1)/(d1-d0)\n",
    "    return integral\n",
    "\n",
    "# For a function defined for x>0, given an array x of grid values, returns an array of average\n",
    "# function values on intervals [x[n]-dx/2, x[n]+dx/2], where x[n] is a grid value and dx the grid spacing\n",
    "def impulse(func,x):\n",
    "    h = np.zeros(len(x))\n",
    "    dx = x[1] - x[0]\n",
    "    h[0] = average(func, 0, 0.5*dx)\n",
    "    for n in range(1,N):\n",
    "        h[n] = average(func, x[n]-0.5*dx, x[n]+0.5*dx)\n",
    "    return h\n",
    "\n",
    "def W(R):\n",
    "    return np.piecewise(R,[(da<=R) & (R<db)],[lambda R: D0*p*np.sin(np.pi/p)*a/(np.pi*(db-R)**(1.0/p)), 0])\n",
    "\n",
    "# Performs the reverse of the transforms defined in Eq. (A3) of Bortfeld & Schlegel (1996)\n",
    "def back_transform(w,da,db,x):\n",
    "    N = len(x)\n",
    "    dx = x[1] - x[0]\n",
    "    Na = int(da/dx)\n",
    "    Nb = int(db/dx)\n",
    "    w = w[:Nb+1]\n",
    "    w_reverse = w[::-1]\n",
    "    return np.concatenate((np.zeros(Na), w_reverse[Na:Nb+1], np.zeros(N-Nb-1)))\n",
    "\n",
    "# Part of the exact solution of the spread-out Bragg peak for p = 1.5 (Bortfeld & Schlegel (1996), Eq. (6))\n",
    "def SOBP_buildup(d,da,db,D0):\n",
    "    r = (da - d)/(db - da)\n",
    "    r_hat = r**(1.0/3.0)\n",
    "    return D0*(0.75+np.sqrt(3)/(4*np.pi)*np.log((1+r_hat)**2/(1-r_hat+r_hat**2))-3.0/(2.0*np.pi)*np.arctan((2*r_hat-1)/np.sqrt(3)))\n",
    "\n",
    "# Full piecewise-defined analytical solution for the SOBP with p = 1.5 (Bortfeld & Schlegel (1996), Eq. (6))\n",
    "def SOBP_analytic(d,da,db,D0):\n",
    "    return np.piecewise(d,[(0<=d) & (d<da),(da<=d) & (d<=db)],[lambda d: SOBP_buildup(d,da,db,D0), D0, 0])\n",
    "\n",
    "# Add grid and legend to plots\n",
    "def add_grid_legend():\n",
    "    plt.grid('on')\n",
    "    handles, labels = plt.gca().get_legend_handles_labels()\n",
    "    plt.legend(handles, labels, loc = 'best')\n",
    "\n",
    "# Definition of the depth grid\n",
    "N = 201                         # Number of grid points\n",
    "dmax = 20.0                     # Maximum depth up to which to generate model output\n",
    "d = np.linspace(0,dmax,N)       # Depth grid\n",
    "dd = d[1] - d[0]                # Depth grid spacing\n",
    "Na = int(da/dd)                 # Grid point index corresponding to d = da\n",
    "Nb = int(db/dd)                 # Grid point index corresponding to d = db\n",
    "\n",
    "g_avs = impulse(g,d)            # Impulse response function (with averaging applied to remove singularity)\n",
    "\n",
    "M = N                           # Length of inverse filter response w(R)\n",
    "d_w = np.arange(M)*dd           # Depth grid for w(R)\n",
    "Nd = M + N - 1                  # Number of output samples needed to obtain the inverse filter response of the required length\n",
    "\n",
    "yd = D0*np.ones(Nd)             # Heaviside step function of length Nd\n",
    "\n",
    "# Perform deconvolution of the step function with the averaged impulse response\n",
    "w, remainder = signal.deconvolve(yd,g_avs)\n",
    "w /= dd                        # Divide by grid spacing to obtain an approxiamtion of the continuous-valued weighting function\n",
    "\n",
    "w2 = back_transform(w,da,db,d)\n",
    "\n",
    "# Obtain the spread-out Bragg peak by deconvolution (cf. Bortfeld & Schlegel (1996), Eq. (B1))\n",
    "SOBP = dd * signal.convolve(w2,g_avs[::-1])\n",
    "SOBP = SOBP[N-1:]               # Remove boundary effects at the beginning of the convolution output\n",
    "\n",
    "# Figure 1: Illustration of the Bragg peak\n",
    "plt.figure(1)\n",
    "Ncont = 2001                        # More finely spaced, 'continuous' grid for plotting exact solution\n",
    "dcont = np.linspace(0,dmax,Ncont)\n",
    "dx_cont = dcont[1] - dcont[0]\n",
    "\n",
    "gx = Bragg_peak(db,dcont)           # Exact Bragg peak (Bortfeld & Schlegel, Eq. (3))\n",
    "gx /= gx[0]                         # Normalize by value at d = 0\n",
    "gx[int(db/dx_cont)] = np.nan                 # Set to not-a-number at the singularity at d = db\n",
    "\n",
    "gn = impulse(lambda d: Bragg_peak(db,d),d)      # Discretized Bragg peak in which the singularity at d = db is removed by averaging\n",
    "gn /= gn[0]                                     # Normalized by value at d = 0\n",
    "\n",
    "plt.plot(d,gn,'b.',label='Averaged')\n",
    "ymin,ymax = plt.gca().get_ylim()                # Record auto-scaled axis limits for the discrete (averaged) Bragg peak only\n",
    "plt.plot(dcont,gx,'c',label='Exact')\n",
    "plt.axvline(x=db,color='c',linestyle='--')\n",
    "plt.title(\"Bragg peak\")\n",
    "plt.xlabel('$d$ (cm)')\n",
    "plt.ylabel(r'$D_{BP}\\left( d \\right)/D_{BP}\\left( 0 \\right)$')\n",
    "plt.gca().set_ylim(ymin,ymax)\n",
    "add_grid_legend()\n",
    "\n",
    "# Figure 2: Weighting function\n",
    "plt.figure(2)\n",
    "wx = W(dcont)           # Exact weighting function (Bortfeld & Schlegel (1996), Eq. (4))\n",
    "wx[int(db/dx_cont)] = np.nan\n",
    "plt.plot(d,w2,'b.',label='Numerical')\n",
    "ymin,ymax = plt.gca().get_ylim()                # Record auto-scaled axis limits\n",
    "plt.plot(dcont,wx,'c',label='Exact')\n",
    "plt.axvline(x=db,color='c',linestyle='--')\n",
    "plt.xlabel('$R$ (cm)')\n",
    "plt.ylabel(r'$W\\left(R\\right)$')\n",
    "plt.title('Weighting function')\n",
    "plt.gca().set_ylim(ymin,ymax)\n",
    "add_grid_legend()\n",
    "\n",
    "# Figure 3: Spread-out Bragg peak\n",
    "plt.figure(3)\n",
    "plt.plot(d,SOBP,'b.',label='Numerical')\n",
    "plt.plot(dcont,SOBP_analytic(dcont,da,db,D0),'c',label='Exact')\n",
    "plt.xlabel('$d$ (cm)')\n",
    "plt.ylabel('$D_{SOBP}(d))/D_0$')\n",
    "plt.title('Spread-out Bragg peak')\n",
    "plt.ylim([0,1.2*D0])\n",
    "add_grid_legend()\n",
    "\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "d5a0e3ca",
   "metadata": {},
   "outputs": [],
   "source": [
    "#https://www.kaggle.com/code/augeropendata/the-heitler-model/notebook"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "cc574235",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
